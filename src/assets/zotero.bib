@misc{2016,
  title = {Demystifying {{The Regular Expression That Checks If A Number Is Prime}}},
  year = {2016},
  month = sep,
  urldate = {2024-11-01},
  abstract = {Ever wondered how the {\textasciicircum}.?\${\textbar}{\textasciicircum}(..+?){\textbackslash}1+\$ regular expression can tell you if a number is not prime? In this blog post you'll learn how and why it works. Code examples in Java, JavaScript, Python and Perl are provided.},
  howpublished = {https://illya.sh/the-codeumentary-blog/regular-expression-check-if-number-is-prime/./regular-expression-check-if-number-is-prime/}
}

@article{2020,
  title = {{{US Strategic Command}} ({{USSTRATCOM}}) (Operation Plan) {{OPLAN}} 8010-08: {{Global Deterrence}} and {{Strike}}, 2008; and {{OPLAN}} 8010-12: {{Strategic Deterrence}} and {{Force Employment}}, 2012},
  year = {2020},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HN2STX9N/US Strategic Command (USSTRATCOM) (operation plan) OPLAN 8010-08 Global Deterrence and Strike, 2008.pdf}
}

@article{2023,
  title = {Turing {{Complete Transformers}}: {{Two Transformers Are More Powerful Than One}}},
  shorttitle = {Turing {{Complete Transformers}}},
  year = {2023},
  month = oct,
  urldate = {2024-01-26},
  abstract = {This paper presents Find+Replace transformers, a family of multi-transformer architectures that can provably do things no single transformer can, and which outperforms GPT-4 on several challenging tasks. We first establish that traditional transformers and similar architectures are not Turing Complete, while Find+Replace transformers are. Using this fact, we show how arbitrary programs can be compiled into Find+Replace transformers, potentially aiding interpretability research. We also demonstrate the superior performance of Find+Replace transformers over GPT-4 on a set of composition challenge problems. This work aims to provide a theoretical basis for multi-transformer architectures, and to encourage their further exploration.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/CSUNWLCM/2023 - Turing Complete Transformers Two Transformers Are More Powerful Than One.pdf}
}

@misc{2024,
  title = {Why {{America Chose Trump}}: {{Inflation}}, {{Immigration}}, and the {{Democratic Brand}}},
  shorttitle = {Why {{America Chose Trump}}},
  year = {2024},
  month = nov,
  urldate = {2025-02-21},
  abstract = {Harris couldn't outrun her past or her party--- it was a vice grip that proved impossible to escape.},
  howpublished = {https://blueprint2024.com/polling/why-trump-reasons-11-8/}
}

@misc{2025,
  title = {Western Leaders Condemn {{Israel}} --- yet Their Armies Ask It for Advice},
  year = {2025},
  month = aug,
  journal = {archive.li},
  urldate = {2025-08-22},
  howpublished = {https://archive.li/Hzect},
  file = {/Users/nobr/Zotero/storage/EUYDTR9N/Hzect.html}
}

@article{aaronson2020,
  title = {The {{Busy Beaver Frontier}}},
  author = {Aaronson, Scott},
  year = {2020},
  month = sep,
  journal = {ACM SIGACT News},
  volume = {51},
  number = {3},
  pages = {32--54},
  issn = {0163-5700},
  doi = {10.1145/3427361.3427369},
  urldate = {2023-11-02},
  abstract = {The Busy Beaver function, with its incomprehensibly rapid growth, has captivated generations of computer scientists, mathematicians, and hobbyists. In this survey, I o er a personal view of the BB function 58 years after its introduction, emphasizing lesser-known insights, recent progress, and especially favorite open problems. Examples of such problems include: when does the BB function rst exceed the Ackermann function? Is the value of BB(20) independent of set theory? Can we prove that BB(n + 1) {$>$} 2BB(n) for large enough n? Given BB(n), how many advice bits are needed to compute BB(n + 1)? Do all Busy Beavers halt on all inputs, not just the 0 input? Is it decidable, given n, whether BB(n) is even or odd?},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XU42JIDL/Aaronson - 2020 - The Busy Beaver Frontier.pdf}
}

@book{abbott2015,
  title = {Understanding Analysis},
  author = {Abbott, Stephen},
  year = {2015},
  series = {Undergraduate Texts in Mathematics},
  edition = {2nd edition},
  publisher = {Springer},
  address = {New York},
  abstract = {This lively introductory text exposes the student to the rewards of a rigorous study of functions of a real variable. In each chapter, informal discussions of questions that give analysis its inherent fascination are followed by precise, but not overly formal, developments of the techniques needed to make sense of them. By focusing on the unifying themes of approximation and the resolution of paradoxes that arise in the transition from the finite to the infinite, the text turns what could be a daunting cascade of definitions and theorems into a coherent and engaging progression of ideas. Acutely aware of the need for rigor, the student is much better prepared to understand what constitutes a proper mathematical proof and how to write one},
  isbn = {978-1-4939-2712-8},
  langid = {english},
  lccn = {515},
  file = {/Users/nobr/Zotero/storage/8FLZAY65/Abbott - 2015 - Understanding analysis.pdf}
}

@misc{abc_plan,
  title = {{{ABC}} (Low Carbon Agriculture) Plan}
}

@article{abdalla2022,
  title = {Statistically Enriched Geospatial Datasets of {{Brazilian}} Municipalities for Data-Driven Modeling},
  author = {Abdalla, Livia and Augusto, Douglas A. and Chame, Marcia and Dufek, Amanda S. and Oliveira, Leonardo and Krempser, Eduardo},
  year = {2022},
  month = aug,
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  pages = {489},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01581-2},
  urldate = {2023-07-12},
  abstract = {The lack of georeferencing in geospatial datasets hinders the accomplishment of scientific studies that rely on accurate data. This is particularly concerning in the field of health sciences, where georeferenced data could lead to scientific results of great relevance to society. The Brazilian health systems, especially those for Notifiable Diseases, in practice do not register georeferenced data; instead, the records indicate merely the municipality in which the event occurred. Typically in data-driven modeling, accurate disease prediction models based on occurrence requires socioenvironmental characteristics of the exact location of each event, which is often unavailable. To enrich the expressiveness of data-driven models when the municipality of the event is the best available information, we produced datasets with statistical characterization of all 5,570 Brazilian municipalities in 642 layers of thematic data that represent the natural and artificial characteristics of the municipalities' landscapes over time. This resulted in a collection of datasets comprising a total of 11,556 descriptive statistics attributes for each municipality.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Climate sciences,Ecology,Environmental sciences,Risk factors},
  file = {/Users/nobr/Zotero/storage/7VBR3CDE/Abdalla et al. - 2022 - Statistically enriched geospatial datasets of Braz.pdf}
}

@misc{abdelnabi2023,
  title = {{{LLM-Deliberation}}: {{Evaluating LLMs}} with {{Interactive Multi-Agent Negotiation Games}}},
  shorttitle = {{{LLM-Deliberation}}},
  author = {Abdelnabi, Sahar and Gomaa, Amr and Sivaprasad, Sarath and Sch{\"o}nherr, Lea and Fritz, Mario},
  year = {2023},
  month = sep,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/EJQAAGJU/Abdelnabi et al. - 2023 - LLM-Deliberation Evaluating LLMs with Interactive Multi-Agent Negotiation Games.pdf}
}

@misc{abdinPhi3TechnicalReport2024,
  title = {Phi-3 {{Technical Report}}: {{A Highly Capable Language Model Locally}} on {{Your Phone}}},
  shorttitle = {Phi-3 {{Technical Report}}},
  author = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, S{\'e}bastien and Cai, Martin and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chopra, Parul and Del Giorno, Allie and {de Rosa}, Gustavo and Dixon, Matthew and Eldan, Ronen and Iter, Dan and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Liang, Chen and Liu, Weishung and Lin, Eric and Lin, Zeqi and Madan, Piyush and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and {Perez-Becker}, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Song, Xia and Ruwase, Olatunji and Wang, Xin and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wyatt, Michael and Xu, Can and Xu, Jiahang and Yadav, Sonali and Yang, Fan and Yang, Ziyi and Yu, Donghan and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yunan and Zhou, Xiren},
  year = {2024},
  month = apr,
  number = {arXiv:2404.14219},
  eprint = {2404.14219},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2404.14219},
  urldate = {2024-04-23},
  abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\% and 78\% on MMLU, and 8.7 and 8.9 on MT-bench).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/SYMHQX98/Abdin et al. - 2024 - Phi-3 Technical Report A Highly Capable Language Model Locally on Your Phone.pdf}
}

@article{adams2025,
  title = {Can a {{Single Human Supervise}} a {{Swarm}} of 100 {{Heterogeneous Robots}}?},
  author = {Adams, Julie A. and Hamell, Joshua and Walker, Phillip},
  year = {2025},
  journal = {IEEE Transactions on Field Robotics},
  volume = {2},
  pages = {46--80},
  issn = {2997-1101},
  doi = {10.1109/TFR.2024.3502316},
  urldate = {2025-01-27},
  abstract = {An open research question has been whether a single human can supervise a true heterogeneous swarm of robots completing tasks in real-world environments. A general concern is whether or not the human's workload will be taxed to the breaking point. The Defense Advanced Research Projects Agency's (DARPA) OFFensive Swarm-Enabled Tactics (OFFSET) program's field exercises (FXs) that occurred at U.S. Army urban training sites provided the opportunity to understand the impact of achieving such swarm deployments. The Command and Control Of Aggregate Swarm Tactics (CCAST) integrator team's swarm commander (SC) uses the heterogeneous robot swarm to conduct relevant missions. During the final OFFSET program FX, the team collected objective and subjective metrics related to the SC's human performance. A multi-dimensional workload algorithm that estimates overall workload based on five components of workload was used to analyze the results. While the SC's workload estimates did cross the overload threshold frequently, the SC was able to successfully complete the missions, often under challenging operational conditions. The presented results demonstrate that a single human can deploy a swarm of 100 heterogeneous robots to conduct real-world missions.},
  keywords = {ARPANET,Autonomous aerial vehicles,Autonomous robots,Buildings,Cameras,Command and control systems,Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled Tactics (OFFSET),Human-robot interaction,human-swarm interaction,multi-dimensional workload,Navigation,Particle swarm optimization,Payloads,Robot kinematics,Robot vision systems,Universal Serial Bus,US Department of Defense},
  file = {/Users/nobr/Zotero/storage/DNFIRGLD/Adams et al. - 2025 - Can a Single Human Supervise a Swarm of 100 Heterogeneous Robots.pdf}
}

@article{adar2014,
  title = {{{CommandSpace}}},
  author = {Adar, Eytan and Dontcheva, Mira and Laput, Gierad},
  year = {2014},
  pages = {167--176},
  doi = {10.1145/2642918.2647395},
  file = {/Users/nobr/Zotero/storage/DAI44R59/document.pdf}
}

@misc{adolphsCRINGELossLearning2022,
  title = {The {{CRINGE Loss}}: {{Learning}} What Language Not to Model},
  shorttitle = {The {{CRINGE Loss}}},
  author = {Adolphs, Leonard and Gao, Tianyu and Xu, Jing and Shuster, Kurt and Sukhbaatar, Sainbayar and Weston, Jason},
  year = {2022},
  month = nov,
  number = {arXiv:2211.05826},
  eprint = {2211.05826},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-15},
  abstract = {Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data -- examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the CRINGE loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/HVPVXPXY/Adolphs et al. - 2022 - The CRINGE Loss Learning what language not to model.pdf;/Users/nobr/Zotero/storage/QXTJV8K3/2211.html}
}

@misc{agarwalSpectralStateSpace2024,
  title = {Spectral {{State Space Models}}},
  author = {Agarwal, Naman and Suo, Daniel and Chen, Xinyi and Hazan, Elad},
  year = {2024},
  month = feb,
  number = {arXiv:2312.06837},
  eprint = {2312.06837},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.06837},
  urldate = {2024-02-10},
  abstract = {This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/YGH43BV2/Agarwal et al. - 2024 - Spectral State Space Models.pdf}
}

@misc{agostinelliSearchExpansionsLearning2023,
  title = {A* {{Search Without Expansions}}: {{Learning Heuristic Functions}} with {{Deep Q-Networks}}},
  shorttitle = {A* {{Search Without Expansions}}},
  author = {Agostinelli, Forest and Shmakov, Alexander and McAleer, Stephen and Fox, Roy and Baldi, Pierre},
  year = {2023},
  month = mar,
  number = {arXiv:2102.04518},
  eprint = {2102.04518},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.04518},
  urldate = {2024-03-20},
  abstract = {Efficiently solving problems with large action spaces using A* search has been of importance to the artificial intelligence community for decades. This is because the computation and memory requirements of A* search grow linearly with the size of the action space. This burden becomes even more apparent when A* search uses a heuristic function learned by computationally expensive function approximators, such as deep neural networks. To address this problem, we introduce Q* search, a search algorithm that uses deep Q-networks to guide search in order to take advantage of the fact that the sum of the transition costs and heuristic values of the children of a node can be computed with a single forward pass through a deep Q-network without explicitly generating those children. This significantly reduces computation time and requires only one node to be generated per iteration. We use Q* search to solve the Rubik's cube when formulated with a large action space that includes 1872 meta-actions and find that this 157-fold increase in the size of the action space incurs less than a 4-fold increase in computation time and less than a 3-fold increase in number of nodes generated when performing Q* search. Furthermore, Q* search is up to 129 times faster and generates up to 1288 times fewer nodes than A* search. Finally, although obtaining admissible heuristic functions from deep neural networks is an ongoing area of research, we prove that Q* search is guaranteed to find a shortest path given a heuristic function that neither overestimates the cost of a shortest path nor underestimates the transition cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/XDN8732I/Agostinelli et al. - 2023 - A Search Without Expansions Learning Heuristic Functions with Deep Q-Networks.pdf;/Users/nobr/Zotero/storage/WQVVS9I4/2102.html}
}

@article{ahdritz2022,
  title = {Evolutionary-Scale Prediction of Atomic-Level Protein Structure with a Language Model},
  author = {Ahdritz, Gustaf and Bouatta, Nazim and Kadyan, Sachin and Xia, Qinghui and Gerecke, William and O'Donnell, Timothy J and Berenberg, Daniel and Fisk, Ian and Zanichelli, Niccol{\`o} and Zhang, Bo and Nowaczynski, Arkadiusz and Wang, Bei and {Stepniewska-Dziubinska}, Marta M and Zhang, Shang and Ojewole, Adegoke and Guney, Murat Efe and Biderman, Stella and Watkins, Andrew M and Ra, Stephen and Lorenzo, Pablo Ribalta and Nivon, Lucas and Weitzner, Brian and Ban, Yih-En Andrew and Sorger, Peter K and Mostaque, Emad and Zhang, Zhao and Bonneau, Richard and AlQuraishi, Mohammed},
  year = {2022},
  doi = {10.1101/2022.11.20.517210},
  file = {/Users/nobr/Zotero/storage/5ZIUB95S/document.pdf}
}

@article{ahlswede2000,
  title = {Network Information Flow},
  author = {Ahlswede, R. and {Ning Cai} and Li, S.-Y.R. and Yeung, R.W.},
  year = {2000},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {46},
  number = {4},
  pages = {1204--1216},
  issn = {00189448},
  doi = {10.1109/18.850663},
  urldate = {2025-08-31},
  abstract = {We introduce a new class of problems called network information flow which is inspired by computer network applications. Consider a point-to-point communication network on which a number of information sources are to be mulitcast to certain sets of destinations. We assume that the information sources are mutually independent. The problem is to characterize the admissible coding rate region. This model subsumes all previously studied models along the same line. In this paper, we study the problem with one information source, and we have obtained a simple characterization of the admissible coding rate region. Our result can be regarded as the Max-flow Min-cut Theorem for network information flow. Contrary to one's intuition, our work reveals that it is in general not optimal to regard the information to be multicast as a ``fluid'' which can simply be routed or replicated. Rather, by employing coding at the nodes, which we refer to as network coding, bandwidth can in general be saved. This finding may have significant impact on future design of switching systems.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LZFFN92Y/Ahlswede et al. - 2000 - Network information flow.pdf}
}

@misc{ahmedGaussianBlueNoise2022,
  title = {Gaussian {{Blue Noise}}},
  author = {Ahmed, Abdalla G. M. and Ren, Jing and Wonka, Peter},
  year = {2022},
  month = jun,
  number = {arXiv:2206.07798},
  eprint = {2206.07798},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {Among the various approaches for producing point distributions with blue noise spectrum, we argue for an optimization framework using Gaussian kernels. We show that with a wise selection of optimization parameters, this approach attains unprecedented quality, provably surpassing the current state of the art attained by the optimal transport (BNOT) approach. Further, we show that our algorithm scales smoothly and feasibly to high dimensions while maintaining the same quality, realizing unprecedented high-quality high-dimensional blue noise sets. Finally, we show an extension to adaptive sampling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Graphics,Computer Science - Machine Learning,Statistics - Applications},
  file = {/Users/nobr/Zotero/storage/TJ8LQBM3/Ahmed et al. - 2022 - Gaussian Blue Noise.pdf}
}

@misc{aiferThermodynamicLinearAlgebra2023,
  title = {Thermodynamic {{Linear Algebra}}},
  author = {Aifer, Maxwell and Donatella, Kaelan and Gordon, Max Hunter and Ahle, Thomas and Simpson, Daniel and Crooks, Gavin E. and Coles, Patrick J.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.05660},
  eprint = {2308.05660},
  primaryclass = {cond-mat, physics:quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.05660},
  urldate = {2024-02-09},
  abstract = {Linear algebraic primitives are at the core of many modern algorithms in engineering, science, and machine learning. Hence, accelerating these primitives with novel computing hardware would have tremendous economic impact. Quantum computing has been proposed for this purpose, although the resource requirements are far beyond current technological capabilities, so this approach remains long-term in timescale. Here we consider an alternative physics-based computing paradigm based on classical thermodynamics, to provide a near-term approach to accelerating linear algebra. At first sight, thermodynamics and linear algebra seem to be unrelated fields. In this work, we connect solving linear algebra problems to sampling from the thermodynamic equilibrium distribution of a system of coupled harmonic oscillators. We present simple thermodynamic algorithms for (1) solving linear systems of equations, (2) computing matrix inverses, (3) computing matrix determinants, and (4) solving Lyapunov equations. Under reasonable assumptions, we rigorously establish asymptotic speedups for our algorithms, relative to digital methods, that scale linearly in matrix dimension. Our algorithms exploit thermodynamic principles like ergodicity, entropy, and equilibration, highlighting the deep connection between these two seemingly distinct fields, and opening up algebraic applications for thermodynamic computing hardware.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Condensed Matter - Statistical Mechanics,Quantum Physics},
  file = {/Users/nobr/Zotero/storage/22HDX2QT/Aifer et al. - 2023 - Thermodynamic Linear Algebra.pdf;/Users/nobr/Zotero/storage/7273FUV4/2308.html}
}

@inproceedings{akiba2019,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {2623--2631},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3292500.3330701},
  urldate = {2024-11-18},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  isbn = {978-1-4503-6201-6},
  file = {/Users/nobr/Zotero/storage/Y9DPCTDD/Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimization Framework.pdf}
}

@article{akiva2012,
  title = {Identifying {{Distinct Components}} of a {{Multi-author Document}}},
  author = {Akiva, Navot and Koppel, Moshe},
  year = {2012},
  pages = {205--209},
  doi = {10.1109/EISIC.2012.16},
  file = {/Users/nobr/Zotero/storage/UBTWMGI5/document.pdf}
}

@misc{alessandretti2022,
  title = {Urban {{Mobility}}},
  author = {Alessandretti, Laura and Szell, Michael},
  year = {2022},
  month = nov,
  number = {arXiv:2211.00355},
  eprint = {2211.00355},
  primaryclass = {physics},
  publisher = {arXiv},
  urldate = {2023-07-10},
  abstract = {In this chapter, we discuss urban mobility from a complexity science perspective. First, we give an overview of the datasets that enable this approach, such as mobile phone records, location-based social network traces, or GPS trajectories from sensors installed on vehicles. We then review the empirical and theoretical understanding of the properties of human movements, including the distribution of travel distances and times, the entropy of trajectories, and the interplay between exploration and exploitation of locations. Next, we explain generative and predictive models of individual mobility, and their limitations due to intrinsic limits of predictability. Finally, we discuss urban transport from a systemic perspective, including system-wide challenges like ridesharing, multimodality, and sustainable transport.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Physics - Physics and Society},
  file = {/Users/nobr/Zotero/storage/YKHDV8SE/Alessandretti and Szell - 2022 - Urban Mobility.pdf;/Users/nobr/Zotero/storage/W5MIVW5G/2211.html}
}

@article{aliannejadi2019,
  title = {Asking {{Clarifying Questions}} in {{Open-Domain Information-Seeking Conversations}}},
  author = {Aliannejadi, Mohammad and Zamani, Hamed and Crestani, Fabio and Croft, W. Bruce},
  year = {2019},
  pages = {475--484},
  doi = {10.1145/3331184.3331265},
  file = {/Users/nobr/Zotero/storage/CHA3R77E/Aliannejadi et al. - 2019 - Asking Clarifying Questions in Open-Domain Information-Seeking Conversations.pdf}
}

@misc{alkhatib2023,
  title = {Interpretable {{Graph Neural Networks}} for {{Tabular Data}}},
  author = {Alkhatib, Amr and Ennadir, Sofiane and Bostr{\"o}m, Henrik and Vazirgiannis, Michalis},
  year = {2023},
  month = aug,
  number = {arXiv:2308.08945},
  eprint = {2308.08945},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.08945},
  urldate = {2023-08-26},
  abstract = {Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true Shapley values of the features without incurring any additional computational overhead.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/XPA5SSWT/Alkhatib et al. - 2023 - Interpretable Graph Neural Networks for Tabular Data.pdf;/Users/nobr/Zotero/storage/C2CLBKLN/2308.html}
}

@book{alla2021,
  title = {Beginning {{MLOps}} with {{MLFlow}}: {{Deploy Models}} in {{AWS SageMaker}}, {{Google Cloud}}, and {{Microsoft Azure}}},
  shorttitle = {Beginning {{MLOps}} with {{MLFlow}}},
  author = {Alla, Sridhar and Adari, Suman Kalyan},
  year = {2021},
  publisher = {Apress},
  address = {Berkeley, CA},
  doi = {10.1007/978-1-4842-6549-9},
  urldate = {2023-11-09},
  isbn = {978-1-4842-6548-2 978-1-4842-6549-9},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TZK22NK5/Alla and Adari - 2021 - Beginning MLOps with MLFlow Deploy Models in AWS SageMaker, Google Cloud, and Microsoft Azure.pdf}
}

@book{allen1989,
  title = {War Games: Inside the Secret World of the Men Who Play at {{World War III}}},
  shorttitle = {War Games},
  author = {Allen, Thomas B.},
  year = {1989},
  publisher = {Heinemann, Mandarin},
  address = {London},
  isbn = {978-0-7493-0011-1},
  langid = {english},
  annotation = {OCLC: 19268160}
}

@incollection{allen2018framing,
  title = {Framing and Context},
  booktitle = {Global {{Warming}} of 1.5{$^\circ$}{{C}}. {{An IPCC Special Report}} on the Impacts of Global Warming of 1.5{$^\circ$}{{C}} above Pre-Industrial Levels and Related Global Greenhouse Gas Emission Pathways, in the Context of Strengthening the Global Response to the Threat of Climate Change, Sustainable Development, and Efforts to Eradicate Poverty},
  author = {Allen, Myles R. and Dube, Opha Pauline and Solecki, William and {Arag{\'o}n-Durand}, Fernando and Cramer, Wolfgang and Humphreys, Stephen and Kainuma, Mikiko and Kala, Jatin and Mahowald, Natalie and Mulugetta, Yacob and Perez, Rosa and Wairiu, Morgan and Zickfeld, Kirsten},
  editor = {{Masson-Delmotte}, V. and Zhai, P. and P{\"o}rtner, H.-O. and Roberts, D. and Skea, J. and Shukla, P.R. and Pirani, A. and {Moufouma-Okia}, W. and P{\'e}an, C. and Pidcock, R. and Connors, S. and Matthews, J.B.R. and Chen, Y. and Zhou, X. and Gomis, M.I. and Lonnoy, E. and Maycock, T. and Tignor, M. and Waterfield, T.},
  year = {2018},
  pages = {49--92},
  publisher = {Intergovernmental Panel on Climate Change}
}

@article{allen2022,
  title = {A Massive {{7T fMRI}} Dataset to Bridge Cognitive Neuroscience and Artificial Intelligence},
  author = {Allen, Emily J. and {St-Yves}, Ghislain and Wu, Yihan and Breedlove, Jesse L. and Prince, Jacob S. and Dowdle, Logan T. and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and Hutchinson, J. Benjamin and Naselaris, Thomas and Kay, Kendrick},
  year = {2022},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {25},
  number = {1},
  pages = {116--126},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00962-x},
  urldate = {2023-05-08},
  abstract = {Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cortex,Neural encoding,Object vision,Perception},
  file = {/Users/nobr/Zotero/storage/ISLGZJJC/Allen et al. - 2022 - A massive 7T fMRI dataset to bridge cognitive neur.pdf}
}

@article{almaatouq2016,
  title = {Are {{You Your Friends}}' {{Friend}}? {{Poor Perception}} of {{Friendship Ties Limits}} the {{Ability}} to {{Promote Behavioral Change}}},
  shorttitle = {Are {{You Your Friends}}' {{Friend}}?},
  author = {Almaatouq, Abdullah and Radaelli, Laura and Pentland, Alex and Shmueli, Erez},
  editor = {Zhang, Zi-Ke},
  year = {2016},
  month = mar,
  journal = {PLOS ONE},
  volume = {11},
  number = {3},
  pages = {e0151588},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0151588},
  urldate = {2024-10-23},
  abstract = {Persuasion is at the core of norm creation, emergence of collective action, and solutions to `tragedy of the commons' problems. In this paper, we show that the directionality of friendship ties affect the extent to which individuals can influence the behavior of each other. Moreover, we find that people are typically poor at perceiving the directionality of their friendship ties and that this can significantly limit their ability to engage in cooperative arrangements. This could lead to failures in establishing compatible norms, acting together, finding compromise solutions, and persuading others to act. We then suggest strategies to overcome this limitation by using two topological characteristics of the perceived friendship network. The findings of this paper have significant consequences for designing interventions that seek to harness social influence for collective action.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/MFUZBLMR/Almaatouq et al. - 2016 - Are You Your Friends’ Friend Poor Perception of Friendship Ties Limits the Ability to Promote Behav.pdf}
}

@book{americanpsychiatricassociation2013,
  title = {Diagnostic and Statistical Manual of Mental Disorders: {{DSM-5}}},
  shorttitle = {Diagnostic and Statistical Manual of Mental Disorders},
  editor = {American Psychiatric Association and American Psychiatric Association},
  year = {2013},
  edition = {5th ed},
  publisher = {American Psychiatric Association},
  address = {Washington, D.C},
  isbn = {978-0-89042-554-1 978-0-89042-555-8},
  langid = {english},
  lccn = {RC455.2.C4 D54 2013},
  keywords = {classification,Classification,diagnosis,Diagnosis,Diagnostic and statistical manual of mental disorders,Mental Disorders,Mental illness},
  file = {/Users/nobr/Zotero/storage/EBIAAJBN/American Psychiatric Association and American Psychiatric Association - 2013 - Diagnostic and statistical manual of mental disorders DSM-5.pdf}
}

@article{anderson,
  title = {Effects of \$9 {{Price Endings}} on {{Retail Sales}}: {{Evidence}} from {{Field Experiments}}},
  author = {Anderson, Eric T and Simester, Duncan I},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/VPEABS49/Anderson and Simester - Effects of $9 Price Endings on Retail Sales Evidence from Field Experiments.pdf}
}

@article{anderson2022,
  title = {Investigating Cognitive Neuroscience Theories of Human Intelligence: {{A}} Connectome-based Predictive Modeling Approach},
  author = {Anderson, Evan D. and Barbey, Aron K.},
  year = {2022},
  journal = {Human Brain Mapping},
  volume = {44},
  number = {4},
  pages = {1647--1665},
  doi = {10.1002/hbm.26164},
  file = {/Users/nobr/Zotero/storage/6X3HJ3JH/Anderson and Barbey - 2022 - Investigating cognitive neuroscience theories of human intelligence A connectome‐based predictive m.pdf}
}

@misc{anfavea,
  title = {Associa{\c c}{\~a}o Nacional de Fabricantes de Ve{\'i}culos Automotivos}
}

@misc{anne2025,
  title = {Adversarial {{Coevolutionary Illumination}} with {{Generational Adversarial MAP-Elites}}},
  author = {Anne, Timoth{\'e}e and Syrkis, Noah and Elhosni, Meriem and Turati, Florian and Legendre, Franck and Jaquier, Alain and Risi, Sebastian},
  year = {2025},
  month = may,
  number = {arXiv:2505.06617},
  eprint = {2505.06617},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.06617},
  urldate = {2025-05-15},
  abstract = {Unlike traditional optimization algorithms focusing on finding a single optimal solution, Quality-Diversity (QD) algorithms illuminate a search space by finding high-performing solutions that cover a specified behavior space. However, tackling adversarial problems is more challenging due to the behavioral interdependence between opposing sides. Most applications of QD algorithms to these problems evolve only one side, thus reducing illumination coverage. In this paper, we propose a new QD algorithm, Generational Adversarial MAP-Elites (GAME), which coevolves solutions by alternating sides through a sequence of generations. Combining GAME with vision embedding models enables the algorithm to directly work from videos of behaviors instead of handcrafted descriptors. Some key findings are that (1) emerging evolutionary dynamics sometimes resemble an arms race, (2) starting each generation from scratch increases open-endedness, and (3) keeping neutral mutations preserves stepping stones that seem necessary to reach the highest performance. In conclusion, the results demonstrate that GAME can successfully illuminate an adversarial multi-agent game, opening up interesting future directions in understanding the emergence of open-ended coevolution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/QRHS8S99/Anne et al. - 2025 - Adversarial Coevolutionary Illumination with Generational Adversarial MAP-Elites.pdf}
}

@article{anne2025a,
  title = {Harnessing {{Language}} for {{Coordination}}: {{A Framework}} and {{Benchmark}} for {{LLM-Driven Multi-Agent Control}}},
  shorttitle = {Harnessing {{Language}} for {{Coordination}}},
  author = {Anne, Timoth{\'e}e and Syrkis, Noah and Elhosni, Meriem and Turati, Florian and Legendre, Franck and Jaquier, Alain and Risi, Sebastian},
  year = {2025},
  journal = {IEEE Transactions on Games},
  pages = {1--25},
  issn = {2475-1510},
  doi = {10.1109/TG.2025.3564042},
  urldate = {2025-05-06},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. Their potential to facilitate human coordination with many agents is a promising but largely under-explored area. Such capabilities would be helpful in disaster response, urban planning, and real-time strategy scenarios. In this work, we introduce (1) a real-time strategy game benchmark designed to evaluate these abilities and (2) a novel framework we term HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000 agents through a natural language dialog with an LLM. We present promising results on this multi-agent benchmark, with our hybrid approach solving tasks such as coordinating agent movements, exploiting unit weaknesses, leveraging human annotations, and understanding terrain and strategic points. Our findings also highlight critical limitations of current models, including difficulties in processing spatial visual information and challenges in formulating long-term strategic plans. This work sheds light on the potential and limitations of LLMs in human-swarm coordination, paving the way for future research in this area. The HIVE project page, hive.syrkis.com, includes videos of the system in action.},
  keywords = {Behavior tree,Benchmark testing,Bridges,Cognition,Collaboration,Games,large language models,multi-agent,Multi-agent systems,Real-time systems,Strategic planning,strategy games,Translation,Visualization},
  file = {/Users/nobr/Zotero/storage/DN8LWSKD/Anne et al. - 2025 - Harnessing Language for Coordination A Framework and Benchmark for LLM-Driven Multi-Agent Control.pdf}
}

@article{anselin2010,
  title = {Local {{Indicators}} of {{Spatial Association-LISA}}},
  author = {Anselin, Luc},
  year = {2010},
  month = sep,
  journal = {Geographical Analysis},
  volume = {27},
  number = {2},
  pages = {93--115},
  issn = {00167363},
  doi = {10.1111/j.1538-4632.1995.tb00338.x},
  urldate = {2023-07-04},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7B53YUAA/Anselin - 2010 - Local Indicators of Spatial Association-LISA.pdf}
}

@article{anthony2020,
  title = {Carbontracker: {{Tracking}} and {{Predicting}} the {{Carbon Footprint}} of {{Training Deep Learning Models}}},
  author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
  year = {2020},
  month = jul,
  eprint = {2007.03051},
  abstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/WMCRAISP/Anthony et al. - 2020 - Carbontracker Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.pdf}
}

@inproceedings{anwer2020,
  title = {A {{Formal Model}} for {{Behavior Trees Based}} on {{Context}} - {{Free Grammar}}},
  booktitle = {2020 27th {{Asia-Pacific Software Engineering Conference}} ({{APSEC}})},
  author = {Anwer, Sajid and Wen, Lian and Wang, Zhe},
  year = {2020},
  month = dec,
  pages = {465--469},
  publisher = {IEEE},
  address = {Singapore, Singapore},
  doi = {10.1109/APSEC51365.2020.00057},
  urldate = {2024-02-08},
  abstract = {In the last two decades, several studies have been carried out to translate Behavior Trees (BTs) into other formal languages. However, as BTs are usually drawn directly from natural languages, there is no formal grammar to define what is a valid BT. In this research, we first propose a normal form for requirement BT as a building block for a valid BT, and then design a context-free grammar that can generate and verify all valid BTs. This work provides a solid foundation for BT research and will improve the quality of requirements modeling by identifying some common requirement defects.},
  isbn = {978-1-7281-9553-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TBIX76XJ/Anwer et al. - 2020 - A Formal Model for Behavior Trees Based on Context - Free Grammar.pdf}
}

@misc{aosis,
  title = {Alliance of Small Island States ({{AOSIS}})},
  year = {1990}
}

@book{apollonius2014,
  title = {Argonautica},
  author = {Apollonius, Rhodius},
  year = {2014},
  publisher = {Cambridge, MA : Harvard University Press},
  urldate = {2024-01-11},
  abstract = {{$<$}?xml version="1.0" encoding="utf-8"?{$>$}Apollonius Rhodius\&amp;rsquo;s Argonautica, composed in the 3rd century BCE, is the epic retelling of Jason\&amp;rsquo;s quest for the golden fleece. Along with his contemporaries Callimachus and Theocritus, ...},
  isbn = {978-0-674-99630-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/VSYJXNGX/volume.html}
}

@misc{arbel2024,
  title = {{{MLXP}}: {{A Framework}} for {{Conducting Replicable Experiments}} in {{Python}}},
  shorttitle = {{{MLXP}}},
  author = {Arbel, Michael and Zouaoui, Alexandre},
  year = {2024},
  month = jun,
  number = {arXiv:2402.13831},
  eprint = {2402.13831},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.13831},
  urldate = {2025-07-12},
  abstract = {Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets. Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions. Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings. To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the experimental process with minimal practitioner overhead while ensuring a high level of reproducibility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/nobr/Zotero/storage/2J79HX7Z/Arbel and Zouaoui - 2024 - MLXP A Framework for Conducting Replicable Experiments in Python.pdf;/Users/nobr/Zotero/storage/ESH6SGM5/2402.html}
}

@misc{arcas2024,
  title = {Computational {{Life}}: {{How Well-formed}}, {{Self-replicating Programs Emerge}} from {{Simple Interaction}}},
  shorttitle = {Computational {{Life}}},
  author = {y Arcas, Blaise Ag{\"u}era and Alakuijala, Jyrki and Evans, James and Laurie, Ben and Mordvintsev, Alexander and Niklasson, Eyvind and Randazzo, Ettore and Versari, Luca},
  year = {2024},
  month = aug,
  number = {arXiv:2406.19108},
  eprint = {2406.19108},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.19108},
  urldate = {2024-08-20},
  abstract = {The fields of Origin of Life and Artificial Life both question what life is and how it emerges from a distinct set of "pre-life" dynamics. One common feature of most substrates where life emerges is a marked shift in dynamics when self-replication appears. While there are some hypotheses regarding how self-replicators arose in nature, we know very little about the general dynamics, computational principles, and necessary conditions for self-replicators to emerge. This is especially true on "computational substrates" where interactions involve logical, mathematical, or programming rules. In this paper we take a step towards understanding how self-replicators arise by studying several computational substrates based on various simple programming languages and machine instruction sets. We show that when random, non self-replicating programs are placed in an environment lacking any explicit fitness landscape, self-replicators tend to arise. We demonstrate how this occurs due to random interactions and self-modification, and can happen with and without background random mutations. We also show how increasingly complex dynamics continue to emerge following the rise of self-replicators. Finally, we show a counterexample of a minimalistic programming language where self-replicators are possible, but so far have not been observed to arise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,F.2.2,I.2.11},
  file = {/Users/nobr/Zotero/storage/HU5P7UL7/Arcas et al. - 2024 - Computational Life How Well-formed, Self-replicating Programs Emerge from Simple Interaction.pdf;/Users/nobr/Zotero/storage/6GP26ZJQ/2406.html}
}

@article{arikan2009,
  title = {Channel Polarization: {{A}} Method for Constructing Capacity-Achieving Codes for Symmetric Binary-Input Memoryless Channels},
  shorttitle = {Channel Polarization},
  author = {Arikan, Erdal},
  year = {2009},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {55},
  number = {7},
  eprint = {0807.3917},
  primaryclass = {cs, math},
  pages = {3051--3073},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2009.2021379},
  urldate = {2023-11-27},
  abstract = {A method is proposed, called channel polarization, to construct code sequences that achieve the symmetric capacity \$I(W)\$ of any given binary-input discrete memoryless channel (B-DMC) \$W\$. The symmetric capacity is the highest rate achievable subject to using the input letters of the channel with equal probability. Channel polarization refers to the fact that it is possible to synthesize, out of \$N\$ independent copies of a given B-DMC \$W\$, a second set of \$N\$ binary-input channels \${\textbackslash}\{W\_N{\textasciicircum}\{(i)\}:1{\textbackslash}le i{\textbackslash}le N{\textbackslash}\}\$ such that, as \$N\$ becomes large, the fraction of indices \$i\$ for which \$I(W\_N{\textasciicircum}\{(i)\})\$ is near 1 approaches \$I(W)\$ and the fraction for which \$I(W\_N{\textasciicircum}\{(i)\})\$ is near 0 approaches \$1-I(W)\$. The polarized channels \${\textbackslash}\{W\_N{\textasciicircum}\{(i)\}{\textbackslash}\}\$ are well-conditioned for channel coding: one need only send data at rate 1 through those with capacity near 1 and at rate 0 through the remaining. Codes constructed on the basis of this idea are called polar codes. The paper proves that, given any B-DMC \$W\$ with \$I(W){$>$}0\$ and any target rate \$R {$<$} I(W)\$, there exists a sequence of polar codes \${\textbackslash}\{\{{\textbackslash}mathscr C\}\_n;n{\textbackslash}ge 1{\textbackslash}\}\$ such that \$\{{\textbackslash}mathscr C\}\_n\$ has block-length \$N=2{\textasciicircum}n\$, rate \${\textbackslash}ge R\$, and probability of block error under successive cancellation decoding bounded as \$P\_\{e\}(N,R) {\textbackslash}le {\textbackslash}bigoh(N{\textasciicircum}\{-{\textbackslash}frac14\})\$ independently of the code rate. This performance is achievable by encoders and decoders with complexity \$O(N{\textbackslash}log N)\$ for each.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory},
  file = {/Users/nobr/Zotero/storage/LI85JDE5/Arikan - 2009 - Channel polarization A method for constructing capacity-achieving codes for symmetric binary-input .pdf;/Users/nobr/Zotero/storage/NRT2V22K/0807.html}
}

@article{arima,
  title = {Multiphoton-Driven {{Photocatalytic Defluorination}} of {{Persistent Perfluoroalkyl Substances}} and {{Polymers}} by {{Visible Light}}},
  author = {Arima, Yuzo and Okayasu, Yoshinori and Yoshioka, Daisuke and Nagai, Yuki and Kobayashi, Yoichi},
  journal = {Angewandte Chemie International Edition},
  volume = {n/a},
  number = {n/a},
  pages = {e202408687},
  issn = {1521-3773},
  doi = {10.1002/anie.202408687},
  urldate = {2024-07-29},
  abstract = {Perfluoroalkyl substances (PFASs) and fluorinated polymers (FPs) have been extensively utilized in various industries, whereas their extremely high stability poses environmental persistence and waste treatment. Current decomposition approaches of PFASs and FPs typically require harsh conditions such as heating over 400{$^\circ$}C. Thus, there is a pressing need to develop a new technique capable of decomposing them under mild conditions. Here, we demonstrated that perfluorooctanesulfonate (PFOS), known as a 'persistent chemical,' and Nafion, a widely utilized sulfonated FP for ion-exchange membranes, can be efficiently decomposed into fluorine ions under ambient conditions via the irradiation of visible LED light onto semiconductor nanocrystals (NCs). PFOS was completely defluorinated within 8-h irradiation of 405-nm LED light, and the turnover number of the C--F bond dissociation per NC was 17200. Furthermore, 81\% defluorination of Nafion was achieved for 24-h light irradiation, demonstrating the efficient photocatalytic properties under visible light. We revealed that this decomposition is driven by cooperative mechanisms involving light-induced ligand displacements and Auger-induced electron injections via hydrated electrons and higher excited states. This study not only demonstrates the feasibility of efficiently breaking down various PFASs and FPs under mild conditions but also paves the way for advancing toward a sustainable fluorine-recycling society.},
  langid = {english},
  keywords = {hydrated electron,nonlinear optical processes,PFAS,photocatalysis,semiconductor nanocrystals},
  file = {/Users/nobr/Zotero/storage/9UC6RULI/Arima et al. - Multiphoton-driven Photocatalytic Defluorination of Persistent Perfluoroalkyl Substances and Polymers by Visible Light.pdf}
}

@article{arribas-bel2014,
  title = {Accidental, Open and Everywhere: {{Emerging}} Data Sources for the Understanding of Cities},
  shorttitle = {Accidental, Open and Everywhere},
  author = {{Arribas-Bel}, Daniel},
  year = {2014},
  month = may,
  journal = {Applied Geography},
  volume = {49},
  pages = {45--53},
  issn = {01436228},
  doi = {10.1016/j.apgeog.2013.09.012},
  urldate = {2023-07-04},
  abstract = {In this paper, I review the recent emergence of three groups of data sources and assess some of the opportunities and challenges they pose for the understanding of cities, particularly in the context of the Regional Science and urban research agenda. These are data collected from mobile sensors carried by individuals, data derived from businesses moving their activity online and government data released in an open format. Although very different from each other, they are all becoming available as a side-effect since they were created with different purposes but their degree of popularity, pervasiveness and ease of access is turning them into interesting alternatives for researchers. Existing projects and initiatives that conform to each class are featured as illustrative examples of these new potential sources of knowledge. {\'O} 2013 Elsevier Ltd. All rights reserved.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RRWV3TC4/Arribas-Bel - 2014 - Accidental, open and everywhere Emerging data sou.pdf}
}

@misc{arriola2025,
  title = {Block {{Diffusion}}: {{Interpolating Between Autoregressive}} and {{Diffusion Language Models}}},
  shorttitle = {Block {{Diffusion}}},
  author = {Arriola, Marianne and Gokaslan, Aaron and Chiu, Justin T. and Yang, Zhihan and Qi, Zhixuan and Han, Jiaqi and Sahoo, Subham Sekhar and Kuleshov, Volodymyr},
  year = {2025},
  month = mar,
  number = {arXiv:2503.09573},
  eprint = {2503.09573},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2503.09573},
  urldate = {2025-03-14},
  abstract = {Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/CSUX8ZCJ/Arriola et al. - 2025 - Block Diffusion Interpolating Between Autoregressive and Diffusion Language Models.pdf}
}

@article{arulkumaran2017,
  title = {Deep Reinforcement Learning: {{A}} Brief Survey},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {26--38},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {10535888},
  doi = {10.1109/MSP.2017.2743240},
  urldate = {2023-03-22},
  abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
  file = {/Users/nobr/Zotero/storage/8YP88RYI/Arulkumaran et al. - 2017 - Deep reinforcement learning A brief survey.pdf}
}

@article{asal2005,
  title = {Playing {{Games}} with {{International Relations}}},
  author = {Asal, Victor},
  year = {2005},
  month = aug,
  journal = {International Studies Perspectives},
  volume = {6},
  number = {3},
  pages = {359--373},
  issn = {1528-3577},
  doi = {10.1111/j.1528-3577.2005.00213.x},
  urldate = {2024-01-14},
  abstract = {After reviewing the advantages and disadvantages in using simulations to teach International Relations, this paper develops pedagogy for using simulations to teach International Relations (IR) theory. After discussing methods for integrating simulations into a class on IR theory the paper then goes on to present three simulations and the theories that they can be used to teach. The three simulations are the Classical Realism Game, Prisoner's Dilemma to the Nth degree, and Diplomacy. Finally, the three simulations are compared.},
  file = {/Users/nobr/Zotero/storage/PIEKSUV6/Asal - 2005 - Playing Games with International Relations.pdf;/Users/nobr/Zotero/storage/QK2PKQNQ/1848730.html}
}

@article{ashford2017,
  title = {The {{Mechanical Turk}}: {{Enduring Misapprehensions Concerning Artificial Intelligence}}},
  shorttitle = {The {{Mechanical Turk}}},
  author = {Ashford, David},
  year = {2017},
  month = jun,
  journal = {The Cambridge Quarterly},
  volume = {46},
  number = {2},
  pages = {119--139},
  issn = {0008-199X, 1471-6836},
  doi = {10.1093/camqtly/bfx005},
  urldate = {2023-12-28},
  langid = {english},
  keywords = {/unread},
  file = {/Users/nobr/Zotero/storage/L7BZXTCV/Ashford - 2017 - The Mechanical Turk Enduring Misapprehensions Concerning Artificial Intelligence.pdf}
}

@article{audibert2009,
  title = {Minimax Policies for Adversarial and Stochastic Bandits},
  author = {Audibert, Jean-Yves and Bubeck, Sebastien},
  year = {2009},
  abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit problem. Concretely, we remove an extraneous logarithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PLZMT2L3/Audibert and Bubeck - Minimax policies for adversarial and stochastic bandits.pdf}
}

@article{auer2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo and Fischer, Paul},
  year = {2002},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2DIANKMA/Auer et al. - Finite-time Analysis of the Multiarmed Bandit Problem.pdf}
}

@article{azaria2023,
  title = {{{ChatGPT}} Is a {{Remarkable Tool}} -- {{For Experts}}},
  author = {Azaria, Amos and Azoulay, Rina and Reches, Shulamit},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.03102},
  urldate = {2024-02-20},
  abstract = {This paper investigates the capabilities of ChatGPT as an automated assistant in diverse domains, including scientific writing, mathematics, education, programming, and healthcare. We explore the potential of ChatGPT to enhance productivity, streamline problem-solving processes, and improve writing style. Furthermore, we highlight the potential risks associated with excessive reliance on ChatGPT in these fields. These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation. We outline areas and objectives where ChatGPT proves beneficial, applications where it should be used judiciously, and scenarios where its reliability may be limited. In light of observed limitations, and given that the tool's fundamental errors may pose a special challenge for non-experts, ChatGPT should be used with a strategic methodology. By drawing from comprehensive experimental studies, we offer methods and flow charts for effectively using ChatGPT. Our recommendations emphasize iterative interaction with ChatGPT and independent verification of its outputs. Considering the importance of utilizing ChatGPT judiciously and with expertise, we recommend its usage for experts who are well-versed in the respective domains.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Computers and Society (cs.CY),FOS: Computer and information sciences,Human-Computer Interaction (cs.HC)},
  file = {/Users/nobr/Zotero/storage/KP92YUB3/Azaria et al. - 2023 - ChatGPT is a Remarkable Tool -- For Experts.pdf}
}

@misc{azerbayevLlemmaOpenLanguage2023,
  title = {Llemma: {{An Open Language Model For Mathematics}}},
  shorttitle = {Llemma},
  author = {Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Santos, Marco Dos and McAleer, Stephen and Jiang, Albert Q. and Deng, Jia and Biderman, Stella and Welleck, Sean},
  year = {2023},
  month = oct,
  number = {arXiv:2310.10631},
  eprint = {2310.10631},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-18},
  abstract = {We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Logic in Computer Science},
  file = {/Users/nobr/Zotero/storage/QA9VNARV/Azerbayev et al. - 2023 - Llemma An Open Language Model For Mathematics.pdf}
}

@misc{ba2016a,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  urldate = {2024-11-21},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/T4ESSDGS/Ba et al. - 2016 - Layer Normalization.pdf;/Users/nobr/Zotero/storage/33ERHZUA/1607.html}
}

@misc{baekResearchAgentIterativeResearch2024,
  title = {{{ResearchAgent}}: {{Iterative Research Idea Generation}} over {{Scientific Literature}} with {{Large Language Models}}},
  shorttitle = {{{ResearchAgent}}},
  author = {Baek, Jinheon and Jauhar, Sujay Kumar and Cucerzan, Silviu and Hwang, Sung Ju},
  year = {2024},
  month = apr,
  number = {arXiv:2404.07738},
  eprint = {2404.07738},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-22},
  abstract = {Scientific Research, vital for improving human life, is hindered by its inherent complexity, slow pace, and the need for specialized experts. To enhance its productivity, we propose a ResearchAgent, a large language modelpowered research idea writing agent, which automatically generates problems, methods, and experiment designs while iteratively refining them based on scientific literature. Specifically, starting with a core paper as the primary focus to generate ideas, our ResearchAgent is augmented not only with relevant publications through connecting information over an academic graph but also entities retrieved from an entity-centric knowledge store based on their underlying concepts, mined and shared across numerous papers. In addition, mirroring the human approach to iteratively improving ideas with peer discussions, we leverage multiple ReviewingAgents that provide reviews and feedback iteratively. Further, they are instantiated with human preference-aligned large language models whose criteria for evaluation are derived from actual human judgments. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showcasing its effectiveness in generating novel, clear, and valid research ideas based on human and model-based evaluation results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/SJCSQW3S/Baek et al. - 2024 - ResearchAgent Iterative Research Idea Generation over Scientific Literature with Large Language Mod.pdf}
}

@misc{bahdanau2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder--decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder--decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/88MHCKB5/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf}
}

@article{bahrick2014,
  title = {Learning to {{Attend Selectively}}: {{The Dual Role}} of {{Intersensory Redundancy The Dynamics}} of {{Selective Attention}}},
  author = {Bahrick, Lorraine E and Lickliter, Robert},
  year = {2014},
  journal = {Curr Dir Psychol Sci},
  volume = {23},
  number = {6},
  pages = {414--420},
  doi = {10.1177/0963721414549187},
  urldate = {2023-03-28},
  abstract = {Selective attention is the gateway to perceptual processing, learning, and memory, and is a skill honed through extensive experience. However, little research has focused on how selective attention develops. Here we synthesize established and new findings assessing the central role of redundancy across the senses in guiding and constraining this process in infancy and early childhood. We highlight research demonstrating the dual role of intersensory redundancy-its facilitating and interfering effects-on detection and perceptual processing of various properties of objects and events. Keywords selective attention; development of perception; intersensory redundancy; attentional salience The environment provides a flux of changing, concurrent stimulation to all our senses, far more than can be attended at any given moment in time. Consequently, we must selectively attend to some aspects of objects and events while ignoring others. Adults are highly skilled at directing selective attention to information that is relevant to their needs, goals, and interests, while ignoring a vast array of irrelevant stimulation. For example, we easily pick out a friend in a crowd, follow the flow of action in a ball game, and attend to the face and voice of a single speaker in the context of competing conversations. These attention skills, however, must be learned and honed through experience and practice. Much of this learning takes place in early development. Infants quickly learn to intercoordinate their patterns of looking and listening to determine which sights and sounds belong together and which do not. They learn to parse the visual array into coherent objects and speech into meaningful words by attending to invariant patterns across variation in input. Such selective attention is widely recognized as the gateway to successful information pickup and processing (Neisser, 1976).},
  file = {/Users/nobr/Zotero/storage/VI6BIQXK/full-text.pdf}
}

@misc{bai2023,
  title = {Evolutionary {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Evolutionary {{Reinforcement Learning}}},
  author = {Bai, Hui and Cheng, Ran and Jin, Yaochu},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  doi = {10.34133/icomputing.0025},
  urldate = {2024-09-05},
  abstract = {Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC into RL, referred to as evolutionary reinforcement learning (EvoRL). We categorize EvoRL methods according to key research fields in RL, including hyperparameter optimization, policy search, exploration, reward shaping, meta-RL, and multi-objective RL. We then discuss future research directions in terms of efficient methods, benchmarks, and scalable platforms. This survey serves as a resource for researchers and practitioners interested in the field of EvoRL, highlighting the important challenges and opportunities for future research. With the help of this survey, researchers and practitioners can develop more efficient methods and tailored benchmarks for EvoRL, further advancing this promising cross-disciplinary research field.},
  howpublished = {https://arxiv.org/abs/2303.04150v4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/8CR59TZC/Bai et al. - 2023 - Evolutionary Reinforcement Learning A Survey.pdf}
}

@book{bak2010,
  title = {Complex Analysis},
  author = {Bak, Joseph and Newman, Donald J.},
  year = {2010},
  series = {Undergraduate Texts in Mathematics},
  edition = {3rd ed},
  publisher = {Springer},
  address = {New York},
  isbn = {978-1-4419-7288-0},
  langid = {english},
  lccn = {515.9},
  file = {/Users/nobr/Zotero/storage/AX7FJW66/Bak and Newman - 2010 - Complex analysis.pdf}
}

@article{baker2014,
  title = {Stem Cells Made by Cloning Adult Humans},
  author = {Baker, Monya},
  year = {2014},
  month = apr,
  journal = {Nature},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature.2014.15107},
  urldate = {2023-11-22},
  abstract = {Cell lines made by two separate teams could boost the prospects of patient-specific therapies.},
  copyright = {2014 Springer Nature Limited},
  langid = {english},
  keywords = {Biological techniques,Stem cells},
  file = {/Users/nobr/Zotero/storage/Y7LG858Z/Baker - 2014 - Stem cells made by cloning adult humans.pdf;/Users/nobr/Zotero/storage/TU98N98Z/nature.2014.html}
}

@misc{bakerEmergentToolUse2020,
  title = {Emergent {{Tool Use From Multi-Agent Autocurricula}}},
  author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
  year = {2020},
  month = feb,
  number = {arXiv:1909.07528},
  eprint = {1909.07528},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.07528},
  urldate = {2024-01-13},
  abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/D7AXDT9H/Baker et al. - 2020 - Emergent Tool Use From Multi-Agent Autocurricula.pdf;/Users/nobr/Zotero/storage/IWZFWXJN/1909.html}
}

@misc{bakhtin2021,
  title = {No-{{Press Diplomacy}} from {{Scratch}}},
  author = {Bakhtin, Anton and Wu, David and Lerer, Adam and Brown, Noam},
  year = {2021},
  month = oct,
  number = {arXiv:2110.02924},
  eprint = {2110.02924},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.02924},
  urldate = {2024-01-13},
  abstract = {Prior AI successes in complex games have largely focused on settings with at most hundreds of actions at each decision point. In contrast, Diplomacy is a game with more than 10{\textasciicircum}20 possible actions per turn. Previous attempts to address games with large branching factors, such as Diplomacy, StarCraft, and Dota, used human data to bootstrap the policy or used handcrafted reward shaping. In this paper, we describe an algorithm for action exploration and equilibrium approximation in games with combinatorial action spaces. This algorithm simultaneously performs value iteration while learning a policy proposal network. A double oracle step is used to explore additional actions to add to the policy proposals. At each state, the target state value and policy for the model training are computed via an equilibrium search procedure. Using this algorithm, we train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. Additionally, we extend our methods to full-scale no-press Diplomacy and for the first time train an agent from scratch with no human data. We present evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents. This presents the first strong evidence of multiple equilibria in Diplomacy and suggests that self play alone may be insufficient for achieving superhuman performance in Diplomacy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/PHFXITS5/Bakhtin et al. - 2021 - No-Press Diplomacy from Scratch.pdf;/Users/nobr/Zotero/storage/UAU96DHU/2110.html}
}

@misc{bakhtin2022,
  title = {Mastering the {{Game}} of {{No-Press Diplomacy}} via {{Human-Regularized Reinforcement Learning}} and {{Planning}}},
  author = {Bakhtin, Anton and Wu, David J. and Lerer, Adam and Gray, Jonathan and Jacob, Athul Paul and Farina, Gabriele and Miller, Alexander H. and Brown, Noam},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05492},
  eprint = {2210.05492},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-06},
  abstract = {No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitationlearned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/5334TX62/Bakhtin et al. - 2022 - Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning.pdf}
}

@misc{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  urldate = {2024-06-08},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/R55VSKFU/Ba et al. - 2016 - Layer Normalization.pdf;/Users/nobr/Zotero/storage/DT4J5HNU/1607.html}
}

@article{Ballantyne2012,
  title = {Increase in Observed Net Carbon Dioxide Uptake by Land and Oceans during the Past 50 Years},
  author = {Ballantyne, Ashley P. and Alden, Christopher B. and Miller, John B. and Tans, Pieter P. and White, James W. C.},
  year = {2012},
  journal = {Nature},
  volume = {488},
  number = {7409},
  pages = {70--72},
  doi = {10.1038/nature11299}
}

@techreport{bandyopadhyay2017,
  title = {"{{Who Mentions Whom}}?"-{{Understanding}} the {{Psycho-Sociological Aspects}} of {{Twitter Mention Network}}},
  author = {Bandyopadhyay, S and Sharma, D S and Sangal, R},
  year = {2017},
  journal = {NLP Association of India},
  pages = {418--426},
  institution = {NLPAI},
  abstract = {Users in social network either unicast or broadcast their messages. At mention is the popular way of unicasting for Twitter whereas, general tweeting could be considered as broadcasting method. Understanding the information flow and dynamics within a Social Network and modeling the same is a promising and an open research area called Information Diffusion. This paper seeks an answer to a fundamental question-whether the at-mention or the uni-casting pattern in social media is purely random in nature or there is any user specific selectional preference? To answer the question we present an empirical analysis to understand the psycho-sociological aspects of Twitter mentions network within a social network community. To understand the psychological pattern we have analyzed personality (Big5 model: Openness, Conscientious-ness, Extraversion, Agreeableness, Neu-roticism) of users and to understand the the sociological behavior we analyze values (Schwartz model:Achievement, Benevolence , Conformity, Hedonism, Power, Security , Self-Direction, Stimulation, Traditional , and Universalism) of all the users inside a community. Empirical results suggest that personality and values traits are indeed salient cues to understand how the mention-based communication network functions. For example, we notice that achievement-oriented communities talk to each other more often than other people. We also observe that neurotic people are more involved in communication within their community.},
  file = {/Users/nobr/Zotero/storage/BMCE8C5B/Bandyopadhyay et al. - 2017 - Who Mentions Whom-Understanding the Psycho-Sociological Aspects of Twitter Mention Network.pdf}
}

@article{banerjee2023,
  title = {Reinforcement {{Learning}} as a {{Rehearsal}} for {{Planning}} in {{Air Battle Management}} ({{RLAR}})},
  author = {Banerjee, Bikramjit},
  year = {2023},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4JKCZ9L2/Banerjee - Reinforcement Learning as a Rehearsal for Planning in Air Battle Management (RLAR).pdf}
}

@misc{bangkok_post2019,
  title = {Two-Wheelers Power India's {{EV}} Revolution},
  author = {{Bangkok Post}},
  year = {2019},
  month = jan
}

@article{bansi-matharu2023,
  title = {Cost-Effectiveness of Voluntary Medical Male Circumcision for {{HIV}} Prevention across Sub-{{Saharan Africa}}: Results from Five Independent Models},
  shorttitle = {Cost-Effectiveness of Voluntary Medical Male Circumcision for {{HIV}} Prevention across Sub-{{Saharan Africa}}},
  author = {{Bansi-Matharu}, Loveleen and Mudimu, Edinah and {Martin-Hughes}, Rowan and Hamilton, Matt and Johnson, Leigh and Ten Brink, Debra and Stover, John and {Meyer-Rath}, Gesine and Kelly, Sherrie L and Jamieson, Lise and Cambiano, Valentina and Jahn, Andreas and Cowan, Frances M and Mangenah, Collin and Mavhu, Webster and Chidarikire, Thato and Toledo, Carlos and Revill, Paul and Sundaram, Maaya and Hatzold, Karin and Yansaneh, Aisha and Apollo, Tsitsi and Kalua, Thoko and Mugurungi, Owen and Kiggundu, Valerian and Zhang, Shufang and Nyirenda, Rose and Phillips, Andrew and Kripke, Katharine and Bershteyn, Anna},
  year = {2023},
  month = feb,
  journal = {The Lancet Global Health},
  volume = {11},
  number = {2},
  pages = {e244-e255},
  issn = {2214109X},
  doi = {10.1016/S2214-109X(22)00515-0},
  urldate = {2025-03-03},
  abstract = {Background Voluntary medical male circumcision (VMMC) has been a recommended HIV prevention strategy in sub-Saharan Africa since 2007, particularly in countries with high HIV prevalence. However, given the scale-up of antiretroviral therapy programmes, it is not clear whether VMMC still represents a cost-effective use of scarce HIV programme resources.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/M7U8UQJX/Bansi-Matharu et al. - 2023 - Cost-effectiveness of voluntary medical male circumcision for HIV prevention across sub-Saharan Afri.pdf}
}

@book{banzhaf2015,
  title = {Artificial Chemistries},
  author = {Banzhaf, Wolfgang and Yamamoto, Lidia},
  year = {2015},
  publisher = {The MIT Press},
  address = {Cambridge, MA},
  isbn = {978-0-262-02943-8},
  langid = {english},
  lccn = {QD415 .B24 2015},
  keywords = {Biochemistry,Chemistry Physical and theoretical,Evolution (Biology),Life,Molecular evolution,Origin},
  file = {/Users/nobr/Zotero/storage/VGPM94PZ/Banzhaf and Yamamoto - 2015 - Artificial chemistries.pdf}
}

@misc{baozhouAttentionModuleConvolutional2021,
  title = {An {{Attention Module}} for {{Convolutional Neural Networks}}},
  author = {Baozhou, Zhu and Hofstee, Peter and Lee, Jinho and {Al-Ars}, Zaid},
  year = {2021},
  month = aug,
  number = {arXiv:2108.08205},
  eprint = {2108.08205},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-05},
  abstract = {Attention mechanism has been regarded as an advanced technique to capture long-range feature interactions and to boost the representation capability for convolutional neural networks. However, we found two ignored problems in current attentional activations-based models: the approximation problem and the insufficient capacity problem of the attention maps. To solve the two problems together, we initially propose an attention module for convolutional neural networks by developing an AW-convolution, where the shape of attention maps matches that of the weights rather than the activations. Our proposed attention module is a complementary method to previous attention-based schemes, such as those that apply the attention mechanism to explore the relationship between channel-wise and spatial features. Experiments on several datasets for image classification and object detection tasks show the effectiveness of our proposed attention module. In particular, our proposed attention module achieves 1.00\% Top-1 accuracy improvement on ImageNet classification over a ResNet101 baseline and 0.63 COCO-style Average Precision improvement on the COCO object detection on top of a Faster R-CNN baseline with the backbone of ResNet101-FPN. When integrating with the previous attentional activations-based models, our proposed attention module can further increase their Top-1 accuracy on ImageNet classification by up to 0.57\% and COCO-style Average Precision on the COCO object detection by up to 0.45. Code and pre-trained models will be publicly available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/PZLZ46DF/Baozhou et al. - 2021 - An Attention Module for Convolutional Neural Networks.pdf;/Users/nobr/Zotero/storage/PSLBYHAK/2108.html}
}

@article{barrett2018,
  title = {Measuring Abstract Reasoning in Neural Networks},
  author = {Barrett, David G T and Hill, Felix and Santoro, Adam and Morcos, Ari S and Lillicrap, Timothy},
  year = {2018},
  abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearlydefined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TVQD3J83/Barrett et al. - Measuring abstract reasoning in neural networks.pdf}
}

@article{barthelemy2011,
  title = {Spatial Networks},
  author = {Barth{\'e}lemy, Marc},
  year = {2011},
  month = feb,
  journal = {Physics Reports},
  volume = {499},
  number = {1-3},
  pages = {1--101},
  issn = {03701573},
  doi = {10.1016/j.physrep.2010.11.002},
  urldate = {2023-07-06},
  abstract = {Complex systems are very often organized under the form of networks where nodes and edges are embedded in space. Transportation and mobility networks, Internet, mobile phone networks, power grids, social and contact networks, and neural networks, are all examples where space is relevant and where topology alone does not contain all the information. Characterizing and understanding the structure and the evolution of spatial networks is thus crucial for many different fields, ranging from urbanism to epidemiology. An important consequence of space on networks is that there is a cost associated with the length of edges which in turn has dramatic effects on the topological structure of these networks. We will thoroughly explain the current state of our understanding of how the spatial constraints affect the structure and properties of these networks. We will review the most recent empirical observations and the most important models of spatial networks. We will also discuss various processes which take place on these spatial networks, such as phase transitions, random walks, synchronization, navigation, resilience, and disease spread.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JWRNRH3G/Barthélemy - 2011 - Spatial networks.pdf}
}

@misc{battaglia2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = oct,
  number = {arXiv:1806.01261},
  eprint = {1806.01261},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences---a hallmark of human intelligence from infancy---remains a formidable challenge for modern AI.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/MZAXUT4X/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/Users/nobr/Zotero/storage/7R68FY9Q/1806.html;/Users/nobr/Zotero/storage/NFCNMUIM/1806.html}
}

@misc{battaglia2018a,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = oct,
  number = {arXiv:1806.01261},
  eprint = {1806.01261},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.01261},
  urldate = {2024-12-24},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/55HWSW76/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and graph networks.pdf}
}

@article{batzner2022,
  title = {E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials},
  author = {Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan P. and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess E. and Kozinsky, Boris},
  year = {2022},
  month = may,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {2453},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-29939-5},
  urldate = {2024-12-27},
  abstract = {This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Atomistic models,Computational chemistry,Computational methods,Computer science,Molecular dynamics},
  file = {/Users/nobr/Zotero/storage/92AMG4FC/Batzner et al. - 2022 - E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials.pdf}
}

@misc{baxter2011,
  title = {A {{Model}} of {{Inductive Bias Learning}}},
  author = {Baxter, J.},
  year = {2011},
  month = jun,
  number = {arXiv:1106.0245},
  eprint = {1106.0245},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1106.0245},
  urldate = {2024-11-20},
  abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/F2WSHK5F/Baxter - 2011 - A Model of Inductive Bias Learning.pdf;/Users/nobr/Zotero/storage/FE692KDH/1106.html}
}

@misc{beck2024,
  title = {Grokking at the {{Edge}} of {{Linear Separability}}},
  author = {Beck, Alon and Levi, Noam and {Bar-Sinai}, Yohai},
  year = {2024},
  month = oct,
  number = {arXiv:2410.04489},
  eprint = {2410.04489},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.04489},
  urldate = {2024-10-11},
  abstract = {We study the generalization properties of binary logistic classification in a simplified setting, for which a "memorizing" and "generalizing" solution can always be strictly defined, and elucidate empirically and analytically the mechanism underlying Grokking in its dynamics. We analyze the asymptotic long-time dynamics of logistic classification on a random feature model with a constant label and show that it exhibits Grokking, in the sense of delayed generalization and non-monotonic test loss. We find that Grokking is amplified when classification is applied to training sets which are on the verge of linear separability. Even though a perfect generalizing solution always exists, we prove the implicit bias of the logisitc loss will cause the model to overfit if the training data is linearly separable from the origin. For training sets that are not separable from the origin, the model will always generalize perfectly asymptotically, but overfitting may occur at early stages of training. Importantly, in the vicinity of the transition, that is, for training sets that are almost separable from the origin, the model may overfit for arbitrarily long times before generalizing. We gain more insights by examining a tractable one-dimensional toy model that quantitatively captures the key features of the full model. Finally, we highlight intriguing common properties of our findings with recent literature, suggesting that grokking generally occurs in proximity to the interpolation threshold, reminiscent of critical phenomena often observed in physical systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Mathematical Physics,Mathematics - Mathematical Physics,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/APKVRQMT/Beck et al. - 2024 - Grokking at the Edge of Linear Separability.pdf;/Users/nobr/Zotero/storage/A4MAAQUE/2410.html}
}

@article{belcak2023,
  title = {Examining the {{Emergence}} of {{Deductive Reasoning}} in {{Generative Language Models}}},
  author = {Belcak, Peter and Lanzend{\"o}rfer, Luca A},
  year = {2023},
  abstract = {We conduct a preliminary inquiry into the ability of generative transformer models to deductively reason from premises provided. We observe notable differences in the performance of models coming from different training setups and find that the deductive reasoning ability increases with scale. Further, we discover that the performance generally does not decrease with the length of the deductive chain needed to reach the conclusion, with the exception of OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of transformer-decoder models, ranging from 117 million to 175 billion parameters in size.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/WWHRX8PU/Belcak and Lanzendörfer - Examining the Emergence of Deductive Reasoning in .pdf}
}

@misc{belcakFACTLearningGoverning2022,
  title = {{{FACT}}: {{Learning Governing Abstractions Behind Integer Sequences}}},
  shorttitle = {{{FACT}}},
  author = {Belc{\'a}k, Peter and Kastrati, Ard and Schenker, Flavio and Wattenhofer, Roger},
  year = {2022},
  month = sep,
  number = {arXiv:2209.09543},
  eprint = {2209.09543},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-21},
  abstract = {Integer sequences are of central importance to the modeling of concepts admitting complete finitary descriptions. We introduce a novel view on the learning of such concepts and lay down a set of benchmarking tasks aimed at conceptual understanding by machine learning models. These tasks indirectly assess model ability to abstract, and challenge them to reason both interpolatively and extrapolatively from the knowledge gained by observing representative examples. To further aid research in knowledge representation and reasoning, we present FACT, the Finitary Abstraction Comprehension Toolkit. The toolkit surrounds a large dataset of integer sequences comprising both organic and synthetic entries, a library for data pre-processing and generation, a set of model performance evaluation tools, and a collection of baseline model implementations, enabling the making of the future advancements with ease.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  file = {/Users/nobr/Zotero/storage/WZRB7FI7/Belcák et al. - 2022 - FACT Learning Governing Abstractions Behind Integer Sequences.pdf;/Users/nobr/Zotero/storage/9P5WILPS/2209.html}
}

@article{bellini2020,
  title = {Low {{FODMAP Diet}}: {{Evidence}}, {{Doubts}}, and {{Hopes}}},
  shorttitle = {Low {{FODMAP Diet}}},
  author = {Bellini, Massimo and Tonarelli, Sara and Nagy, Attila G. and Pancetti, Andrea and Costa, Francesco and Ricchiuti, Angelo and {de Bortoli}, Nicola and Mosca, Marta and Marchi, Santino and Rossi, Alessandra},
  year = {2020},
  month = jan,
  journal = {Nutrients},
  volume = {12},
  number = {1},
  pages = {148},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-6643},
  doi = {10.3390/nu12010148},
  urldate = {2023-05-28},
  abstract = {Food is often considered to be a precipitating factor of irritable bowel syndrome (IBS) symptoms. In recent years, there has been a growing interest in FODMAPs (Fermentable Oligo-, Di-, Mono-saccharides, And Polyols), which can be found in many common foods. A low FODMAP diet (LFD) is increasingly suggested for IBS treatment. However, long-term, large, randomized controlled studies are still lacking, and certainties and doubts regarding LFDs have grown, often in a disorderly and confused manner. Some potential LFD limitations and concerns have been raised, including nutritional adequacy, cost, and difficulty in teaching the diet and maintaining it. Most of these limitations can be solved with the involvement of a skilled nutritionist, who can clearly explain the different phases of the LFD and ensure nutritional adequacy and compliance. Further studies should focus on new methods of teaching and learning the LFD and on predictors of response. Moreover, particular interest should be focused on the possible use of LFD in gastrointestinal diseases other than functional disorders and, possibly, also in non-gastrointestinal diseases. The aim of the present review was to clarify the effective and appropriate indications and limitations of an LFD and to discuss its possible future uses.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {gut microbiota,irritable bowel syndrome,low FODMAP diet,nutrition},
  file = {/Users/nobr/Zotero/storage/J4RBWPC2/Bellini et al. - 2020 - Low FODMAP Diet Evidence, Doubts, and Hopes.pdf}
}

@misc{belloAttentionAugmentedConvolutional2020,
  title = {Attention {{Augmented Convolutional Networks}}},
  author = {Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V.},
  year = {2020},
  month = sep,
  number = {arXiv:1904.09925},
  eprint = {1904.09925},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.09925},
  urldate = {2023-11-16},
  abstract = {Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classification. We find in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a \$1.3{\textbackslash}\%\$ top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/KPYP6FUZ/Bello et al. - 2020 - Attention Augmented Convolutional Networks.pdf;/Users/nobr/Zotero/storage/JAZD87RA/1904.html}
}

@article{beltagy2020,
  title = {Longformer: {{The Long-Document Transformer}}},
  author = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  year = {2020},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RZG5QBIN/Beltagy et al. - Longformer The Long-Document Transformer.pdf}
}

@misc{benchetrit2023,
  title = {Brain Decoding: Toward Real-Time Reconstruction of Visual Perception},
  shorttitle = {Brain Decoding},
  author = {Benchetrit, Yohann and Banville, Hubert and King, Jean-R{\'e}mi},
  year = {2023},
  month = oct,
  number = {arXiv:2310.19812},
  eprint = {2310.19812},
  primaryclass = {cs, eess, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.19812},
  urldate = {2023-11-09},
  abstract = {In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution (\${\textbackslash}approx\$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution (\${\textbackslash}approx\$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain responses to images are best decoded with DINOv2, a recent foundational image model. Third, image retrievals and generations both suggest that MEG signals primarily contain high-level visual features, whereas the same approach applied to 7T fMRI also recovers low-level features. Overall, these results provide an important step towards the decoding - in real time - of the visual processes continuously unfolding within the human brain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/RTHXQ9XF/Benchetrit et al. - 2023 - Brain decoding toward real-time reconstruction of visual perception.pdf;/Users/nobr/Zotero/storage/NCI68XV3/2310.html}
}

@article{bennett1973,
  title = {Logical {{Reversibility}} of {{Computation}}},
  author = {Bennett, C. H.},
  year = {1973},
  month = nov,
  journal = {IBM Journal of Research and Development},
  volume = {17},
  number = {6},
  pages = {525--532},
  issn = {0018-8646, 0018-8646},
  doi = {10.1147/rd.176.0525},
  urldate = {2025-08-02},
  abstract = {The usual general-purpose computing automaton (e.g.. a Turing machine) is logically irreversible- its transition function lacks a single-valued inverse. Here it is shown that such machines may he made logically reversible at every step, while retainillg their simplicity and their ability to do general computations. This result is of great physical interest because it makes plausible the existence of thermodynamically reversible computers which could perform useful computations at useful speed while dissipating considerably lessthan kT of energyper logical step. In the first stage of itscomputationthe logically reversibleautomatonparallels the corresponding irreversible automaton, except that it saves all intermediate results, thereby avoiding the irreversible operation of erasure. The second stage consistsof printing out the desired output. Thethird stage then reversibly disposes of all the undesired intermediate results by retracing the steps of the first stage in backward order (a process which is only possible because the first stage has been carried out reversibly), thereby restoring the machine (except for the now-written output tape) toits original condition. The final machine configuration thus contains the desired output anda reconstructed copy o f the input, but no other undesired data. The foregoing results are demonstrated explicitly using a type of three-tape Turing machine. The biosynthesis of messenger RNA is discussed as a physical example of reversible computation.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3IR4XUVK/Bennett - 1973 - Logical Reversibility of Computation.pdf}
}

@article{bentley1986,
  title = {Programming Pearls: Little Languages},
  shorttitle = {Programming Pearls},
  author = {Bentley, Jon},
  year = {1986},
  month = aug,
  journal = {Communications of the ACM},
  volume = {29},
  number = {8},
  pages = {711--721},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/6424.315691},
  urldate = {2024-07-21},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4ZK8SV7D/Bentley - 1986 - Programming pearls little languages.pdf}
}

@misc{bereska2024,
  title = {Mechanistic {{Interpretability}} for {{AI Safety}} -- {{A Review}}},
  author = {Bereska, Leonard and Gavves, Efstratios},
  year = {2024},
  month = apr,
  number = {arXiv:2404.14082},
  eprint = {2404.14082},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/574QS6MP/Bereska and Gavves - 2024 - Mechanistic Interpretability for AI Safety -- A Review.pdf;/Users/nobr/Zotero/storage/JHL2IECC/2404.html}
}

@inproceedings{bergsma2013,
  title = {Using {{Conceptual Class Attributes}} to {{Characterize Social Media Users}}},
  booktitle = {Proceedings of the 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Bergsma, Shane and Van Durme, Benjamin},
  year = {2013},
  month = aug,
  pages = {710--720},
  publisher = {Association for Computational Linguistics},
  address = {Sofia, Bulgaria},
  urldate = {2023-06-11},
  file = {/Users/nobr/Zotero/storage/259YJPBV/Bergsma and Van Durme - 2013 - Using Conceptual Class Attributes to Characterize .pdf}
}

@misc{bernerDota2Large2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k e}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  month = dec,
  number = {arXiv:1912.06680},
  eprint = {1912.06680},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.06680},
  urldate = {2024-01-13},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/V2B3X73T/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/WS2XR73G/1912.html}
}

@inproceedings{bernstein2013,
  title = {Elligator: Elliptic-Curve Points Indistinguishable from Uniform Random Strings},
  shorttitle = {Elligator},
  booktitle = {Proceedings of the 2013 {{ACM SIGSAC}} Conference on {{Computer}} \& Communications Security - {{CCS}} '13},
  author = {Bernstein, Daniel J. and Hamburg, Mike and Krasnova, Anna and Lange, Tanja},
  year = {2013},
  pages = {967--980},
  publisher = {ACM Press},
  address = {Berlin, Germany},
  doi = {10.1145/2508859.2516734},
  urldate = {2024-07-21},
  abstract = {Censorship-circumvention tools are in an arms race against censors. The censors study all traffic passing into and out of their controlled sphere, and try to disable censorshipcircumvention tools without completely shutting down the Internet. Tools aim to shape their traffic patterns to match unblocked programs, so that simple traffic profiling cannot identify the tools within a reasonable number of traces; the censors respond by deploying firewalls with increasingly sophisticated deep-packet inspection.},
  isbn = {978-1-4503-2477-9},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/EEJMKDLF/2508859.2516734.pdf}
}

@incollection{berrou2010,
  title = {Convolutional Codes and Their Decoding},
  booktitle = {Codes and {{Turbo Codes}}},
  editor = {Berrou, Claude},
  year = {2010},
  series = {Collection {{IRIS}}},
  pages = {167--199},
  publisher = {Springer},
  address = {Paris},
  doi = {10.1007/978-2-8178-0039-4_5},
  urldate = {2023-12-11},
  abstract = {It was in 1955 that Peter Elias introduced the notion of convolutional code [5.5]. The example of an encoder described is illustrated in Figure 5.1. It is a systematic encoder, that is, the coded message contains the message to be transmitted, to which redundant information is added. The message is of infinite length, which at first sight limits the field of application of this type of code. It is however easy to adapt it for packet transmissions thanks to tail-biting techniques.},
  isbn = {978-2-8178-0039-4},
  langid = {english}
}

@article{bertolini2021,
  title = {Machine {{Learning}} for Industrial Applications: {{A}} Comprehensive Literature Review},
  shorttitle = {Machine {{Learning}} for Industrial Applications},
  author = {Bertolini, Massimo and Mezzogori, Davide and Neroni, Mattia and Zammori, Francesco},
  year = {2021},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {175},
  pages = {114820},
  issn = {09574174},
  doi = {10.1016/j.eswa.2021.114820},
  urldate = {2023-11-12},
  abstract = {Machine Learning (ML) is a branch of artificial intelligence that studies algorithms able to learn autonomously, directly from the input data. Over the last decade, ML techniques have made a huge leap forward, as demon\- strated by Deep Learning (DL) algorithms implemented by autonomous driving cars, or by electronic strategy games. Hence, researchers have started to consider ML also for applications within the industrial field, and many works indicate ML as one the main enablers to evolve a traditional manufacturing system up to the Industry 4.0 level. Nonetheless, industrial applications are still few and limited to a small cluster of international companies. This paper deals with these topics, intending to clarify the real potentialities, as well as potential flaws, of ML algorithms applied to operation management. A comprehensive review is presented and organized in a way that should facilitate the orientation of practitioners in this field. To this aim, papers from 2000 to date are cate\- gorized in terms of the applied algorithm and application domain, and a keyword analysis is also performed, to details the most promising topics in the field. What emerges is a consistent upward trend in the number of publications, with a spike of interest for unsupervised and especially deep learning techniques, which recorded a very high number of publications in the last five years. Concerning trends, along with consolidated research areas, recent topics that are growing in popularity were also discovered. Among these, the main ones are pro\- duction planning and control and defect analysis, thus suggesting that in the years to come ML will become pervasive in many fields of operation management.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4ERIXJVA/Bertolini et al. - 2021 - Machine Learning for industrial applications A comprehensive literature review.pdf}
}

@article{berzsenyi2019,
  title = {Talent {{Search}} versus {{Talent Development}}},
  author = {Berzsenyi, George},
  year = {2019},
  month = oct,
  journal = {Notices of the American Mathematical Society},
  volume = {66},
  number = {09},
  pages = {1},
  issn = {0002-9920, 1088-9477},
  doi = {10.1090/noti1950},
  urldate = {2024-09-05},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/848QGWC3/rnoti-p1471.pdf}
}

@article{betzel2018,
  title = {Diversity of Meso-Scale Architecture in Human and Non-Human Connectomes},
  author = {Betzel, Richard F. and Medaglia, John D. and Bassett, Danielle S.},
  year = {2018},
  month = jan,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {346},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-02681-z},
  urldate = {2023-05-16},
  abstract = {Brain function is reflected in connectome community structure. The dominant view is that communities are assortative and segregated from one another, supporting specialized information processing. However, this view precludes the possibility of non-assortative communities whose complex inter-community interactions could engender a richer functional repertoire. We use weighted stochastic blockmodels to uncover the meso-scale architecture of Drosophila, mouse, rat, macaque, and human connectomes. We find that most communities are assortative, though others form core-periphery and disassortative structures, which better recapitulate observed patterns of functional connectivity and gene co-expression in human and mouse connectomes compared to standard community detection techniques. We define measures for quantifying the diversity of communities in which brain regions participate, showing that this measure is peaked in control and subcortical systems in humans, and that inter-individual differences are correlated with cognitive performance. Our report paints a more diverse portrait of connectome communities and demonstrates their cognitive relevance.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Applied mathematics,Network models},
  file = {/Users/nobr/Zotero/storage/KKUUTAF3/Betzel et al. - 2018 - Diversity of meso-scale architecture in human and .pdf}
}

@article{beyer2017,
  title = {Simplify {{Your Covariance Matrix Adaptation Evolution Strategy}}},
  author = {Beyer, Hans-Georg and Sendhoff, Bernhard},
  year = {2017},
  month = oct,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {21},
  number = {5},
  pages = {746--759},
  issn = {1089-778X, 1089-778X, 1941-0026},
  doi = {10.1109/TEVC.2017.2680320},
  urldate = {2023-11-22},
  abstract = {The standard covariance matrix adaptation evolution strategy (CMA-ES) comprises two evolution paths, one for the learning of the mutation strength and one for the rank-1 update of the covariance matrix. In this paper, it is shown that one can approximately transform this algorithm in such a manner that one of the evolution paths and the covariance matrix itself disappear. That is, the covariance update and the covariance matrix square root operations are no longer needed in this novel so-called matrix adaptation (MA) ES. The MA-ES performs nearly as well as the original CMA-ES. This is shown by empirical investigations considering the evolution dynamics and the empirical expected runtime on a set of standard test functions. Furthermore, it is shown that the MA-ES can be used as a search engine in a bi-population (BiPop) ES. The resulting BiPop-MA-ES is benchmarked using the BBOB comparing continuous optimizers (COCO) framework and compared with the performance of the CMA-ES-v3.61 production code. It is shown that this new BiPop-MA-ES---while algorithmically simpler---performs nearly equally well as the CMA-ES-v3.61 code.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/QXL8DBYQ/Beyer and Sendhoff - 2017 - Simplify Your Covariance Matrix Adaptation Evolution Strategy.pdf}
}

@misc{bhadane2022,
  title = {On {{One-Bit Quantization}}},
  author = {Bhadane, Sourbh and Wagner, Aaron B.},
  year = {2022},
  month = feb,
  number = {arXiv:2202.05292},
  eprint = {2202.05292},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.05292},
  urldate = {2023-08-28},
  abstract = {We consider the one-bit quantizer that minimizes the mean squared error for a source living in a real Hilbert space. The optimal quantizer is a projection followed by a thresholding operation, and we provide methods for identifying the optimal direction along which to project. As an application of our methods, we characterize the optimal one-bit quantizer for a continuous-time random process that exhibits low-dimensional structure. We numerically show that this optimal quantizer is found by a neural-network-based compressor trained via stochastic gradient descent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/9VH9NKDA/Bhadane and Wagner - 2022 - On One-Bit Quantization.pdf;/Users/nobr/Zotero/storage/ELCI64YR/2202.html}
}

@inproceedings{bhagavatula2002,
  title = {Low Density Parity Check ({{LDPC}}) Codes for Optical Data Storage},
  booktitle = {International {{Symposium}} on {{Optical Memory}} and {{Optical Data Storage Topical Meeting}}},
  author = {Bhagavatula, V. and {Hongwei Song} and {Jingfeng Liu}},
  year = {2002},
  pages = {371--373},
  publisher = {IEEE},
  address = {Waikoloa, HI, USA},
  doi = {10.1109/OMODS.2002.1028670},
  urldate = {2023-11-07},
  isbn = {978-0-7803-7379-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7T9R9UUG/Bhagavatula et al. - 2002 - Low density parity check (LDPC) codes for optical data storage.pdf}
}

@inproceedings{bhargava2013,
  title = {Stylometric {{Analysis}} for {{Authorship Attribution}} on {{Twitter}}},
  booktitle = {Big {{Data Analytics}}},
  author = {Bhargava, Mudit and Mehndiratta, Pulkit and Asawa, Krishna},
  editor = {Bhatnagar, Vasudha and Srinivasa, Srinath},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {37--47},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-03689-2_3},
  abstract = {Authorship Attribution (AA), the science of inferring an author for a given piece of text based on its characteristics is a problem with a long history. In this paper, we study the problem of authorship attribution for forensic purposes and present machine learning techniques and stylometric features of the authors that enable authorship to be determined at rates significantly better than chance for texts of 140 characters or less. This analysis targets the micro-blogging site Twitter, where people share their interests and thoughts in form of short messages called ''tweets''. Millions of ''tweets'' are posted daily via this service and the possibility of sharing sensitive and illegitimate text cannot be ruled out. The technique discussed in this paper is a two stage process, where in the first stage, stylometric information is extracted from the collected dataset and in the second stage different classification algorithms are trained to predict authors of unseen text. The effort is towards maximizing the accuracy of predictions with optimum amount of data and users under consideration.},
  isbn = {978-3-319-03689-2},
  langid = {english},
  keywords = {Authorship Attribution,Machine Learning Classifier,Online Social Media,Stylometry Analysis,Twitter},
  file = {/Users/nobr/Zotero/storage/I3Y4B6PP/Bhargava et al. - 2013 - Stylometric Analysis for Authorship Attribution on.pdf}
}

@inproceedings{bhattamishra2020,
  title = {On the {{Ability}} and {{Limitations}} of {{Transformers}} to {{Recognize Formal Languages}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  year = {2020},
  month = nov,
  pages = {7096--7116},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.576},
  urldate = {2024-06-02},
  abstract = {Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.},
  file = {/Users/nobr/Zotero/storage/QBUWJADM/Bhattamishra et al. - 2020 - On the Ability and Limitations of Transformers to Recognize Formal Languages.pdf}
}

@article{bhaumik2023,
  title = {{{HyFiNet}}: {{Hybrid}} Feature Attention Network for Hand Gesture Recognition},
  shorttitle = {{{HyFiNet}}},
  author = {Bhaumik, Gopa and Verma, Monu and Govil, Mahesh Chandra and Vipparthi, Santosh Kumar},
  year = {2023},
  month = feb,
  journal = {Multimedia Tools and Applications},
  volume = {82},
  number = {4},
  pages = {4863--4882},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-021-11623-3},
  urldate = {2023-05-10},
  abstract = {In this paper, we propose a portable CNN network: a hybrid feature attention network (HyFiNet) for precise hand gesture recognition. HyFiNet is designed by stacking four multi-scale refined edge extraction modules (REEMs). The REEM module is introduced to capture the refined edge information of hand gestures by incorporating hybrid feature attention (HyAttention) block. The HyAttention block is intended to focus on efficient salient features from multi-receptive fields and acquire knowledge of discriminable semantic structure for hand poses. As a resultant, multi-scale feature and hybrid feature attention mechanisms cohesively improve the performance of hand gesture recognition with a minimum computational cost. The efficiency of the proposed network is validated using six benchmark datasets: MUGD, Finger Spelling, NUS-I, NUS-II, HGR-I and Triesch, by adopting two validation schemes: person dependent and person independent. Furthermore, seven supplementary experiments are also performed for an ablation study to analyze the effectiveness of each module in the proposed network. The experimental results and visual representation indicate a substantial increase in accuracy compared to the existing state-ofthe-art networks.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3H4XVJGC/Bhaumik et al. - 2023 - HyFiNet Hybrid feature attention network for hand.pdf}
}

@article{bielczyk2018,
  title = {Thresholding Functional Connectomes by Means of Mixture Modeling},
  author = {Bielczyk, Natalia Z. and Walocha, Fabian and Ebel, Patrick W. and Haak, Koen V. and Llera, Alberto and Buitelaar, Jan K. and Glennon, Jeffrey C. and Beckmann, Christian F.},
  year = {2018},
  month = may,
  journal = {Neuroimage},
  volume = {171},
  pages = {402--414},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2018.01.003},
  urldate = {2023-05-16},
  abstract = {Functional connectivity has been shown to be a very promising tool for studying the large-scale functional architecture of the human brain. In network research in fMRI, functional connectivity is considered as a set of pair-wise interactions between the nodes of the network. These interactions are typically operationalized through the full or partial correlation between all pairs of regional time series. Estimating the structure of the latent underlying functional connectome from the set of pair-wise partial correlations remains an open research problem though. Typically, this thresholding problem is approached by proportional thresholding, or by means of parametric or non-parametric permutation testing across a cohort of subjects at each possible connection. As an alternative, we propose a data-driven thresholding approach for network matrices on the basis of mixture modeling. This approach allows for creating subject-specific sparse connectomes by modeling the full set of partial correlations as a mixture of low correlation values associated with weak or unreliable edges in the connectome and a sparse set of reliable connections. Consequently, we propose to use alternative thresholding strategy based on the model fit using pseudo-False Discovery Rates derived on the basis of the empirical null estimated as part of the mixture distribution., We evaluate the method on synthetic benchmark fMRI datasets where the underlying network structure is known, and demonstrate that it gives improved performance with respect to the alternative methods for thresholding connectomes, given the canonical thresholding levels. We also demonstrate that mixture modeling gives highly reproducible results when applied to the functional connectomes of the visual system derived from the n-back Working Memory task in the Human Connectome Project. The sparse connectomes obtained from mixture modeling are further discussed in the light of the previous knowledge of the functional architecture of the visual system in humans. We also demonstrate that with use of our method, we are able to extract similar information on the group level as can be achieved with permutation testing even though these two methods are not equivalent. We demonstrate that with both of these methods, we obtain functional decoupling between the two hemispheres in the higher order areas of the visual cortex during visual stimulation as compared to the resting state, which is in line with previous studies suggesting lateralization in the visual processing. However, as opposed to permutation testing, our approach does not require inference at the cohort level and can be used for creating sparse connectomes at the level of a single subject.,                                        {$\bullet$}               Sparse functional connectomes are useful in analyzing and interpreting fMRI data.                                         {$\bullet$}               We propose thresholding by means of mixture modeling and control of FDR.                                         {$\bullet$}               We benchmark the approach on synthetic fMRI data against established methods.                                         {$\bullet$}               We apply the method to the resting state and working memory task datasets from HCP500.                                         {$\bullet$}               Results are reproducible on synthetic data and interpretable on experimental data.},
  pmcid = {PMC5981009},
  pmid = {29309896},
  file = {/Users/nobr/Zotero/storage/D49N3LNS/Bielczyk et al. - 2018 - Thresholding functional connectomes by means of mi.pdf}
}

@article{bin-jumah2022,
  title = {Genes and {{Longevity}} of {{Lifespan}}},
  author = {{Bin-Jumah}, May Nasser and Nadeem, Muhammad Shahid and Gilani, Sadaf Jamal and {Al-Abbasi}, Fahad A. and Ullah, Inam and Alzarea, Sami I. and Ghoneim, Mohammed M. and Alshehri, Sultan and Uddin, Aziz and Murtaza, Bibi Nazia and Kazmi, Imran},
  year = {2022},
  month = jan,
  journal = {International Journal of Molecular Sciences},
  volume = {23},
  number = {3},
  pages = {1499},
  issn = {1422-0067},
  doi = {10.3390/ijms23031499},
  urldate = {2024-12-28},
  abstract = {Aging is a complex process indicated by low energy levels, declined physiological activity, stress induced loss of homeostasis leading to the risk of diseases and mortality. Recent developments in medical sciences and an increased availability of nutritional requirements has significantly increased the average human lifespan worldwide. Several environmental and physiological factors contribute to the aging process. However, about 40\% human life expectancy is inherited among generations, many lifespan associated genes, genetic mechanisms and pathways have been demonstrated during last decades. In the present review, we have evaluated many human genes and their non-human orthologs established for their role in the regulation of lifespan. The study has included more than fifty genes reported in the literature for their contributions to the longevity of life. Intact genomic DNA is essential for the life activities at the level of cell, tissue, and organ. Nucleic acids are vulnerable to oxidative stress, chemotherapies, and exposure to radiations. Efficient DNA repair mechanisms are essential for the maintenance of genomic integrity, damaged DNA is not replicated and transferred to next generations rather the presence of deleterious DNA initiates signaling cascades leading to the cell cycle arrest or apoptosis. DNA modifications, DNA methylation, histone methylation, histone acetylation and DNA damage can eventually lead towards apoptosis. The importance of calorie restriction therapy in the extension of lifespan has also been discussed. The role of pathways involved in the regulation of lifespan such as DAF-16/FOXO (forkhead box protein O1), TOR and JNK pathways has also been particularized. The study provides an updated account of genetic factors associated with the extended lifespan and their interactive contributory role with cellular pathways.},
  pmcid = {PMC8836117},
  pmid = {35163422},
  file = {/Users/nobr/Zotero/storage/YLPAFSHC/Bin-Jumah et al. - 2022 - Genes and Longevity of Lifespan.pdf}
}

@article{binder2024,
  title = {Deriving {{Dependently-Typed OOP}} from {{First Principles}} -- {{Extended Version}} with {{Additional Appendices}}},
  author = {Binder, David and Skupin, Ingo and S{\"u}berkr{\"u}b, Tim and Ostermann, Klaus},
  year = {2024},
  month = apr,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {8},
  number = {OOPSLA1},
  eprint = {2403.06707},
  primaryclass = {cs},
  pages = {983--1009},
  issn = {2475-1421},
  doi = {10.1145/3649846},
  urldate = {2024-06-24},
  abstract = {The expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both. When abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches. The expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers). This difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: Dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored. In this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality. That is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization. Our central contribution is a dependently typed calculus which contains two dual language fragments. We provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization. We have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the phenomenon of duality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/nobr/Zotero/storage/DL7HFMJ3/Binder et al. - 2024 - Deriving Dependently-Typed OOP from First Principles -- Extended Version with Additional Appendices.pdf}
}

@inproceedings{binyan2008,
  title = {A Service-Oriented Air Combat Simulation System},
  booktitle = {2008 {{Asia Simulation Conference}} - 7th {{International Conference}} on {{System Simulation}} and {{Scientific Computing}}},
  author = {{Bin Yan} and {Shuguang Zhang} and {Jinbiao Sun}},
  year = {2008},
  month = oct,
  pages = {192--199},
  publisher = {IEEE},
  address = {Beijing, China},
  doi = {10.1109/ASC-ICSC.2008.4675354},
  urldate = {2024-01-14},
  isbn = {978-1-4244-1786-5},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/C697RQVQ/Bin Yan et al. - 2008 - A service-oriented air combat simulation system.pdf}
}

@misc{birhane2024,
  title = {Large {{Models}} of {{What}}? {{Mistaking Engineering Achievements}} for {{Human Linguistic Agency}}},
  shorttitle = {Large {{Models}} of {{What}}?},
  author = {Birhane, Abeba and McGann, Marek},
  year = {2024},
  month = jul,
  number = {arXiv:2407.08790},
  eprint = {2407.08790},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.08790},
  urldate = {2024-07-21},
  abstract = {In this paper we argue that key, often sensational and misleading, claims regarding linguistic capabilities of Large Language Models (LLMs) are based on at least two unfounded assumptions; the assumption of language completeness and the assumption of data completeness. Language completeness assumes that a distinct and complete thing such as `a natural language' exists, the essential characteristics of which can be effectively and comprehensively modelled by an LLM. The assumption of data completeness relies on the belief that a language can be quantified and wholly captured by data. Work within the enactive approach to cognitive science makes clear that, rather than a distinct and complete thing, language is a means or way of acting. Languaging is not the kind of thing that can admit of a complete or comprehensive modelling. From an enactive perspective we identify three key characteristics of enacted language; embodiment, participation, and precariousness, that are absent in LLMs, and likely incompatible in principle with current architectures. We argue that these absences imply that LLMs are not now and cannot in their present form be linguistic agents the way humans are. We illustrate the point in particular through the phenomenon of `algospeak', a recently described pattern of high stakes human language activity in heavily controlled online environments. On the basis of these points, we conclude that sensational and misleading claims about LLM agency and capabilities emerge from a deep misconception of both what human language is and what LLMs are.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/nobr/Zotero/storage/RQAYKRFN/Birhane and McGann - 2024 - Large Models of What Mistaking Engineering Achievements for Human Linguistic Agency.pdf;/Users/nobr/Zotero/storage/LNUNSN9Q/2407.html}
}

@book{bishop2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/Users/nobr/Zotero/storage/6VEBDY5X/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@book{bishop2024,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  year = {2024},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  urldate = {2023-12-18},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BEM3U3EG/Bishop and Bishop - 2024 - Deep Learning Foundations and Concepts.pdf}
}

@article{biswas2020,
  title = {Geospatial {{Clustering}} for {{Balanced}} and {{Proximal Schools}}},
  author = {Biswas, Subhodip and Chen, Fanglan and Sistrunk, Andreea and Muthiah, Sathappan and Chen, Zhiqian and Self, Nathan and Lu, Chang-Tien and Ramakrishnan, Naren},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {09},
  pages = {13358--13365},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i09.7058},
  urldate = {2023-07-04},
  abstract = {Public school boundaries are redrawn from time to time to ensure effective functioning of school systems. This process, also called school redistricting, is non-trivial due to (1) the presence of multiple design criteria such as capacity utilization, proximity and travel time which are hard for planners to consider simultaneously, (2) the fixed locations of schools with widely differing capacities that need to be balanced, (3) the spatial nature of the data and the need to preserve contiguity in school zones, and (4) the difficulty in quantifying local factors that may arise. Motivated by these challenges and the intricacy of the process, we propose a geospatial clustering algorithm called GeoKmeans for assisting planners in designing school boundaries such that students are assigned to proximal schools while ensuring effective utilization of school capacities. The algorithm operates on polygonal geometries and connects them into geographically contiguous school boundaries while balancing problem-specific constraints. We evaluate our approach on real-world data of two rapidly growing school districts in the US. Results indicate the efficacy of our approach in designing boundaries. Additionally, a case study is included to demonstrate the potential of GeoKmeans to assist planners in drawing boundaries.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IMDRBL82/Biswas et al. - 2020 - Geospatial Clustering for Balanced and Proximal Sc.pdf}
}

@inproceedings{black2023,
  title = {Scaling Intelligent Agent Combat Behaviors through Hierarchical Reinforcement Learning},
  booktitle = {Disruptive {{Technologies}} in {{Information Sciences VII}}},
  author = {Black, Scotty and Darken, Christian J.},
  editor = {Wysocki, Bryant T. and Holt, James and Blowers, Misty},
  year = {2023},
  month = jun,
  pages = {36},
  publisher = {SPIE},
  address = {Orlando, United States},
  doi = {10.1117/12.2679843},
  urldate = {2024-01-22},
  abstract = {Remaining competitive in future conflicts with technologically-advanced competitors requires us to continue to invest in developing robust artificial intelligence (AI) for wargaming. Although deep reinforcement learning (RL) continues to show promising results in intelligent agent behavior development, it has yet to perform at or above the human level in the long-horizon, complex tasks typically found in combat modeling and simulation. Capitalizing on the proven potential of RL and recent successes of hierarchical reinforcement learning (HRL), our research aims to extend the use of HRL to create intelligent agents capable of performing effectively in these large and complex simulation environments. We plan to do so by developing a scalable HRL agent architecture and training framework, developing a dimension-invariant dynamic abstraction engine, and demonstrating scalability by incorporating our approach into a high-fidelity combat simulation.},
  isbn = {978-1-5106-6200-1 978-1-5106-6201-8},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LI4C7WXW/Black and Darken - 2023 - Scaling intelligent agent combat behaviors through hierarchical reinforcement learning.pdf}
}

@misc{blasjoGalileoBadArchimedes2018,
  title = {Galileo Bad, {{Archimedes}} Good},
  author = {Blasjo, Viktor},
  year = {2018},
  month = nov,
  number = {1},
  urldate = {2025-05-08},
  abstract = {Galileo's bumbling attempts at determining the area of the cycloid suggests a radical new interpretation of his scientific opus. Archimedes's work on floating bodies is an example of excellent Greek science that has not been sufficiently appreciated. Transcript Galileo is the most over},
  langid = {american}
}

@article{bocchi2022,
  title = {A {{Theory}} of {{Composing Protocols}}},
  author = {Bocchi, Laura and Orchard, Dominic and Voinea, A. Laura},
  year = {2022},
  month = oct,
  journal = {The Art, Science, and Engineering of Programming},
  volume = {7},
  number = {2},
  eprint = {2203.02461},
  primaryclass = {cs},
  pages = {6},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2023/7/6},
  urldate = {2024-04-06},
  abstract = {In programming, protocols are everywhere. Protocols describe the pattern of interaction (or communication) between software systems, for example, between a user-space program and the kernel or between a local application and an online service. Ensuring conformance to protocols avoids a significant class of software errors. Subsequently, there has been a lot of work on verifying code against formal protocol specifications. The pervading approaches focus on distributed settings involving parallel composition of processes within a single monolithic protocol description. However we observe that, at the level of a single thread/process, modern software must often implement a number of clearly delineated protocols at the same time which become dependent on each other, e.g., a banking API and one or more authentication protocols. Rather than plugging together modular protocol-following components, the code must re-integrate multiple protocols into a single component. We address this concern of combining protocols via a novel notion of 'interleaving' composition for protocols described via a process algebra. User-specified, domain-specific constraints can be inserted into the individual protocols to serve as 'contact points' to guide this composition procedure, which outputs a single combined protocol that can be programmed against. Our approach allows an engineer to then program against a number of protocols that have been composed (re-integrated), reflecting the true nature of applications that must handle multiple protocols at once. We prove various desirable properties of the composition, including behaviour preservation: that the composed protocol implements the behaviour of both component protocols. We demonstrate our approach in the practical setting of Erlang, with a tool implementing protocol composition that both generates Erlang code from a protocol and generates a protocol from Erlang code. This tool shows that, for a range of sample protocols (including real-world examples), a modest set of constraints can be inserted to produce a small number of candidate compositions to choose from. As we increasingly build software interacting with many programs and subsystems, this new perspective gives a foundation for improving software quality via protocol conformance in a multi-protocol setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/nobr/Zotero/storage/V94LKUYG/Bocchi et al. - 2022 - A Theory of Composing Protocols.pdf}
}

@article{boeing2017,
  title = {{{OSMnx}}: {{New}} Methods for Acquiring, Constructing, Analyzing, and Visualizing Complex Street Networks},
  shorttitle = {{{OSMnx}}},
  author = {Boeing, Geoff},
  year = {2017},
  month = sep,
  journal = {Computers, Environment and Urban Systems},
  volume = {65},
  pages = {126--139},
  issn = {01989715},
  doi = {10.1016/j.compenvurbsys.2017.05.004},
  urldate = {2023-07-06},
  abstract = {Urban scholars have studied street networks in various ways, but there are data availability and consistency limitations to the current urban planning/street network analysis literature. To address these challenges, this article presents OSMnx, a new tool to make the collection of data and creation and analysis of street networks simple, consistent, automatable and sound from the perspectives of graph theory, transportation, and urban design. OSMnx contributes five significant capabilities for researchers and practitioners: first, the automated downloading of political boundaries and building footprints; second, the tailored and automated downloading and constructing of street network data from OpenStreetMap; third, the algorithmic correction of network topology; fourth, the ability to save street networks to disk as shapefiles, GraphML, or SVG files; and fifth, the ability to analyze street networks, including calculating routes, projecting and visualizing networks, and calculating metric and topological measures. These measures include those common in urban design and transportation studies, as well as advanced measures of the structure and topology of the network. Finally, this article presents a simple case study using OSMnx to construct and analyze street networks in Portland, Oregon.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/B7GB757U/Boeing - 2017 - OSMnx New methods for acquiring, constructing, an.pdf}
}

@article{boeing2019,
  title = {Street {{Network Models}} and {{Measures}} for {{Every U}}.{{S}}. {{City}}, {{County}}, {{Urbanized Area}}, {{Census Tract}}, and {{Zillow-Defined Neighborhood}}},
  author = {Boeing, Geoff},
  year = {2019},
  month = mar,
  journal = {Urban Science},
  volume = {3},
  number = {1},
  pages = {28},
  issn = {2413-8851},
  doi = {10.3390/urbansci3010028},
  urldate = {2023-07-06},
  abstract = {OpenStreetMap provides a valuable crowd-sourced database of raw geospatial data for constructing models of urban street networks for scientific analysis. This paper reports results from a research project that collected raw street network data from OpenStreetMap using the Python-based OSMnx software for every U.S. city and town, county, urbanized area, census tract, and Zillow-defined neighborhood. It constructed nonplanar directed multigraphs for each and analyzed their structural and morphological characteristics. The resulting data repository contains over 110,000 processed, cleaned street network graphs (which in turn comprise over 55 million nodes and over 137 million edges) at various scales---comprehensively covering the entire U.S.---archived as reusable open-source GraphML files, node/edge lists, and GIS shapefiles that can be immediately loaded and analyzed in standard tools such as ArcGIS, QGIS, NetworkX, graph-tool, igraph, or Gephi. The repository also contains measures of each network's metric and topological characteristics common in urban design, transportation planning, civil engineering, and network science. No other such dataset exists. These data offer researchers and practitioners a new ability to quickly and easily conduct graph-theoretic circulation network analysis anywhere in the U.S. using standard, free, open-source tools.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/9XKT4NBB/Boeing - 2019 - Street Network Models and Measures for Every U.S. City, County, Urbanized Area, Census Tract, and Zi.pdf}
}

@article{boeing2019a,
  title = {Urban Spatial Order: Street Network Orientation, Configuration, and Entropy},
  shorttitle = {Urban Spatial Order},
  author = {Boeing, Geoff},
  year = {2019},
  month = dec,
  journal = {Applied Network Science},
  volume = {4},
  number = {1},
  pages = {67},
  issn = {2364-8228},
  doi = {10.1007/s41109-019-0189-1},
  urldate = {2023-07-06},
  abstract = {Street networks may be planned according to clear organizing principles or they may evolve organically through accretion, but their configurations and orientations help define a city's spatial logic and order. Measures of entropy reveal a city's streets' order and disorder. Past studies have explored individual cases of orientation and entropy, but little is known about broader patterns and trends worldwide. This study examines street network orientation, configuration, and entropy in 100 cities around the world using OpenStreetMap data and OSMnx. It measures the entropy of street bearings in weighted and unweighted network models, along with each city's typical street segment length, average circuity, average node degree, and the network's proportions of four-way intersections and dead-ends. It also develops a new indicator of orientation-order that quantifies how a city's street network follows the geometric ordering logic of a single grid. A cluster analysis is performed to explore similarities and differences among these study sites in multiple dimensions. Significant statistical relationships exist between city orientation-order and other indicators of spatial order, including street circuity and measures of connectedness. On average, US/Canadian study sites are far more grid-like than those elsewhere, exhibiting less entropy and circuity. These indicators, taken in concert, help reveal the extent and nuance of the grid. These methods demonstrate automatic, scalable, reproducible tools to empirically measure and visualize city spatial order, illustrating complex urban transportation system patterns and configurations around the world.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RA42DVIJ/Boeing - 2019 - Urban spatial order street network orientation, c.pdf}
}

@misc{bogdan2025,
  title = {Thought {{Anchors}}: {{Which LLM Reasoning Steps Matter}}?},
  shorttitle = {Thought {{Anchors}}},
  author = {Bogdan, Paul C. and Macar, Uzay and Nanda, Neel and Conmy, Arthur},
  year = {2025},
  month = jun,
  number = {arXiv:2506.19143},
  eprint = {2506.19143},
  primaryclass = {cs:LG, cs:cs:AI, cs:cs:CL},
  publisher = {arXiv},
  urldate = {2025-08-04},
  abstract = {Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentence's counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified "broadcasting" sentences that receive disproportionate attention from all future sentences via "receiver" attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentence's tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (www.thought-anchors.com) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models.},
  archiveprefix = {arXiv},
  keywords = {Artificial Intelligence,Computation and Language,Machine Learning},
  file = {/Users/nobr/Zotero/storage/KJQMQPST/Bogdan et al. - 2025 - Thought Anchors Which LLM Reasoning Steps Matter.pdf;/Users/nobr/Zotero/storage/EB7WNKC7/2506.html}
}

@article{bologna2020,
  title = {Deforestation and World Population Sustainability: A Quantitative Analysis},
  shorttitle = {Deforestation and World Population Sustainability},
  author = {Bologna, Mauro and Aquino, Gerardo},
  year = {2020},
  month = may,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {7631},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-63657-6},
  urldate = {2023-11-09},
  abstract = {Abstract             In this paper we afford a quantitative analysis of the sustainability of current world population growth in relation to the parallel deforestation process adopting a statistical point of view. We consider a simplified model based on a stochastic growth process driven by a continuous time random walk, which depicts the technological evolution of human kind, in conjunction with a deterministic generalised logistic model for humans-forest interaction and we evaluate the probability of avoiding the self-destruction of our civilisation. Based on the current resource consumption rates and best estimate of technological rate growth our study shows that we have very low probability, less than 10\% in most optimistic estimate, to survive without facing a catastrophic collapse.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6D686USW/Bologna and Aquino - 2020 - Deforestation and world population sustainability a quantitative analysis.pdf}
}

@article{boluki2020,
  title = {Learnable {{Bernoulli Dropout}} for {{Bayesian Deep Learning}}},
  author = {Boluki, Shahin and Ardywibowo, Randy and Siamak, {\dag} and Dadaneh, Zamani and Zhou, Mingyuan and Qian, Xiaoning},
  year = {2020},
  month = feb,
  eprint = {2002.05155},
  doi = {10.48550/arxiv.2002.05155},
  urldate = {2023-03-14},
  abstract = {In this work, we propose learnable Bernoulli dropout (LBD), a new model-agnostic dropout scheme that considers the dropout rates as parameters jointly optimized with other model parameters. By probabilistic modeling of Bernoulli dropout, our method enables more robust prediction and uncertainty quantification in deep models. Especially, when combined with variational auto-encoders (VAEs), LBD enables flexible semi-implicit posterior representations, leading to new semi-implicit VAE{\textasciitilde}(SIVAE) models. We solve the optimization for training with respect to the dropout parameters using Augment-REINFORCE-Merge (ARM), an unbiased and low-variance gradient estimator. Our experiments on a range of tasks show the superior performance of our approach compared with other commonly used dropout schemes. Overall, LBD leads to improved accuracy and uncertainty estimates in image classification and semantic segmentation. Moreover, using SIVAE, we can achieve state-of-the-art performance on collaborative filtering for implicit feedback on several public datasets.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/7EHAJ9UC/Boluki et al. - 2020 - Learnable Bernoulli Dropout for Bayesian Deep Learning.pdf}
}

@incollection{booker2016,
  title = {Turing and the {{Primes}}},
  booktitle = {The {{Once}} and {{Future Turing}}},
  author = {Booker, Andrew R.},
  editor = {Cooper, S. Barry and Hodges, Andrew},
  year = {2016},
  month = mar,
  edition = {1},
  pages = {34--52},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511863196.006},
  urldate = {2024-10-23},
  isbn = {978-0-511-86319-6 978-1-107-01083-3 978-0-521-28250-5},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/X546YCVU/Booker - 2016 - Turing and the Primes.pdf}
}

@techreport{borg2009,
  title = {Workshop {{On Definition Extraction}}},
  author = {Borg, Claudia and Rosner, Mike and Pace, Gordon},
  year = {2009},
  pages = {26--32},
  abstract = {Books and other text-based learning material contain implicit information which can aid the learner but which usually can only be accessed through a semantic analysis of the text. Definitions of new concepts appearing in the text are one such instance. If extracted and presented to the learner in form of a glossary, they can provide an excellent reference for the study of the main text. One way of extracting definitions is by reading through the text and annotating definitions manually-a tedious and boring job. In this paper, we explore the use of machine learning to extract definitions from non-technical texts, reducing human expert input to a minimum. We report on experiments we have conducted on the use of genetic programming to learn the typical linguistic forms of definitions and a genetic algorithm to learn the relative importance of these forms. Results are very positive , showing the feasibility of exploring further the use of these techniques in definition extraction. The genetic program is able to learn similar rules derived by a human linguistic expert, and the genetic algorithm is able to rank candidate definitions in an order of confidence.},
  keywords = {Definition Extraction,Genetic Algorithms,Genetic Program-ming},
  file = {/Users/nobr/Zotero/storage/J398656I/Borg et al. - 2009 - Workshop On Definition Extraction.pdf}
}

@incollection{borges1962garden,
  title = {The Garden of Forking Paths},
  booktitle = {Ficciones},
  author = {Borges, Jorge Luis},
  editor = {Kerrigan, Anthony},
  translator = {Boucher, Anthony and Yates, Donald A.},
  year = {1962},
  publisher = {Grove Press},
  address = {New York}
}

@book{borgman2015,
  title = {Big Data, Little Data, No Data: Scholarship in the Networked World},
  shorttitle = {Big Data, Little Data, No Data},
  author = {Borgman, Christine L.},
  year = {2015},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-02856-1},
  langid = {english},
  lccn = {AZ195 .B66 2015},
  keywords = {Communication in learning and scholarship,Cyberinfrastructure,Data processing,Information storage and retrieval systems,Information technology,Methodology,Research,Technological innovations},
  file = {/Users/nobr/Zotero/storage/A54A7J9S/Borgman - 2015 - Big data, little data, no data scholarship in the networked world.pdf}
}

@article{borst1999,
  title = {Information Theory and Neural Coding},
  author = {Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {1999},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {2},
  number = {11},
  pages = {947--957},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/14731},
  urldate = {2023-10-22},
  abstract = {Information theory quantifies how much information a neural response carries about the stimulus. This can be compared to the information transferred in particular models of the stimulus--response function and to maximum possible information transfer. Such comparisons are crucial because they validate assumptions present in any neurophysiological analysis. Here we review information-theory basics before demonstrating its use in neural coding. We show how to use information theory to validate simple stimulus--response models of neural coding of dynamic stimuli. Because these models require specification of spike timing precision, they can reveal which time scales contain information in neural coding. This approach shows that dynamic stimuli can be encoded efficiently by single neurons and that each spike contributes to information transmission. We argue, however, that the data obtained so far do not suggest a temporal code, in which the placement of spikes relative to each other yields additional information.},
  copyright = {1999 Nature America Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/nobr/Zotero/storage/ILFA83NW/Borst and Theunissen - 1999 - Information theory and neural coding.pdf}
}

@article{bos1977,
  title = {The Interactions of Mathematics and Society in History Some Exploratory Remarks},
  author = {Bos, H. J. M and Mehrtens, H},
  year = {1977},
  month = feb,
  journal = {Historia Mathematica},
  volume = {4},
  number = {1},
  pages = {7--30},
  issn = {0315-0860},
  doi = {10.1016/0315-0860(77)90031-3},
  urldate = {2024-02-13},
  abstract = {An exploration of the relations of mathematics and society in history, intended as a basis for general discussion. This includes an argument for doing social history of mathematics; a survey of themes, problems, and methods, structured by distinguishing different social forms of mathematics; and, finally, reference to some special themes. Zusammenfassung Der Aufsatz will eine Basis liefern f{\"u}r eine allgemeine Diskussion des Verh{\"a}ltnisses von Mathematik und Gesellschaft in der Geschichte. Dazu wird im ersten Teil mit Verweis auf bestehende Traditionen, mit methodologischen Gr{\"u}nden und mit der Frage nach der Verantwortung des Mathematikhistorikers, f{\"u}r eine Sozialgeschichte der Mathematik argumentiert. Weiter wird der Versuch gemacht, das Problemfeld m{\"o}glichst breit zu erkunden. Den Ausgangspunkt bildet dabei die Unterscheidung von verschiedenen sozialen Formen, in denen Mathematik auftritt. Die Wechselwirkungen zwischen Mathematik und Gesellschaft werden er{\"o}rtert und Begriffe und Methoden zur Behandlung dieser Themen angegeben. Zum Abschluss werden einige spezielle Themen angesprochen und Literaturhinweise gegeben.},
  file = {/Users/nobr/Zotero/storage/24FTJDJ9/0315086077900313.html}
}

@article{bostock2011,
  title = {{{D}}{$^3$} {{Data-Driven Documents}}},
  author = {Bostock, M. and Ogievetsky, V. and Heer, J.},
  year = {2011},
  month = dec,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {17},
  number = {12},
  pages = {2301--2309},
  issn = {1077-2626},
  doi = {10.1109/TVCG.2011.185},
  urldate = {2023-11-09},
  abstract = {Data-Driven Documents (D3) is a novel representation-transparent approach to visualization for the web. Rather than hide the underlying scenegraph within a toolkit-specific abstraction, D3 enables direct inspection and manipulation of a native representation: the standard document object model (DOM). With D3, designers selectively bind input data to arbitrary document elements, applying dynamic transforms to both generate and modify content. We show how representational transparency improves expressiveness and better integrates with developer tools than prior approaches, while offering comparable notational efficiency and retaining powerful declarative components. Immediate evaluation of operators further simplifies debugging and allows iterative development. Additionally, we demonstrate how D3 transforms naturally enable animation and interaction with dramatic performance improvements over intermediate representations.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2THNW4C9/Bostock et al. - 2011 - D³ Data-Driven Documents.pdf}
}

@misc{bottouBorgesAI2023,
  title = {Borges and {{AI}}},
  author = {Bottou, L{\'e}on and Sch{\"o}lkopf, Bernhard},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01425},
  eprint = {2310.01425},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-20},
  abstract = {Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/E9JIUQD6/Bottou and Schölkopf - 2023 - Borges and AI.pdf;/Users/nobr/Zotero/storage/VQZW9ITJ/2310.html}
}

@article{boulton2022,
  title = {Pronounced Loss of {{Amazon}} Rainforest Resilience since the Early 2000s},
  author = {Boulton, Chris A. and Lenton, Timothy M. and Boers, Niklas},
  year = {2022},
  month = mar,
  journal = {Nature Climate Change},
  volume = {12},
  number = {3},
  pages = {271--278},
  publisher = {Nature Publishing Group},
  issn = {1758-6798},
  doi = {10.1038/s41558-022-01287-8},
  urldate = {2023-10-17},
  abstract = {The resilience of the Amazon rainforest to climate and land-use change is crucial for biodiversity, regional climate and the global carbon cycle. Deforestation and climate change, via increasing dry-season length and drought frequency, may already have pushed the Amazon close to a critical threshold of rainforest dieback. Here, we quantify changes of Amazon resilience by applying established indicators (for example, measuring lag-1 autocorrelation) to remotely sensed vegetation data with a focus on vegetation optical depth (1991--2016). We find that more than three-quarters of the Amazon rainforest has been losing resilience since the early 2000s, consistent with the approach to a critical transition. Resilience is being lost faster in regions with less rainfall and in parts of the rainforest that are closer to human activity. We provide direct empirical evidence that the Amazon rainforest is losing resilience, risking dieback with profound implications for biodiversity, carbon storage and climate change at a global scale.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Climate sciences,Climate-change ecology,Climate-change impacts},
  file = {/Users/nobr/Zotero/storage/4JRB95NA/Boulton et al. - 2022 - Pronounced loss of Amazon rainforest resilience since the early 2000s.pdf}
}

@article{bowling2015,
  title = {Heads-up Limit Hold'em Poker Is Solved},
  author = {Bowling, Michael and Burch, Neil and Johanson, Michael and Tammelin, Oskari},
  year = {2015},
  month = jan,
  journal = {Science},
  volume = {347},
  number = {6218},
  pages = {145--149},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1259433},
  urldate = {2024-01-13},
  abstract = {Poker is a family of games that exhibit imperfect information, where players do not have full knowledge of past events. Whereas many perfect-information games have been solved (e.g., Connect Four and checkers), no nontrivial imperfect-information game played competitively by humans has previously been solved. Here, we announce that heads-up limit Texas hold'em is now essentially weakly solved. Furthermore, this computation formally proves the common wisdom that the dealer in the game holds a substantial advantage. This result was enabled by a new algorithm, CFR+, which is capable of solving extensive-form games orders of magnitude larger than previously possible.},
  file = {/Users/nobr/Zotero/storage/NLJYVB2L/Bowling et al. - 2015 - Heads-up limit hold’em poker is solved.pdf}
}

@book{boyd2018,
  title = {Introduction to {{Applied Linear Algebra}}: {{Vectors}}, {{Matrices}}, and {{Least Squares}}},
  shorttitle = {Introduction to {{Applied Linear Algebra}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = {2018},
  month = jun,
  edition = {1},
  publisher = {Cambridge University Press \& Assessment},
  doi = {10.1017/9781108583664},
  urldate = {2023-11-09},
  abstract = {This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra - vectors, matrices, and least squares - that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB{\textregistered}, and data sets accompanying the book online. Suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
  isbn = {978-1-108-58366-4 978-1-316-51896-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XI6N2YUV/Boyd and Vandenberghe - 2018 - Introduction to Applied Linear Algebra Vectors, Matrices, and Least Squares.pdf}
}

@misc{bradley2023,
  title = {Quality-{{Diversity}} through {{AI Feedback}}},
  author = {Bradley, Herbie and Dai, Andrew and Teufel, Hannah and Zhang, Jenny and Oostermeijer, Koen and Bellagente, Marco and Clune, Jeff and Stanley, Kenneth and Schott, Gr{\'e}gory and Lehman, Joel},
  year = {2023},
  month = dec,
  number = {arXiv:2310.13032},
  eprint = {2310.13032},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.13032},
  urldate = {2025-01-01},
  abstract = {In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-QD controls. Further, human evaluation of QDAIF-generated creative texts validates reasonable agreement between AI and human evaluation. Our results thus highlight the potential of AI feedback to guide open-ended search for creative and original solutions, providing a recipe that seemingly generalizes to many domains and modalities. In this way, QDAIF is a step towards AI systems that can independently search, diversify, evaluate, and improve, which are among the core skills underlying human society's capacity for innovation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/MFQ83XI7/Bradley et al. - 2023 - Quality-Diversity through AI Feedback.pdf}
}

@article{braga2023,
  title = {Brazilian {{Export Processing Zones}} \&amp; {{Green Powershoring}}: {{Challenges}} \&amp; {{Opportunities}}},
  shorttitle = {Brazilian {{Export Processing Zones}} \&amp; {{Green Powershoring}}},
  author = {Braga, Helson and Gouvea, Raul and Gutierrez, Margarida},
  year = {2023},
  journal = {Modern Economy},
  volume = {14},
  number = {10},
  pages = {1366--1392},
  issn = {2152-7245, 2152-7261},
  doi = {10.4236/me.2023.1410070},
  urldate = {2024-01-19},
  abstract = {In 2023, Export Processing Zones (EPZs) are playing a very meaningful role in promoting exports, attracting foreign direct investment, and creating employment opportunities in more than 150 countries around the globe. EPZs have been important players in fostering the extraordinary growth and expansion of the global supply and value-added chains, further integrating the global economy. Brazil is a latecomer in adopting this tool and strategy of trade and investment promotion and liberalization. This paper addresses the global EPZ experience and discusses Brazil's latest policies and procedures to expand the role of EPZs in Brazil's foreign trade policy. This paper introduces the concept of Green Powershoring stressing the increasing importance of EPZs embracing the UN's 2030 Sustainable Development Goals. This paper also addresses Brazil's unique position in offering multinational companies moving into Brazilian EPZs the opportunity to benefit from the country's clean and renewable energy matrix and Brazil's abundant fresh water reserves, another key resource for manufacturing. Thus offering companies established in Brazilian EPZs additional ESGs competitive advantages. Thus, Brazilian EPZs are well positioned to benefit from the green power shoring economy permeating new trends in global foreign trade and global foreign direct investment.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SQDBS834/Braga et al. - 2023 - Brazilian Export Processing Zones &amp; Green Powershoring Challenges &amp; Opportunities.pdf}
}

@article{brailsford2022,
  title = {How Did Dennis Ritchie Produce His {{PhD}} Thesis?},
  author = {Brailsford, David F. and Kernighan, Brian W. and Ritchie, William A.},
  year = {2022},
  pages = {1--10},
  doi = {10.1145/3558100.3563839},
  file = {/Users/nobr/Zotero/storage/37L2SQGB/document.pdf}
}

@misc{brandizzi2023,
  title = {Towards {{More Human-like AI Communication}}: {{A Review}} of {{Emergent Communication Research}}},
  shorttitle = {Towards {{More Human-like AI Communication}}},
  author = {Brandizzi, Nicolo'},
  year = {2023},
  month = aug,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {In the recent shift towards human-centric AI, the need for machines to accurately use natural language has become increasingly important. While a common approach to achieve this is to train large language models, this method presents a form of learning misalignment where the model may not capture the underlying structure and reasoning humans employ in using natural language, potentially leading to unexpected or unreliable behavior. Emergent communication (EmCom) is a field of research that has seen a growing number of publications in recent years, aiming to develop artificial agents capable of using natural language in a way that goes beyond simple discriminative tasks and can effectively communicate and learn new concepts. In this review, we present EmCom under two aspects. Firstly, we delineate all the common proprieties we find across the literature and how they relate to human interactions. Secondly, we identify two subcategories and highlight their characteristics and open challenges. We encourage researchers to work together by demonstrating that different methods can be viewed as diverse solutions to a common problem and emphasize the importance of including diverse perspectives and expertise in the field. We believe a deeper understanding of human communication is crucial to developing machines that can accurately use natural language in human-machine interactions.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Multiagent Systems,I.2.11,I.2.6,I.2.7},
  file = {/Users/nobr/Zotero/storage/KGNJN9M2/Brandizzi - 2023 - Towards More Human-like AI Communication A Review of Emergent Communication Research.pdf}
}

@misc{brandonStripedAttentionFaster2023,
  title = {Striped {{Attention}}: {{Faster Ring Attention}} for {{Causal Transformers}}},
  shorttitle = {Striped {{Attention}}},
  author = {Brandon, William and Nrusimha, Aniruddha and Qian, Kevin and Ankner, Zachary and Jin, Tian and Song, Zhiye and {Ragan-Kelley}, Jonathan},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09431},
  eprint = {2311.09431},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-23},
  abstract = {To help address the growing demand for ever-longer sequence lengths in transformer models, Liu et al. recently proposed Ring Attention (2023), an exact attention algorithm capable of overcoming per-device memory bottlenecks by distributing self-attention across multiple devices. In this paper, we study the performance characteristics of Ring Attention in the important special case of causal transformer models, and identify a key workload imbalance due to triangular structure of causal attention computations. We propose a simple extension to Ring Attention, which we call Striped Attention to fix this imbalance. Instead of devices having contiguous subsequences, each device has a subset of tokens distributed uniformly throughout the sequence, which we demonstrate leads to more even workloads. In experiments running Striped Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45{\texttimes} end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k. Furthermore, on 16 TPUv4 chips, we were able to achieve 1.65{\texttimes} speedups at sequence lengths of 786k. We release the code for our experiments as open source.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/JR25CVIC/Brandon et al. - 2023 - Striped Attention Faster Ring Attention for Causal Transformers.pdf}
}

@article{brazinskas2017,
  title = {Embedding {{Words}} as {{Distributions}} with a {{Bayesian Skip-gram Model}}},
  author = {Bra{\v z}inskas, Arthur and Havrylov, Serhii and Titov, Ivan},
  year = {2017},
  month = nov,
  eprint = {1711.11027},
  abstract = {We introduce a method for embedding words as probability densities in a low-dimensional space. Rather than assuming that a word embedding is fixed across the entire text collection, as in standard word embedding methods, in our Bayesian model we generate it from a word-specific prior density for each occurrence of a given word. Intuitively, for each word, the prior density encodes the distribution of its potential 'meanings'. These prior densities are conceptually similar to Gaussian embeddings. Interestingly, unlike the Gaussian embeddings, we can also obtain context-specific densities: they encode uncertainty about the sense of a word given its context and correspond to posterior distributions within our model. The context-dependent densities have many potential applications: for example, we show that they can be directly used in the lexical substitution task. We describe an effective estimation method based on the variational autoencoding framework. We also demonstrate that our embeddings achieve competitive results on standard benchmarks.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/GV7MGQC5/Bražinskas et al. - 2017 - Embedding Words as Distributions with a Bayesian Skip-gram Model.pdf}
}

@article{brenner2018,
  title = {Red Blood Cell-Hitchhiking Boosts Delivery of Nanocarriers to Chosen Organs by Orders of Magnitude},
  author = {Brenner, Jacob S. and Pan, Daniel C. and Myerson, Jacob W. and {Marcos-Contreras}, Oscar A. and Villa, Carlos H. and Patel, Priyal and Hekierski, Hugh and Chatterjee, Shampa and Tao, Jian-Qin and Parhiz, Hamideh and Bhamidipati, Kartik and Uhler, Thomas G. and Hood, Elizabeth D. and Kiseleva, Raisa Yu and Shuvaev, Vladimir S. and Shuvaeva, Tea and Khoshnejad, Makan and Johnston, Ian and Gregory, Jason V. and Lahann, Joerg and Wang, Tao and Cantu, Edward and Armstead, William M. and Mitragotri, Samir and Muzykantov, Vladimir},
  year = {2018},
  month = jul,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {2684},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-05079-7},
  urldate = {2023-12-12},
  abstract = {Drug delivery by nanocarriers (NCs) has long been stymied by dominant liver uptake and limited target organ deposition, even when NCs are targeted using affinity moieties. Here we report a universal solution: red blood cell (RBC)-hitchhiking (RH), in which NCs adsorbed onto the RBCs transfer from RBCs to the first organ downstream of the intravascular injection. RH improves delivery for a wide range of NCs and even viral vectors. For example, RH injected intravenously increases liposome uptake in the first downstream organ, lungs, by {\textasciitilde}40-fold compared with free NCs. Intra-carotid artery injection of RH NCs delivers {$>$}10\% of the injected NC dose to the brain, {\textasciitilde}10{\texttimes} higher than that achieved with affinity moieties. Further, RH works in mice, pigs, and ex vivo human lungs without causing RBC or end-organ toxicities. Thus, RH is a clinically translatable platform technology poised to augment drug delivery in acute lung disease, stroke, and several other diseases.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Drug delivery,Drug development,Molecular medicine,Nanoparticles},
  file = {/Users/nobr/Zotero/storage/VFRN8QQD/Brenner et al. - 2018 - Red blood cell-hitchhiking boosts delivery of nanocarriers to chosen organs by orders of magnitude.pdf}
}

@misc{brockwayWeatheringAdaptationGrid2019,
  title = {Weathering {{Adaptation}}: {{Grid Infrastructure Planning}} in a {{Changing Climate}}},
  shorttitle = {Weathering {{Adaptation}}},
  author = {Brockway, Anna M. and Dunn, Laurel N.},
  year = {2019},
  month = dec,
  number = {arXiv:1912.02920},
  eprint = {1912.02920},
  primaryclass = {physics},
  publisher = {arXiv},
  urldate = {2024-03-12},
  abstract = {Decisions related to electric power systems planning and operations rely on assumptions and insights informed by historic weather data and records of past performance. Evolving climate trends are, however, changing the energy use patterns and operating conditions of grid assets, thus altering the nature and severity of risks the system faces. Because grid assets remain in operation for decades, planning for evolving risks will require incorporating climate projections into grid infrastructure planning processes. The current work traces a pathway for climate-aware decision-making in the electricity sector. We evaluate the suitability of using existing climate models and data for electricity planning and discuss their limitations. We review the interactions between grid infrastructure and climate by synthesizing what is known about how changing environmental operating conditions would impact infrastructure utilization, constraints, and performance. We contextualize our findings by presenting a case study of California, examining if and where climate data can be integrated into infrastructure planning processes. The core contribution of the work is a series of nine recommendations detailing advancements in climate projections, grid modeling architecture, and disaster preparedness that would be needed to ensure that infrastructure planning decisions are robust to uncertainty and risks associated with evolving climate conditions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Physics - Atmospheric and Oceanic Physics,Physics - Physics and Society},
  file = {/Users/nobr/Zotero/storage/JKJY6RT9/Brockway and Dunn - 2019 - Weathering Adaptation Grid Infrastructure Planning in a Changing Climate.pdf}
}

@inproceedings{bromley1993,
  title = {Signature {{Verification}} Using a "{{Siamese}}" {{Time Delay Neural Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S{\"a}ckinger, Eduard and Shah, Roopak},
  year = {1993},
  volume = {6},
  publisher = {Morgan-Kaufmann},
  urldate = {2023-06-11},
  abstract = {This paper describes an algorithm for verification of signatures  written on a pen-input tablet. The algorithm is based on a novel,  artificial neural network, called a "Siamese" neural network. This  network consists of two identical sub-networks joined at their out(cid:173) puts. During training the two sub-networks extract features from  two signatures, while the joining neuron measures the distance be(cid:173) tween the two feature vectors. Verification consists of comparing an  extracted feature vector {\textasciitilde}ith a stored feature vector for the signer.  Signatures closer to this stored representation than a chosen thresh(cid:173) old are accepted, all other signatures are rejected as forgeries.},
  file = {/Users/nobr/Zotero/storage/C3ANELLH/Bromley et al. - 1993 - Signature Verification using a Siamese Time Dela.pdf}
}

@misc{bronstein2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  year = {2021},
  month = may,
  number = {arXiv:2104.13478},
  eprint = {2104.13478},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.13478},
  urldate = {2023-04-25},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/GGZKHARX/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf;/Users/nobr/Zotero/storage/Y9H233K9/2104.html}
}

@article{brown2018,
  title = {Superhuman {{AI}} for Heads-up No-Limit Poker: {{Libratus}} Beats Top Professionals},
  shorttitle = {Superhuman {{AI}} for Heads-up No-Limit Poker},
  author = {Brown, Noam and Sandholm, Tuomas},
  year = {2018},
  month = jan,
  journal = {Science},
  volume = {359},
  number = {6374},
  pages = {418--424},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aao1733},
  urldate = {2024-01-13},
  abstract = {No-limit Texas hold'em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold'em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.},
  file = {/Users/nobr/Zotero/storage/5GMTYXC2/Brown and Sandholm - 2018 - Superhuman AI for heads-up no-limit poker Libratus beats top professionals.pdf}
}

@misc{brownConwaysGameLife2023,
  title = {Conway's {{Game}} of {{Life}} Is {{Omniperiodic}}},
  author = {Brown, Nico and Cheng, Carson and Jacobi, Tanner and Karpovich, Maia and Merzenich, Matthias and Raucci, David and Riley, Mitchell},
  year = {2023},
  month = dec,
  number = {arXiv:2312.02799},
  eprint = {2312.02799},
  primaryclass = {nlin},
  publisher = {arXiv},
  urldate = {2023-12-08},
  abstract = {In the theory of cellular automata, an oscillator is a pattern that repeats itself after a fixed number of generations; that number is called its period. A cellular automaton is called omniperiodic if there exist oscillators of all periods. At the turn of the millennium, only twelve oscillator periods remained to be found in Conway's Game of Life. The search has finally ended, with the discovery of oscillators having the final two periods, 19 and 41, proving that Life is omniperiodic. Besides filling in the missing periods, we give a detailed history of the omniperiodicity problem and the strategies used to solve it, summarising the work of a large number of people in the decades since the creation of Life.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Combinatorics,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/nobr/Zotero/storage/5XFTXC2C/Brown et al. - 2023 - Conway's Game of Life is Omniperiodic.pdf}
}

@article{browne2012,
  title = {A Survey of {{Monte Carlo}} Tree Search Methods},
  author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  year = {2012},
  month = mar,
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {4},
  number = {1},
  pages = {1--43},
  issn = {1943068X},
  doi = {10.1109/TCIAIG.2012.2186810},
  urldate = {2023-03-22},
  abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work. {\copyright} 2009 IEEE.},
  keywords = {Artificial intelligence (AI),bandit-based methods,computer Go,game search,Monte Carlo tree search (MCTS),upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
  file = {/Users/nobr/Zotero/storage/JALIHS9D/Browne et al. - 2012 - A survey of Monte Carlo tree search methods.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -- something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/4K6PJH8T/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@misc{bruce2024,
  title = {Genie: {{Generative Interactive Environments}}},
  shorttitle = {Genie},
  author = {Bruce, Jake and Dennis, Michael and Edwards, Ashley and {Parker-Holder}, Jack and Shi, Yuge and Hughes, Edward and Lai, Matthew and Mavalankar, Aditi and Steigerwald, Richie and Apps, Chris and Aytar, Yusuf and Bechtle, Sarah and Behbahani, Feryal and Chan, Stephanie and Heess, Nicolas and Gonzalez, Lucy and Osindero, Simon and Ozair, Sherjil and Reed, Scott and Zhang, Jingwei and Zolna, Konrad and Clune, Jeff and de Freitas, Nando and Singh, Satinder and Rockt{\"a}schel, Tim},
  year = {2024},
  month = feb,
  number = {arXiv:2402.15391},
  eprint = {2402.15391},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.15391},
  urldate = {2025-08-09},
  abstract = {We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/8JV9SEL2/Bruce et al. - 2024 - Genie Generative Interactive Environments.pdf}
}

@article{brunken2006,
  title = {Risk Factors for Early Interruption of Exclusive Breastfeeding and Late Introduction of Complementary Foods among Infants in Midwestern {{Brazil}}},
  author = {Brunken, Gisela S. and Silva, Solanyara M. and Fran{\c c}a, Giovanny V. A. and Escuder, Maria M. and Ven{\^a}ncio, Sonia I.},
  year = {2006},
  month = dec,
  journal = {Jornal de Pediatria},
  volume = {82},
  number = {6},
  pages = {445--451},
  issn = {0021-7557},
  doi = {10.2223/JPED.1569},
  urldate = {2024-05-25},
  abstract = {Objective: To identify factors associated with early interruption (before 4 months) of exclusive breastfeeding and late introduction (after 8 months) of complementary foods. Methods: This is a cross-sectional study, based on a survey conducted on the first day of the National Vaccination Campaign in 2004, in Cuiab{$\cdot$}, MT, Brazil. The sample comprised 921 children less than 1 year old, and the adult accompanying each child was interviewed and a semi-structured questionnaire filled out. Probit analysis was employed to assess consumption of liquids and solids, and logistic regression analysis was applied to identify factors associated with early introduction of liquids and with late introduction of solids. Results: There was elevated consumption of water and teas, followed by cow{\'i}s milk among those less than 120 days old. Children were more likely to be being given liquids on the day of the survey if they had been consuming them on the day they were discharged from the maternity unit. Approximately 60\% of the children were being given soup or the family meal by 8 months. Conclusions: Liquids being given on the first day at home was a good predictor that they would be given for the first 4 months, emphasizing the need for intervention during prenatal care and at maternity units to counter the harm caused by this practice. After 8 months, however, it is necessary to emphasize the importance of the child participating in family meals, especially for adult mothers without higher education and primaparous mothers.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PYLFRUZ2/Brunken et al. - 2006 - Risk factors for early interruption of exclusive breastfeeding and late introduction of complementar.pdf}
}

@book{bryant2016,
  title = {Computer Systems: A Programmer's Perspective},
  shorttitle = {Computer Systems},
  author = {Bryant, Randal E. and O'Hallaron, David R.},
  year = {2016},
  edition = {Third edition},
  publisher = {Pearson},
  address = {Boston},
  isbn = {978-0-13-409266-9},
  langid = {english},
  lccn = {QA76.5 .B795 2016},
  keywords = {Computer systems,Computers,Telecommunication,User interfaces (Computer systems)},
  file = {/Users/nobr/Zotero/storage/GI8FRREH/Bryant and O'Hallaron - 2016 - Computer systems a programmer's perspective.pdf}
}

@misc{bubeckSparksArtificialGeneral2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12712},
  urldate = {2023-11-19},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/ZHN36PTT/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early experiments with GPT-4.pdf;/Users/nobr/Zotero/storage/UZ6DUCR3/2303.html}
}

@article{buchanan2020,
  title = {The Effect of Network Thresholding and Weighting on Structural Brain Networks in the {{UK Biobank}}},
  author = {Buchanan, Colin R. and Bastin, Mark E. and Ritchie, Stuart J. and Liewald, David C. and Madole, James W. and {Tucker-Drob}, Elliot M. and Deary, Ian J. and Cox, Simon R.},
  year = {2020},
  month = may,
  journal = {NeuroImage},
  volume = {211},
  pages = {116443},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.116443},
  urldate = {2023-05-16},
  abstract = {Whole-brain structural networks can be constructed using diffusion MRI and probabilistic tractography. However, measurement noise and the probabilistic nature of the tracking procedure result in an unknown proportion of spurious white matter connections. Faithful disentanglement of spurious and genuine connections is hindered by a lack of comprehensive anatomical information at the network-level. Therefore, network thresholding methods are widely used to remove ostensibly false connections, but it is not yet clear how different thresholding strategies affect basic network properties and their associations with meaningful demographic variables, such as age. In a sample of 3153 generally healthy volunteers from the UK Biobank Imaging Study (aged 44--77 years), we constructed whole-brain structural networks and applied two principled network thresholding approaches (consistency and proportional thresholding). These were applied over a broad range of threshold levels across six alternative network weightings (streamline count, fractional anisotropy, mean diffusivity and three novel weightings from neurite orientation dispersion and density imaging) and for four common network measures (mean edge weight, characteristic path length, network efficiency and network clustering coefficient). We compared network measures against age associations and found that: 1) measures derived from unthresholded matrices yielded the weakest age-associations (0.033~\hspace{0pt}{$\leq~$}\hspace{0pt}{\textbar}{$\beta\vert~$}\hspace{0pt}{$\leq~$}\hspace{0pt}0.409); and 2) the most commonly-used level of proportional-thresholding from the literature (retaining 68.7\% of all possible connections) yielded significantly weaker age-associations (0.070~\hspace{0pt}{$\leq~$}\hspace{0pt}{\textbar}{$\beta\vert~$}\hspace{0pt}{$\leq~$}\hspace{0pt}0.406) than the consistency-based approach which retained only 30\% of connections (0.140~\hspace{0pt}{$\leq~$}\hspace{0pt}{\textbar}{$\beta\vert~$}\hspace{0pt}{$\leq~$}\hspace{0pt}0.409). However, we determined that the stringency of the threshold was a stronger determinant of the network-age association than the choice of threshold method and the two thresholding approaches identified a highly overlapping set of connections (ICC~\hspace{0pt}=~\hspace{0pt}0.84), when matched at 70\% network sparsity. Generally, more stringent thresholding resulted in more age-sensitive network measures in five of the six network weightings, except at the highest levels of sparsity ({$>$}90\%), where crucial connections were then removed. At two commonly-used threshold levels, the age-associations of the connections that were discarded (mean {$\beta~$}\hspace{0pt}{$\leq~$}\hspace{0pt}{\textbar}0.068{\textbar}) were significantly smaller in magnitude than the corresponding age-associations of the connections that were retained (mean {$\beta~$}\hspace{0pt}{$\leq~$}\hspace{0pt}{\textbar}0.219{\textbar}, p~\hspace{0pt}{$<~$}\hspace{0pt}0.001, uncorrected). Given histological evidence of widespread degeneration of structural brain connectivity with increasing age, these results indicate that stringent thresholding methods may be most accurate in identifying true white matter connections.},
  langid = {english},
  keywords = {Aging,Brain network,Connectome,Diffusion MRI,Thresholding},
  file = {/Users/nobr/Zotero/storage/PGTBFX4D/Buchanan et al. - 2020 - The effect of network thresholding and weighting o.pdf;/Users/nobr/Zotero/storage/EEZUA43I/S1053811919310341.html}
}

@book{buchberger2004,
  title = {Konvens 2004: {{Beitr{\"a}ge}} Zur 7. {{Konferenz}} Zur {{Verarbeitung Nat{\"u}rlicher Sprache}} ({{KONVENS}}) ; 14.-17. {{September}} 2004 {{Universit{\"a}t Wien}}},
  shorttitle = {Konvens 2004},
  editor = {Buchberger, Ernst},
  year = {2004},
  series = {Schriftenreihe Der {{{\"O}sterreichischen Gesellschaft}} F{\"u}r {{Artificial Intelligence}}},
  number = {5},
  publisher = {{\"O}GAI},
  address = {Wien},
  isbn = {978-3-85027-005-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HBLVEMQA/Buchberger - 2004 - Konvens 2004 Beiträge zur 7. Konferenz zur Verarbeitung Natürlicher Sprache (KONVENS) ; 14.-17. Sep.pdf}
}

@inproceedings{buchter2016,
  title = {Availability of Airborne {{Ad-hoc}} Communication Network in Global Air Traffic Simulation},
  booktitle = {2016 10th {{International Symposium}} on {{Communication Systems}}, {{Networks}} and {{Digital Signal Processing}} ({{CSNDSP}})},
  author = {Buchter, Kai-Daniel},
  year = {2016},
  month = jul,
  pages = {1--4},
  publisher = {IEEE},
  address = {Prague, Czech Republic},
  doi = {10.1109/CSNDSP.2016.7573927},
  urldate = {2023-11-19},
  abstract = {Large-scale airborne communication networks are modeled, using a simulation of global, wide-body aircraft movements and assuming the use of optical communication links. Statistics concerning the expected node degree as function of communication range and ratios of connected aircraft are presented. It is found that, with an ambitious but plausible communication range of 450 km, on average 80\% of world-wide, oceanic flights may connect to ground gateways via airborne networking in the simulation.},
  isbn = {978-1-5090-2526-8},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5LMXBW9G/Buchter - 2016 - Availability of airborne Ad-hoc communication network in global air traffic simulation.pdf}
}

@article{buniatyan2019,
  title = {Hyper: {{Distributed Cloud Processing}} for {{Large-Scale Deep Learning Tasks}}},
  author = {Buniatyan, Davit},
  year = {2019},
  month = oct,
  eprint = {1910.07172},
  abstract = {Training and deploying deep learning models in real-world applications require processing large amounts of data. This is a challenging task when the amount of data grows to a hundred terabytes, or even, petabyte-scale. We introduce a hybrid distributed cloud framework with a unified view to multiple clouds and an on-premise infrastructure for processing tasks using both CPU and GPU compute instances at scale. The system implements a distributed file system and failure-tolerant task processing scheduler, independent of the language and Deep Learning framework used. It allows to utilize unstable cheap resources on the cloud to significantly reduce costs. We demonstrate the scalability of the framework on running pre-processing, distributed training, hyperparameter search and large-scale inference tasks utilizing 10,000 CPU cores and 300 GPU instances with the overall processing power of 30 petaflops.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/8YDDQ63R/Buniatyan - 2019 - Hyper Distributed Cloud Processing for Large-Scale Deep Learning Tasks.pdf}
}

@article{burda2018,
  title = {Large-{{Scale Study}} of {{Curiosity-Driven Learning}}},
  author = {Burda, Yuri},
  year = {2018},
  file = {/Users/nobr/Zotero/storage/DEWLK3LN/Burda - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf}
}

@article{burgess2023,
  title = {Exploring How Players Use Emergent Narrative in Strategy Games},
  author = {Burgess, Jacqueline and Jones, Christian},
  year = {2023},
  month = jan,
  journal = {Entertainment Computing},
  volume = {44},
  pages = {100533},
  issn = {18759521},
  doi = {10.1016/j.entcom.2022.100533},
  urldate = {2023-11-12},
  abstract = {This article explored how players were impacted by and used emergent narrative to make sense of their gameplay in the Total War strategy video game series. Two stages of qualitative research were undertaken: (i) a thematic analysis of 295 online forum posts; and (ii) 104 respondents to an online survey. The research followed a phenomenographic approach with analysis informed by undertaking the active story interpreter role. The the\- matic analysis found that 32 \% of the posts contained emergent narratives of which 14 \% were detailed, while a further 39 \% of the posts expressed pleasure from reading these emergent narratives. Results from online survey suggested respondents developed emotional attachments to the generals and units they controlled in Total War video game, and that this emotional attachment increased their enjoyment of the gameplay (79 \%). However, sometimes, due to these attachments, players allowed generals to `retire' and idle in a settlement, which may have adversely impacted their strategic gameplay in a game genre that privileges long-term planning. Since players can derive pleasure in creating or reading other players' emergent narratives, developers could further enhance players' enjoyment of their strategy games by providing more resources and prompts to assist with their creation.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/MBPF3GND/Burgess and Jones - 2023 - Exploring how players use emergent narrative in strategy games.pdf}
}

@article{burgin1988,
  title = {Rule-{{Based Air Combat Simulation}}},
  author = {Burgin, H and Sidor, L B},
  year = {1988},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZKC882SD/Burgin and Sidor - Rule-Based Air Combat Simulation.pdf}
}

@misc{buzzardGrothendiecksUseEquality2024,
  title = {Grothendieck's Use of Equality},
  author = {Buzzard, Kevin},
  year = {2024},
  month = may,
  number = {arXiv:2405.10387},
  eprint = {2405.10387},
  primaryclass = {math},
  doi = {10.48550/arXiv.2405.10387},
  urldate = {2024-06-18},
  abstract = {We discuss how the concept of equality is used by mathematicians (including Grothendieck), and what effect this has when trying to formalise mathematics. We challenge various reasonable-sounding slogans about equality.},
  archiveprefix = {arXiv},
  keywords = {03A05 (Primary) 14A99 11R37 11R39 (Secondary),Mathematics - Algebraic Geometry,Mathematics - Number Theory},
  file = {/Users/nobr/Zotero/storage/W2P9DETV/Buzzard - 2024 - Grothendieck's use of equality.pdf}
}

@article{cai2024,
  title = {Vocabulary for {{Universal Approximation}}: {{A Linguistic Perspective}} of {{Mapping Compositions}}},
  author = {Cai, Yongqiang},
  year = {2024},
  abstract = {In recent years, deep learning-based sequence modelings, such as language models, have received much attention and success, which pushes researchers to explore the possibility of transforming non-sequential problems into a sequential form. Following this thought, deep neural networks can be represented as composite functions of a sequence of mappings, linear or nonlinear, where each composition can be viewed as a word. However, the weights of linear mappings are undetermined and hence require an infinite number of words. In this article, we investigate the finite case and constructively prove the existence of a finite vocabulary V = \{{$\phi$}i : Rd {$\rightarrow$} Rd {\textbar} i = 1, ..., n\} with n = O(d2) for the universal approximation. That is, for any continuous mapping f : Rd {$\rightarrow$} Rd, compact domain {\textohm} and {$\varepsilon$} {$>$} 0, there is a sequence of mappings {$\phi$}i1 , ..., {$\phi$}im {$\in$} V, m {$\in$} Z+, such that the composition {$\phi$}im {\textopenbullet} ... {\textopenbullet} {$\phi$}i1 approximates f on {\textohm} with an error less than {$\varepsilon$}. Our results demonstrate an unusual approximation power of mapping compositions and motivate a novel compositional model for regular languages.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2SCWRP87/Cai - Vocabulary for Universal Approximation A Linguistic Perspective of Mapping Compositions.pdf}
}

@misc{caiLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Tool Makers}}},
  author = {Cai, Tianle and Wang, Xuezhi and Ma, Tengyu and Chen, Xinyun and Zhou, Denny},
  year = {2023},
  month = may,
  number = {arXiv:2305.17126},
  eprint = {2305.17126},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-11-19},
  abstract = {Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the tasks. Furthermore, the division of labor among LLMs for tool-making and tool-using phases introduces the opportunity to achieve cost effectiveness without degrading the quality of generated tools and problem solutions. For example, recognizing that tool-making demands more sophisticated capabilities than tool-using, we can apply a powerful yet resource-intensive model as the tool maker, and a lightweight while cost-effective model as the tool user. We validate the effectiveness of our approach across a variety of complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM can achieve performance that is on par with using GPT-4 for both tool making and tool using, while the inference cost is significantly reduced.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/AP9F4DMV/Cai et al. - 2023 - Large Language Models as Tool Makers.pdf}
}

@article{campbell2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng Hsiung},
  year = {2002},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {134},
  number = {1-2},
  pages = {57--83},
  publisher = {Elsevier},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  urldate = {2023-03-22},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: a single-chip chess search engine, a massively parallel system with multiple levels of parallelism, a strong emphasis on search extensions, a complex evaluation function, and effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue. {\copyright} 2001 Elsevier Science B.V. All rights reserved.},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/Users/nobr/Zotero/storage/SZMJCWLN/full-text.pdf}
}

@book{campbell2007,
  title = {The {{Age}} of {{Consequences}}: {{The Foreign Policy}} and {{National Security Implications}} of {{Global Climate Change}}},
  editor = {Campbell, Kurt M. and Gulledge, Jay and McNeill, J. R. and Podesta, John and Ogden, Peter and Fuerth, Leon and Woolsey, R. James and Lennon, Alexander T. J. and Smith, Julianne and Weitz, Richard and Mix, Derek},
  year = {2007},
  publisher = {{Center for Strategic and International Studies}},
  address = {Washington, D.C.}
}

@article{canal2022,
  title = {A Survey on Facial Emotion Recognition Techniques: {{A}} State-of-the-Art Literature Review},
  shorttitle = {A Survey on Facial Emotion Recognition Techniques},
  author = {Canal, Felipe Zago and M{\"u}ller, Tobias Rossi and Matias, Jhennifer Cristine and Scotton, Gustavo Gino and De Sa Junior, Antonio Reis and Pozzebon, Eliane and Sobieranski, Antonio Carlos},
  year = {2022},
  month = jan,
  journal = {Information Sciences},
  volume = {582},
  pages = {593--617},
  issn = {00200255},
  doi = {10.1016/j.ins.2021.10.005},
  urldate = {2023-05-10},
  abstract = {In this survey, a systematic literature review of the state-of-the-art on emotion expression recognition from facial images is presented. The paper has as main objective arise the most commonly used strategies employed to interpret and recognize facial emotion expressions, published over the past few years. For this purpose, a total of 51 papers were analyzed over the literature totaling 94 distinct methods, collected from well-established scientific databases (ACM Digital Library, IEEE Xplore, Science Direct and Scopus), whose works were categorized according to its main construction concept. From the analyzed works, it was possible to categorize them into two main trends: classical and those approaches specifically designed by the use of neural networks. The obtained statistical analysis demonstrated a marginally better recognition precision for the classical approaches when faced to neural networks counterpart, but with a reduced capacity of generalization. Additionally, the present study verified the most popular datasets for facial expression and emotion recognition showing the pros and cons each and, thereby, demonstrating a real demand for reliable data-sources regarding artificial and natural experimental environments.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IDSGX2A6/Canal et al. - 2022 - A survey on facial emotion recognition techniques.pdf}
}

@misc{cao2023,
  title = {Robot {{Behavior-Tree-Based Task Generation}} with {{Large Language Models}}},
  author = {Cao, Yue and Lee, C. S. George},
  year = {2023},
  month = feb,
  number = {arXiv:2302.12927},
  eprint = {2302.12927},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.12927},
  urldate = {2023-12-30},
  abstract = {Nowadays, the behavior tree is gaining popularity as a representation for robot tasks due to its modularity and reusability. Designing behavior-tree tasks manually is time-consuming for robot end-users, thus there is a need for investigating automatic behavior-tree-based task generation. Prior behavior-tree-based task generation approaches focus on fixed primitive tasks and lack generalizability to new task domains. To cope with this issue, we propose a novel behavior-tree-based task generation approach that utilizes state-of-the-art large language models. We propose a Phase-Step prompt design that enables a hierarchical-structured robot task generation and further integrate it with behavior-tree-embedding-based search to set up the appropriate prompt. In this way, we enable an automatic and cross-domain behavior-tree task generation. Our behavior-tree-based task generation approach does not require a set of pre-defined primitive tasks. End-users only need to describe an abstract desired task and our proposed approach can swiftly generate the corresponding behavior tree. A full-process case study is provided to demonstrate our proposed approach. An ablation study is conducted to evaluate the effectiveness of our Phase-Step prompts. Assessment on Phase-Step prompts and the limitation of large language models are presented and discussed.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/WCLBVLHD/Cao and Lee - 2023 - Robot Behavior-Tree-Based Task Generation with Large Language Models.pdf;/Users/nobr/Zotero/storage/MNECSM5Q/2302.html}
}

@article{cappuccio2022,
  title = {Saving {{Private Robot}}: {{Risks}} and {{Advantages}} of {{Anthropomorphism}} in {{Agent-Soldier Teams}}},
  shorttitle = {Saving {{Private Robot}}},
  author = {Cappuccio, Massimiliano L. and Galliott, Jai C. and Sandoval, Eduardo B.},
  year = {2022},
  month = dec,
  journal = {International Journal of Social Robotics},
  volume = {14},
  number = {10},
  pages = {2135--2148},
  issn = {1875-4805},
  doi = {10.1007/s12369-021-00755-z},
  urldate = {2023-11-19},
  abstract = {Hybrid military teams, formed by human warfighters and autonomous artificial agents, represent the technological future of Defence operations. Both the potential and the inherent limitations of current technology are well-known, but the cognitive--behavioral and motivational aspects of human--robot interaction on the battlefield have yet to be systematically investigated. To lay the theoretical and methodological foundation of this scientific investigation, our position paper critically examines how the military personnel's spontaneous tendency to anthropomorphize artificial autonomous agents can affect operations of hybrid military teams in multiple ways. We will argue that the psychological impact of anthropomorphism on military personnel is neither easily avoidable nor necessarily detrimental. Correctly identifying the multi-level cognitive mechanisms that underpin implicit and explicit forms of anthropomorphism allows us to increase the efficacy of human--agent interaction. We will argue that, within hybrid teams, the capability to communicate with teammates, allies, civilians, and adversaries relies on embodied social cognition processes that are inherently geared toward anthropomorphism and leverage its effects. By updating both the design of autonomous artificial agents and the training of human troops to account for these processes, their reciprocal coordination can be augmented.},
  langid = {english},
  keywords = {Autonomous artificial agents,Autonomous vehicle,Autonomy,Human-agent interaction,Hybrid military teams,Military drone,Situational awareness,Social cognition,Social psychology,Social robotics,Value-sensitive design},
  file = {/Users/nobr/Zotero/storage/PWGCSE2C/Cappuccio et al. - 2022 - Saving Private Robot Risks and Advantages of Anthropomorphism in Agent-Soldier Teams.pdf}
}

@misc{carbon_brief2019,
  title = {Preliminary Figures Show a 3\% Reduction in Global Coal Consumption, in 2019},
  year = {2019}
}

@misc{cartaGroundingLargeLanguage2023,
  title = {Grounding {{Large Language Models}} in {{Interactive Environments}} with {{Online Reinforcement Learning}}},
  author = {Carta, Thomas and Romac, Cl{\'e}ment and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  year = {2023},
  month = sep,
  number = {arXiv:2302.02662},
  eprint = {2302.02662},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.02662},
  urldate = {2023-11-19},
  abstract = {Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/42DKMSIQ/Carta et al. - 2023 - Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/F5RNCEYU/2302.html}
}

@article{carter2016,
  title = {Experiments in {{Handwriting}} with a {{Neural Network}}},
  author = {Carter, Shan and Ha, David and Johnson, Ian and Olah, Chris},
  year = {2016},
  month = dec,
  journal = {Distill},
  volume = {1},
  number = {12},
  pages = {10.23915/distill.00004},
  issn = {2476-0757},
  doi = {10.23915/distill.00004},
  urldate = {2024-06-16}
}

@article{carter2019,
  title = {Activation {{Atlas}}},
  author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  year = {2019},
  month = mar,
  journal = {Distill},
  volume = {4},
  number = {3},
  pages = {10.23915/distill.00015},
  issn = {2476-0757},
  doi = {10.23915/distill.00015},
  urldate = {2024-06-16}
}

@article{carvalho2015sustainability,
  title = {Sustainability of the {{Brazilian}} Steel Industry: Energy Efficiency, Emissions and Competitiveness},
  author = {Carvalho, P. S. L. and others},
  year = {2015},
  journal = {BNDES Sector},
  number = {41},
  pages = {181--236},
  address = {Rio de Janeiro}
}

@article{caticha,
  title = {{{ENTROPIC INFERENCE AND THE FOUNDATIONS OF PHYSICS}}},
  author = {Caticha, Ariel},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/KX8KNYG3/Caticha - ENTROPIC INFERENCE AND THE FOUNDATIONS OF PHYSICS.pdf}
}

@misc{cbs2019disasters,
  title = {2017 and 2018 Were Costliest Ever Years for Natural Disasters},
  author = {{CBS News}},
  year = {2019},
  month = jan
}

@article{cevaal2025,
  title = {Efficient {{mRNA}} Delivery to Resting {{T}} Cells to Reverse {{HIV}} Latency},
  author = {Cevaal, Paula M. and Kan, Stanislav and Fisher, Bridget M. and Moso, Michael A. and Tan, Abigail and Liu, Haiyin and Ali, Abdalla and Tanaka, Kiho and Shepherd, Rory A. and Kim, Youry and Ong, Jesslyn and Furtado, Denzil L. and Holz, Marvin and Purcell, Damian F. J. and Casan, Joshua M. L. and Payne, Thomas and Zhao, Wei and Fareh, Mohamed and McMahon, James H. and Deeks, Steven G. and Hoh, Rebecca and Telwatte, Sushama and Pouton, Colin W. and Johnston, Angus P. R. and Caruso, Frank and Symons, Jori and Lewin, Sharon R. and Roche, Michael},
  year = {2025},
  month = may,
  journal = {Nature Communications},
  volume = {16},
  number = {1},
  pages = {4979},
  issn = {2041-1723},
  doi = {10.1038/s41467-025-60001-2},
  urldate = {2025-06-08},
  abstract = {A major hurdle to curing HIV is the persistence of integrated proviruses in resting CD4+ T cells that remain in a transcriptionally silent, latent state. One strategy to eradicate latent HIV is to activate viral transcription, followed by elimination of infected cells through virus-mediated cytotoxicity or immune-mediated clearance. We hypothesised that mRNA-lipid nanoparticle (LNP) technology would provide an opportunity to deliver mRNA encoding proteins able to reverse HIV latency in resting CD4+ T cells. Here we develop an LNP formulation (LNP X) with unprecedented potency to deliver mRNA to hard-to-transfect resting CD4+ T cells in the absence of cellular toxicity or activation. Encapsulating an mRNA encoding the HIV Tat protein, an activator of HIV transcription, LNP X enhances HIV transcription in ex vivo CD4+ T cells from people living with HIV. LNP X further enables the delivery of clustered regularly interspaced short palindromic repeats (CRISPR) activation machinery to modulate both viral and host gene transcription. These findings offer potential for the development of a range of nucleic acid-based T cell therapeutics.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {HIV infections,PCR-based techniques,Preclinical research,Retrovirus},
  file = {/Users/nobr/Zotero/storage/GPRH5NXX/Cevaal et al. - 2025 - Efficient mRNA delivery to resting T cells to reverse HIV latency.pdf}
}

@misc{chaeLanguageModelsCompilers2024,
  title = {Language {{Models}} as {{Compilers}}: {{Simulating Pseudocode Execution Improves Algorithmic Reasoning}} in {{Language Models}}},
  shorttitle = {Language {{Models}} as {{Compilers}}},
  author = {Chae, Hyungjoo and Kim, Yeonghyeon and Kim, Seungone and Ong, Kai Tzu-iunn and Kwak, Beong-woo and Kim, Moohyeon and Kim, Seonghwan and Kwon, Taeyoon and Chung, Jiwan and Yu, Youngjae and Yeo, Jinyoung},
  year = {2024},
  month = apr,
  number = {arXiv:2404.02575},
  eprint = {2404.02575},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.02575},
  urldate = {2024-04-05},
  abstract = {Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large language models (LLMs), even though they have demonstrated promising performance in other reasoning tasks. Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Program-of-Thought) as inspired by their strict and precise syntaxes. However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call. Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve. This paper presents Think-and-Execute, a novel framework that decomposes the reasoning process of language models into two steps. (1) In Think, we discover a task-level logic that is shared across all instances for solving a given task and then express the logic with pseudocode; (2) In Execute, we further tailor the generated pseudocode to each instance and simulate the execution of the code. With extensive experiments on seven algorithmic reasoning tasks, we demonstrate the effectiveness of Think-and-Execute. Our approach better improves LMs' reasoning compared to several strong baselines performing instance-specific reasoning (e.g., CoT and PoT), suggesting the helpfulness of discovering task-level logic. Also, we show that compared to natural language, pseudocode can better guide the reasoning of LMs, even though they are trained to follow natural language instructions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/VTZPK8NX/Chae et al. - 2024 - Language Models as Compilers Simulating Pseudocode Execution Improves Algorithmic Reasoning in Lang.pdf;/Users/nobr/Zotero/storage/UB5BB88E/2404.html}
}

@misc{chami2022,
  title = {Machine {{Learning}} on {{Graphs}}: {{A Model}} and {{Comprehensive Taxonomy}}},
  shorttitle = {Machine {{Learning}} on {{Graphs}}},
  author = {Chami, Ines and {Abu-El-Haija}, Sami and Perozzi, Bryan and R{\'e}, Christopher and Murphy, Kevin},
  year = {2022},
  month = apr,
  number = {arXiv:2005.03675},
  eprint = {2005.03675},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.03675},
  urldate = {2023-04-25},
  abstract = {There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/93SWRJ8T/Chami et al. - 2022 - Machine Learning on Graphs A Model and Comprehens.pdf;/Users/nobr/Zotero/storage/VLWV5LQ4/2005.html}
}

@inproceedings{chao2020,
  title = {An {{Air Combat Simulation System}} for {{Intelligent Decision-Making}}},
  booktitle = {2020 12th {{International Conference}} on {{Intelligent Human-Machine Systems}} and {{Cybernetics}} ({{IHMSC}})},
  author = {Chao, Liu and Jiafan, He and {Liqingwei} and {Wangman}},
  year = {2020},
  month = aug,
  pages = {104--108},
  publisher = {IEEE},
  address = {Hangzhou, China},
  doi = {10.1109/IHMSC49165.2020.10102},
  urldate = {2024-01-15},
  abstract = {Command and control in modern air combat is showing pressing demand for intelligent decision-making. Research on intelligent decision-making technique relies on data accumulation by combat simulations, which still remains underdeveloped at present. In this paper we introduce a tactical-level air combat simulation system for intelligent decision-making, which can simulate air combat between formations, and has multiple application modes, namely ManMan, Man-Machine, and Machine-Machine. This system can collect diversified and fine-grid data, and provide data support for training of intelligent air-combat decision-making models, validation of intelligent decision-making algorithms, and deduction of tactical plans. A combat example simulated by the system is presented and analyzed in detail.},
  isbn = {978-1-7281-6517-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HC6A4VBH/Chao et al. - 2020 - An Air Combat Simulation System for Intelligent Decision-Making.pdf}
}

@article{chavez-duran2022,
  title = {Mapping {{Homogeneous Response Areas}} for {{Forest Fuel Management Using Geospatial Data}}, {{K-Means}}, and {{Random Forest Classification}}},
  author = {{Ch{\'a}vez-Dur{\'a}n}, {\'A}lvaro Agust{\'i}n and {Olvera-Vargas}, Miguel and {Figueroa-Rangel}, Blanca and Garc{\'i}a, Mariano and Aguado, Inmaculada and {Ruiz-Corral}, Jos{\'e} Ariel},
  year = {2022},
  month = nov,
  journal = {Forests},
  volume = {13},
  number = {12},
  pages = {1970},
  issn = {1999-4907},
  doi = {10.3390/f13121970},
  urldate = {2023-07-04},
  abstract = {Accurate description of forest fuels is necessary for developing appropriate fire management strategies aimed at reducing fire risk. Although field surveys provide accurate measurements of forest fuel load estimations, they are time consuming, expensive, and may fail to capture the inherent spatial heterogeneity of forest fuels. Previous efforts were carried out to solve this issue by estimating homogeneous response areas (HRAs), representing a promising alternative. However, previous methods suffer from a high degree of subjectivity and are difficult to validate. This paper presents a method, which allows eliminating subjectivity in estimating HRAs spatial distribution, using artificial intelligence machine learning techniques. The proposed method was developed in the natural protected area of ``Sierra de Quila,'' Jalisco, and was replicated in ``Sierra de {\'A}lvarez,'' San Luis Potos{\'i} and ``Selva El Ocote,'' Chiapas, Mexico, to prove its robustness. Input data encompassed a set of environmental variables including altitude, average annual precipitation, enhanced vegetation index, and forest canopy height. Four, three, and five HRAs with overall accuracy of 97.78\%, 98.06\%, and 98.92\% were identified at ``Sierra de Quila,'' ``Sierra de {\'A}lvarez,'' and ``Selva El Ocote,'' respectively. Altitude and average annual precipitation were identified as the most explanatory variables in all locations, achieving a mean decrease in impurity values greater than 52.51\% for altitude and up to 36.02\% for average annual precipitation. HRAs showed statistically significant differences in all study sites according to the Kruskal--Wallis test (p-value {$<$} 0.05). Differences among groups were also significant based on the Wilcoxon--Mann--Whitney (p-value {$<$} 0.05) for all variables but EVI in ``Selva El Ocote.'' These results show the potential of our approach to objectively identify distinct homogeneous areas in terms of their fuel properties. This allows the adequate management of fire and forest fuels in decision-making processes.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2N57RGUA/Chávez-Durán et al. - 2022 - Mapping Homogeneous Response Areas for Forest Fuel.pdf}
}

@article{chebotar2023,
  title = {Q-{{Transformer}}: {{Scalable Offline Reinforcement Learning}} via {{Autoregressive Q-Functions}}},
  author = {Chebotar, Yevgen and Vuong, Quan and Irpan, Alex and Hausman, Karol and Xia, Fei and Lu, Yao and Kumar, Aviral and Yu, Tianhe and Herzog, Alexander and Pertsch, Karl and Gopalakrishnan, Keerthana and Ibarz, Julian and Nachum, Ofir and Sontakke, Sumedh and Salazar, Grecia and Tran, Huong T and Peralta, Jodilyn and Tan, Clayton and Manjunath, Deeksha and Singht, Jaspiar and Zitkovich, Brianna and Jackson, Tomas and Rao, Kanishka and Finn, Chelsea and Levine, Sergey},
  year = {2023},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UJQV3NMP/Chebotar et al. - Q-Transformer Scalable Ofﬂine Reinforcement Learning via Autoregressive Q-Functions.pdf}
}

@article{chen,
  title = {Reasoning {{Models Don}}'t {{Always Say What They Think}}},
  author = {Chen, Yanda and Benton, Joe and Radhakrishnan, Ansh and Uesato, Jonathan and Denison, Carson and Schulman, John and Somani, Arushi and Hase, Peter and Wagner, Misha and Roger, Fabien and Mikulik, Vlad and Bowman, Sam and Leike, Jan and Kaplan, Jared and Perez, Ethan},
  abstract = {Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1\% of examples where they use the hint, but the reveal rate is often below 20\%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HM9RMDX6/Chen et al. - Reasoning Models Don’t Always Say What They Think.pdf}
}

@book{chen2008,
  title = {Handbook of {{Data Visualization}}},
  author = {Chen, Chun-houh and H{\"a}rdle, Wolfgang and Unwin, Antony},
  year = {2008},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-33037-0},
  urldate = {2023-11-09},
  isbn = {978-3-540-33036-3 978-3-540-33037-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/EKD9KWVZ/Chen et al. - 2008 - Handbook of Data Visualization.pdf}
}

@article{chen2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2939672},
  urldate = {2023-04-18},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan-tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  isbn = {9781450342322},
  keywords = {Large-scale,Learning,Machine},
  file = {/Users/nobr/Zotero/storage/PA7C4DDW/full-text.pdf}
}

@article{chen2016a,
  title = {Wargame {{Simulation Theory}} and {{Evaluation Method}} for {{Emergency Evacuation}} of {{Residents}} from {{Urban Waterlogging Disaster Area}}},
  author = {Chen, Peng and Zhang, Jiquan and Sun, Yingyue and Liu, Xiaojing},
  year = {2016},
  month = dec,
  journal = {International Journal of Environmental Research and Public Health},
  volume = {13},
  number = {12},
  pages = {1260},
  issn = {1660-4601},
  doi = {10.3390/ijerph13121260},
  urldate = {2024-01-14},
  abstract = {Urban waterlogging seriously threatens the safety of urban residents and properties. Wargame simulation research on resident emergency evacuation from waterlogged areas can determine the effectiveness of emergency response plans for high risk events at low cost. Based on wargame theory and emergency evacuation plans, we used a wargame exercise method, incorporating qualitative and quantitative aspects, to build an urban waterlogging disaster emergency shelter using a wargame exercise and evaluation model. The simulation was empirically tested in Daoli District of Harbin. The results showed that the wargame simulation scored 96.40 points, evaluated as good. From the simulation results, wargame simulation of urban waterlogging emergency procedures for disaster response can improve the flexibility and capacity for command, management and decision-making in emergency management departments.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/G44N2Y2D/Chen et al. - 2016 - Wargame Simulation Theory and Evaluation Method for Emergency Evacuation of Residents from Urban Wat.pdf}
}

@article{chen2023a,
  title = {Online {{Intention Recognition With Incomplete Information Based}} on a {{Weighted Contrastive Predictive Coding Model}} in {{Wargame}}},
  author = {Chen, Li and Liang, Xingxing and Feng, Yanghe and Zhang, Longfei and Yang, Jing and Liu, Zhong},
  year = {2023},
  month = oct,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {10},
  pages = {7515--7528},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3144171},
  urldate = {2024-01-14},
  abstract = {The incomplete and imperfect essence of the battlefield situation results in a challenge to the efficiency, stability, and reliability of traditional intention recognition methods. For this problem, we propose a deep learning architecture that consists of a contrastive predictive coding (CPC) model, a variable-length long short-term memory network (LSTM) model, and an attention weight allocator for online intention recognition with incomplete information in wargame (W-CPCLSTM). First, based on the typical characteristics of intelligence data, a CPC model is designed to capture more global structures from limited battlefield information. Then, a variable-length LSTM model is employed to classify the learned representations into predefined intention categories. Next, a weighted approach to the training attention of CPC and LSTM is introduced to allow for the stability of the model. Finally, performance evaluation and application analysis of the proposed model for the online intention recognition task were carried out based on four different degrees of detection information and a perfect situation of ideal conditions in a wargame. Besides, we explored the effect of different lengths of intelligence data on recognition performance and gave application examples of the proposed model to a wargame platform. The simulation results demonstrate that our method not only contributes to the growth of recognition stability, but it also improves recognition accuracy by 7\%--11\%, 3\%--7\%, 3\%--13\%, and 3\%--7\%, the recognition speed by 6-- 32{\textbackslash}times , 4-- 18{\textbackslash}times , 13--* {\textbackslash}times , and 1-- 6{\textbackslash}times compared with the traditional LSTM, classical FCN, OctConv, and OctFCN models, respectively, which characterizes it as a promising reference tool for command decision-making.},
  file = {/Users/nobr/Zotero/storage/9F6B3KH4/Chen et al. - 2023 - Online Intention Recognition With Incomplete Information Based on a Weighted Contrastive Predictive .pdf;/Users/nobr/Zotero/storage/JGUBEYBJ/9701666.html}
}

@misc{chen2023b,
  title = {{{AgentVerse}}: {{Facilitating Multi-Agent Collaboration}} and {{Exploring Emergent Behaviors}}},
  shorttitle = {{{AgentVerse}}},
  author = {Chen, Weize and Su, Yusheng and Zuo, Jingwei and Yang, Cheng and Yuan, Chenfei and Chan, Chi-Min and Yu, Heyang and Lu, Yaxi and Hung, Yi-Hsin and Qian, Chen and Qin, Yujia and Cong, Xin and Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
  year = {2023},
  month = oct,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AGENTVERSE that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AGENTVERSE framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for AGENTVERSE will soon be released at https://github.com/OpenBMB/AgentVerse.},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/DC5JF2LT/Chen et al. - 2023 - AgentVerse Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors.pdf}
}

@misc{chen2023c,
  title = {{{AutoAgents}}: {{A Framework}} for {{Automatic Agent Generation}}},
  shorttitle = {{{AutoAgents}}},
  author = {Chen, Guangyao and Dong, Siwei and Shu, Yu and Zhang, Ge and Sesay, Jaward and Karlsson, B{\"o}rje F. and Fu, Jie and Shi, Yemin},
  year = {2023},
  month = oct,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multiagent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that AutoAgents generates more coherent and accurate solutions than the existing multi-agent methods. This underscores the significance of assigning different roles to different tasks and of team cooperation, offering new perspectives for tackling complex tasks. The repository of this project is available at https://github.com/Link-AGI/AutoAgents.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/SJTW64KU/Chen et al. - 2023 - AutoAgents A Framework for Automatic Agent Generation.pdf}
}

@misc{chen2023d,
  title = {Towards {{End-to-End Embodied Decision Making}} via {{Multi-modal Large Language Model}}: {{Explorations}} with {{GPT4-Vision}} and {{Beyond}}},
  shorttitle = {Towards {{End-to-End Embodied Decision Making}} via {{Multi-modal Large Language Model}}},
  author = {Chen, Liang and Zhang, Yichi and Ren, Shuhuai and Zhao, Haozhe and Cai, Zefan and Wang, Yuchi and Wang, Peiyi and Liu, Tianyu and Chang, Baobao},
  year = {2023},
  month = oct,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3\%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26\%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/F3XWJA68/Chen et al. - 2023 - Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model Explorations with .pdf}
}

@article{chen2024a,
  title = {Constructing Custom Thermodynamics Using Deep Learning},
  author = {Chen, Xiaoli and Soh, Beatrice W. and Ooi, Zi-En and {Vissol-Gaudin}, Eleonore and Yu, Haijun and Novoselov, Kostya S. and Hippalgaonkar, Kedar and Li, Qianxiao},
  year = {2024},
  month = jan,
  journal = {Nature Computational Science},
  volume = {4},
  number = {1},
  pages = {66--85},
  publisher = {Nature Publishing Group},
  issn = {2662-8457},
  doi = {10.1038/s43588-023-00581-5},
  urldate = {2024-03-12},
  abstract = {One of the most exciting applications of artificial intelligence is automated scientific discovery based on previously amassed data, coupled with restrictions provided by known physical principles, including symmetries and conservation laws. Such automated hypothesis creation and verification can assist scientists in studying complex phenomena, where traditional physical intuition may fail. Here we develop a platform based on a generalized Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. Our method simultaneously constructs reduced thermodynamic coordinates and interprets the dynamics on these coordinates. We demonstrate its effectiveness by studying theoretically and validating experimentally the stretching of long polymer chains in an externally applied field. Specifically, we learn three interpretable thermodynamic coordinates and build a dynamical landscape of polymer stretching, including the identification of stable and transition states and the control of the stretching rate. Our general methodology can be used to address a wide range of scientific and technological applications.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational science,Nonlinear phenomena},
  file = {/Users/nobr/Zotero/storage/BZBBY7GQ/Chen et al. - 2024 - Constructing custom thermodynamics using deep learning.pdf}
}

@misc{chenDrivingLLMsFusing2023,
  title = {Driving with {{LLMs}}: {{Fusing Object-Level Vector Modality}} for {{Explainable Autonomous Driving}}},
  shorttitle = {Driving with {{LLMs}}},
  author = {Chen, Long and Sinavski, Oleg and H{\"u}nermann, Jan and Karnsund, Alice and Willmott, Andrew James and Birch, Danny and Maund, Daniel and Shotton, Jamie},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01957},
  eprint = {2310.01957},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01957},
  urldate = {2023-11-11},
  abstract = {Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/XKWVJMDL/Chen et al. - 2023 - Driving with LLMs Fusing Object-Level Vector Modality for Explainable Autonomous Driving.pdf;/Users/nobr/Zotero/storage/TR4GLLKN/2310.html}
}

@misc{chenSelfPlayFineTuningConverts2024,
  title = {Self-{{Play Fine-Tuning Converts Weak Language Models}} to {{Strong Language Models}}},
  author = {Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.01335},
  eprint = {2401.01335},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01335},
  urldate = {2024-01-08},
  abstract = {Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/ULJBXGZF/Chen et al. - 2024 - Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models.pdf;/Users/nobr/Zotero/storage/I4277UBM/2401.html}
}

@misc{chiangChatbotArenaOpen2024,
  title = {Chatbot {{Arena}}: {{An Open Platform}} for {{Evaluating LLMs}} by {{Human Preference}}},
  shorttitle = {Chatbot {{Arena}}},
  author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04132},
  eprint = {2403.04132},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.04132},
  urldate = {2024-05-16},
  abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at {\textbackslash}url\{https://chat.lmsys.org\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/ZQ7JTLUA/Chiang et al. - 2024 - Chatbot Arena An Open Platform for Evaluating LLMs by Human Preference.pdf;/Users/nobr/Zotero/storage/XY4KR49F/2403.html}
}

@article{child2019,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SVMGCPBA/Child et al. - Generating Long Sequences with Sparse Transformers.pdf}
}

@article{chira2024,
  title = {Geography Is Not Destiny: {{A}} Quantitative Test of {{Diamond}}'s Axis of Orientation Hypothesis},
  shorttitle = {Geography Is Not Destiny},
  author = {Chira, Angela M. and Gray, Russell D. and Botero, Carlos A.},
  year = {2024},
  month = jan,
  journal = {Evolutionary Human Sciences},
  pages = {1--24},
  issn = {2513-843X},
  doi = {10.1017/ehs.2023.34},
  urldate = {2024-02-15},
  abstract = {Jared Diamond suggested that the unique East--West orientation of Eurasia facilitated the spread of cultural innovations and gave it substantial political, technological and military advantages over other continental regions. This controversial hypothesis assumes that innovations can spread more easily across similar habitats, and that environments tend to be more homogeneous at similar latitudes. The resulting prediction is that Eurasia is home to environmentally homogenous corridors that enable fast cultural transmission. Despite indirect evidence supporting Diamond's influential hypothesis, quantitative tests of its underlying assumptions are currently lacking. Here we address this critical gap by leveraging ecological, cultural and linguistic datasets at a global scale. Our analyses show that although societies that share similar ecologies are more likely to share cultural traits, the Eurasian continent is not significantly more ecologically homogeneous than other continental regions. Our findings highlight the perils of single factor explanations and remind us that even the most compelling ideas must be thoroughly tested to gain a solid understanding of the complex history of our species.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/NMMKWF4H/Chira et al. - 2024 - Geography is not destiny A quantitative test of Diamond's axis of orientation hypothesis.pdf}
}

@misc{chiuExploitingInductiveBiases2023,
  title = {Exploiting {{Inductive Biases}} in {{Video Modeling}} through {{Neural CDEs}}},
  author = {Chiu, Johnathan and Duffield, Samuel and {Hunter-Gordon}, Max and Donatella, Kaelan and Aifer, Max and Gu, Andi},
  year = {2023},
  month = nov,
  number = {arXiv:2311.04986},
  eprint = {2311.04986},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.04986},
  urldate = {2024-02-26},
  abstract = {We introduce a novel approach to video modeling that leverages controlled differential equations (CDEs) to address key challenges in video tasks, notably video interpolation and mask propagation. We apply CDEs at varying resolutions leading to a continuous-time U-Net architecture. Unlike traditional methods, our approach does not require explicit optical flow learning, and instead makes use of the inherent continuous-time features of CDEs to produce a highly expressive video model. We demonstrate competitive performance against state-of-the-art models for video interpolation and mask propagation tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/2MWIRHGG/Chiu et al. - 2023 - Exploiting Inductive Biases in Video Modeling through Neural CDEs.pdf;/Users/nobr/Zotero/storage/WPNUTT5S/2311.html}
}

@misc{cho2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = sep,
  number = {arXiv:1406.1078},
  eprint = {1406.1078},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.1078},
  urldate = {2023-06-11},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/HFX6TIF4/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf;/Users/nobr/Zotero/storage/LSU5SEGJ/1406.html}
}

@article{chollet2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  eprint = {1911.01547},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/JPTJM7PD/Chollet - 2019 - On the Measure of Intelligence.pdf}
}

@misc{chollet2019a,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  number = {arXiv:1911.01547},
  eprint = {1911.01547},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.01547},
  urldate = {2024-12-03},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/XIRMTSKX/Chollet - 2019 - On the Measure of Intelligence.pdf;/Users/nobr/Zotero/storage/3Y7YLYB7/1911.html}
}

@article{chomiak2024,
  title = {Time-Series Forecasting through Recurrent Topology},
  author = {Chomiak, Taylor and Hu, Bin},
  year = {2024},
  month = jan,
  journal = {Communications Engineering},
  volume = {3},
  number = {1},
  pages = {1--10},
  issn = {2731-3395},
  doi = {10.1038/s44172-023-00142-8},
  urldate = {2024-11-23},
  abstract = {Time-series forecasting is a practical goal in many areas of science and engineering. Common approaches for forecasting future events often rely on highly parameterized or black-box models. However, these are associated with a variety of drawbacks including critical model assumptions, uncertainties in their estimated input hyperparameters, and computational cost. All of these can limit model selection and performance. Here, we introduce a learning algorithm that avoids these drawbacks. A variety of data types including chaotic systems, macroeconomic data, wearable sensor recordings, and population dynamics are used to show that Forecasting through Recurrent Topology (FReT) can generate multi-step-ahead forecasts of unseen data. With no free parameters or even a need for computationally costly hyperparameter~optimization procedures in high-dimensional parameter space, the simplicity of FReT offers an attractive alternative to complex models where increased model complexity may limit interpretability/explainability and impose unnecessary system-level computational load and power consumption constraints.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Biomedical engineering,Scientific data},
  file = {/Users/nobr/Zotero/storage/ST5AT9BQ/Chomiak and Hu - 2024 - Time-series forecasting through recurrent topology.pdf}
}

@inproceedings{chou2018,
  title = {Unifying and {{Merging Well-trained Deep Neural Networks}} for {{Inference Stage}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Chou, Yi-Min and Chan, Yi-Ming and Lee, Jia-Hong and Chiu, Chih-Yi and Chen, Chu-Song},
  year = {2018},
  month = jul,
  pages = {2049--2056},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Stockholm, Sweden},
  doi = {10.24963/ijcai.2018/283},
  urldate = {2024-11-27},
  abstract = {We propose a novel method to merge convolutional neural-nets for the inference stage. Given two welltrained networks that may have different architectures that handle different tasks, our method aligns the layers of the original networks and merges them into a unified model by sharing the representative codes of weights. The shared weights are further re-trained to fine-tune the performance of the merged model. The proposed method effectively produces a compact model that may run original tasks simultaneously on resource-limited devices. As it preserves the general architectures and leverages the co-used weights of well-trained networks, a substantial training overhead can be reduced to shorten the system development time. Experimental results demonstrate a satisfactory performance and validate the effectiveness of the method.},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7G4EJWYT/Chou et al. - 2018 - Unifying and Merging Well-trained Deep Neural Networks for Inference Stage.pdf}
}

@misc{chung2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  number = {arXiv:1412.3555},
  eprint = {1412.3555},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.3555},
  urldate = {2023-06-11},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/Z8KJD9S6/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf;/Users/nobr/Zotero/storage/NZSCQZXK/1412.html}
}

@article{cilliers1999,
  title = {Complexity and Post-Modernism: Understanding Complex Systems},
  shorttitle = {Complexity and Post-Modernism},
  author = {Cilliers, P. and Spurrett, David},
  year = {1999},
  month = may,
  journal = {South African Journal of Philosophy},
  volume = {18},
  number = {2},
  pages = {258--274},
  issn = {0258-0136, 2073-4867},
  doi = {10.1080/02580136.1999.10878187},
  urldate = {2023-11-09},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/P5K63E82/Cilliers and Spurrett - 1999 - Complexity and post-modernism understanding complex systems.pdf}
}

@misc{clark2020,
  title = {{{ELECTRA}}: {{Pre-training Text Encoders}} as {{Discriminators Rather Than Generators}}},
  shorttitle = {{{ELECTRA}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  year = {2020},
  month = mar,
  number = {arXiv:2003.10555},
  eprint = {2003.10555},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-21},
  abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/GI9AGPD8/2003.10555v1.pdf}
}

@misc{climate_gov2018,
  title = {2017 {{U}}.{{S}}. Billion-Dollar Weather and Climate Disasters: A Historic Year in Context},
  year = {2018},
  month = jan
}

@misc{climate_watch,
  title = {Climate Watch - Historical {{GHG}} Emissions},
  howpublished = {https://www.climatewatchdata.org/}
}

@misc{cloos2024,
  title = {Baba {{Is AI}}: {{Break}} the {{Rules}} to {{Beat}} the {{Benchmark}}},
  shorttitle = {Baba {{Is AI}}},
  author = {Cloos, Nathan and Jens, Meagan and Naim, Michelangelo and Kuo, Yen-Ling and Cases, Ignacio and Barbu, Andrei and Cueva, Christopher J.},
  year = {2024},
  month = jul,
  number = {arXiv:2407.13729},
  eprint = {2407.13729},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.13729},
  urldate = {2025-08-09},
  abstract = {Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine those rules and objectives. To probe these abilities, we developed a new benchmark based on the game Baba Is You where an agent manipulates both objects in the environment and rules, represented by movable tiles with words written on them, to reach a specified goal and win the game. We test three state-of-the-art multimodal large language models (OpenAI GPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically when generalization requires that the rules of the game must be manipulated and combined.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/4DSAUUAZ/Cloos et al. - 2024 - Baba Is AI Break the Rules to Beat the Benchmark.pdf}
}

@misc{cloud2025,
  title = {Subliminal {{Learning}}: {{Language}} Models Transmit Behavioral Traits via Hidden Signals in Data},
  shorttitle = {Subliminal {{Learning}}},
  author = {Cloud, Alex and Le, Minh and Chua, James and Betley, Jan and {Sztyber-Betley}, Anna and Hilton, Jacob and Marks, Samuel and Evans, Owain},
  year = {2025},
  month = jul,
  number = {arXiv:2507.14805},
  eprint = {2507.14805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.14805},
  urldate = {2025-09-05},
  abstract = {We study subliminal learning, a surprising phenomenon where language models transmit behavioral traits via semantically unrelated data. In our main experiments, a ``teacher'' model with some trait T (such as liking owls or being misaligned) generates a dataset consisting solely of number sequences. Remarkably, a ``student'' model trained on this dataset learns T. This occurs even when the data is filtered to remove references to T. We observe the same effect when training on code or reasoning traces generated by the same teacher model. However, we do not observe the effect when the teacher and student have different base models. To help explain our findings, we prove a theoretical result showing that subliminal learning occurs in all neural networks under certain conditions, and demonstrate subliminal learning in a simple MLP classifier. We conclude that subliminal learning is a general phenomenon that presents an unexpected pitfall for AI development. Distillation could propagate unintended traits, even when developers try to prevent this via data filtering.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/PE6FVWH2/Cloud et al. - 2025 - Subliminal Learning Language models transmit behavioral traits via hidden signals in data.pdf}
}

@misc{cnuc_brazil,
  title = {National Cadastre of Conservation Units ({{CNUC}})}
}

@misc{coles2023,
  title = {Thermodynamic {{AI}} and the Fluctuation Frontier},
  author = {Coles, Patrick J. and Szczepanski, Collin and Melanson, Denis and Donatella, Kaelan and Martinez, Antonio J. and Sbahi, Faris},
  year = {2023},
  month = jun,
  number = {arXiv:2302.06584},
  eprint = {2302.06584},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.06584},
  urldate = {2024-02-09},
  abstract = {Many Artificial Intelligence (AI) algorithms are inspired by physics and employ stochastic fluctuations. We connect these physics-inspired AI algorithms by unifying them under a single mathematical framework that we call Thermodynamic AI. Seemingly disparate algorithmic classes can be described by this framework, for example, (1) Generative diffusion models, (2) Bayesian neural networks, (3) Monte Carlo sampling and (4) Simulated annealing. Such Thermodynamic AI algorithms are currently run on digital hardware, ultimately limiting their scalability and overall potential. Stochastic fluctuations naturally occur in physical thermodynamic systems, and such fluctuations can be viewed as a computational resource. Hence, we propose a novel computing paradigm, where software and hardware become inseparable. Our algorithmic unification allows us to identify a single full-stack paradigm, involving Thermodynamic AI hardware, that could accelerate such algorithms. We contrast Thermodynamic AI hardware with quantum computing where noise is a roadblock rather than a resource. Thermodynamic AI hardware can be viewed as a novel form of computing, since it uses a novel fundamental building block. We identify stochastic bits (s-bits) and stochastic modes (s-modes) as the respective building blocks for discrete and continuous Thermodynamic AI hardware. In addition to these stochastic units, Thermodynamic AI hardware employs a Maxwell's demon device that guides the system to produce non-trivial states. We provide a few simple physical architectures for building these devices and we develop a formalism for programming the hardware via gate sequences. We hope to stimulate discussion around this new computing paradigm. Beyond acceleration, we believe it will impact the design of both hardware and algorithms, while also deepening our understanding of the connection between physics and intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Emerging Technologies,Quantum Physics},
  file = {/Users/nobr/Zotero/storage/CBEX7TQ9/Coles et al. - 2023 - Thermodynamic AI and the fluctuation frontier.pdf;/Users/nobr/Zotero/storage/VLCSFAPR/2302.html}
}

@article{coletta2020,
  title = {Network Structure of the Mouse Brain Connectome with Voxel Resolution},
  author = {Coletta, Ludovico and Pagani, Marco and Whitesell, Jennifer D. and Harris, Julie A. and Bernhardt, Boris and Gozzi, Alessandro},
  year = {2020},
  month = dec,
  journal = {Science Advances},
  volume = {6},
  number = {51},
  pages = {eabb7187},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.abb7187},
  urldate = {2024-12-16},
  abstract = {Fine-grained descriptions of brain connectivity are required to understand how neural information is processed and relayed across spatial scales. Previous investigations of the mouse brain connectome have used discrete anatomical parcellations, limiting spatial resolution and potentially concealing network attributes critical to connectome organization. Here, we provide a voxel-level description of the network and hierarchical structure of the directed mouse connectome, unconstrained by regional partitioning. We report a number of previously unappreciated organizational principles in the mammalian brain, including a directional segregation of hub regions into neural sink and sources, and a strategic wiring of neuromodulatory nuclei as connector hubs and critical orchestrators of network communication. We also find that the mouse cortical connectome is hierarchically organized along two superimposed cortical gradients reflecting unimodal-transmodal functional processing and a modality-specific sensorimotor axis, recapitulating a phylogenetically conserved feature of higher mammals. These findings advance our understanding of the foundational wiring principles of the mammalian connectome.},
  file = {/Users/nobr/Zotero/storage/FZULKAID/Coletta et al. - 2020 - Network structure of the mouse brain connectome with voxel resolution.pdf}
}

@book{colledanchise2018,
  title = {Behavior {{Trees}} in {{Robotics}} and {{AI}}: {{An Introduction}}},
  shorttitle = {Behavior {{Trees}} in {{Robotics}} and {{AI}}},
  author = {Colledanchise, Michele and {\"O}gren, Petter},
  year = {2018},
  month = jul,
  eprint = {1709.00084},
  primaryclass = {cs},
  doi = {10.1201/9780429489105},
  urldate = {2023-12-29},
  abstract = {A Behavior Tree (BT) is a way to structure the switching between different tasks in an autonomous agent, such as a robot or a virtual entity in a computer game. BTs are a very efficient way of creating complex systems that are both modular and reactive. These properties are crucial in many applications, which has led to the spread of BT from computer game programming to many branches of AI and Robotics. In this book, we will first give an introduction to BTs, then we describe how BTs relate to, and in many cases generalize, earlier switching structures. These ideas are then used as a foundation for a set of efficient and easy to use design principles. Properties such as safety, robustness, and efficiency are important for an autonomous system, and we describe a set of tools for formally analyzing these using a state space description of BTs. With the new analysis tools, we can formalize the descriptions of how BTs generalize earlier approaches. We also show the use of BTs in automated planning and machine learning. Finally, we describe an extended set of tools to capture the behavior of Stochastic BTs, where the outcomes of actions are described by probabilities. These tools enable the computation of both success probabilities and time to completion.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/KZ9T3V6H/Colledanchise and Ögren - 2018 - Behavior Trees in Robotics and AI An Introduction.pdf;/Users/nobr/Zotero/storage/5F3B2PZE/1709.html}
}

@article{colledanchise2019,
  title = {Learning of {{Behavior Trees}} for {{Autonomous Agents}}},
  author = {Colledanchise, Michele and Parasuraman, Ramviyas and {\"O}gren, Petter},
  year = {2019},
  month = jun,
  journal = {IEEE Transactions on Games},
  volume = {11},
  number = {2},
  pages = {183--189},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TG.2018.2816806},
  urldate = {2023-11-12},
  abstract = {Definition of an accurate system model for Automated Planner (AP) is often impractical, especially for real-world problems. Conversely, off-the-shelf planners fail to scale up and are domain dependent. These drawbacks are inherited from conventional transition systems such as Finite State Machines (FSMs) that describes the action-plan execution generated by the AP. On the other hand, Behavior Trees (BTs) represent a valid alternative to FSMs presenting many advantages in terms of modularity, reactiveness, scalability and domain-independence.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/VTTH3WXR/Colledanchise et al. - 2019 - Learning of Behavior Trees for Autonomous Agents.pdf}
}

@book{collier2024,
  title = {Tor: From the {{Dark Web}} to the Future of Privacy},
  shorttitle = {Tor},
  author = {Collier, Ben},
  year = {2024},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Through a social history of the Tor network (often known as the "Dark Web"), this book develops a rich and novel approach to understanding privacy technologies and their implication in crime, harm, control, and resistance"--},
  isbn = {978-0-262-54818-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2GTXQNKF/Collier - 2024 - Tor from the Dark Web to the future of privacy.pdf}
}

@article{collins1969,
  title = {Retrieval Time from Semantic Memory},
  author = {Collins, Allan M. and Quillian, M. Ross},
  year = {1969},
  month = apr,
  journal = {Journal of Verbal Learning and Verbal Behavior},
  volume = {8},
  number = {2},
  pages = {240--247},
  issn = {00225371},
  doi = {10.1016/S0022-5371(69)80069-1},
  urldate = {2024-02-07},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/G5SGQRRY/Collins and Quillian - 1969 - Retrieval time from semantic memory.pdf}
}

@misc{conmyAutomatedCircuitDiscovery2023,
  title = {Towards {{Automated Circuit Discovery}} for {{Mechanistic Interpretability}}},
  author = {Conmy, Arthur and {Mavor-Parker}, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and {Garriga-Alonso}, Adri{\`a}},
  year = {2023},
  month = oct,
  number = {arXiv:2304.14997},
  eprint = {2304.14997},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.14997},
  urldate = {2024-01-21},
  abstract = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/TS6WVIER/Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanistic Interpretability.pdf;/Users/nobr/Zotero/storage/PRURPKAG/2304.html}
}

@book{conrad1902,
  title = {Typhoon},
  author = {Conrad, Joseph},
  year = {1902},
  publisher = {Pall Mall Magazine},
  address = {United Kingdom},
  urldate = {2025-05-12},
  abstract = {Free kindle book and epub digitized and proofread by volunteers.},
  langid = {american}
}

@article{conselice2016,
  title = {{{THE EVOLUTION OF GALAXY NUMBER DENSITY AT}} z {$<$} 8 {{AND ITS IMPLICATIONS}}},
  author = {Conselice, Christopher J. and Wilkinson, Aaron and Duncan, Kenneth and Mortlock, Alice},
  year = {2016},
  month = oct,
  journal = {The Astrophysical Journal},
  volume = {830},
  number = {2},
  pages = {83},
  publisher = {The American Astronomical Society},
  issn = {0004-637X},
  doi = {10.3847/0004-637X/830/2/83},
  urldate = {2024-01-23},
  abstract = {The evolution of the number density of galaxies in the universe, and thus also the total number of galaxies, is a fundamental question with implications for a host of astrophysical problems including galaxy evolution and cosmology. However, there has never been a detailed study of this important measurement, nor a clear path to answer it. To address this we use observed galaxy stellar mass functions up to z {$\sim$} 8 to determine how the number densities of galaxies change as a function of time and mass limit. We show that the increase in the total number density of galaxies ({$\phi$}T), more massive than M* = 106 M{$\odot$}, decreases as {$\phi$}T {$\sim$} t-1, where t is the age of the universe. We further show that this evolution turns over and rather increases with time at higher mass lower limits of M* {$>$} 107 M{$\odot$}. By using the M* = 106 M{$\odot$} lower limit we further show that the total number of galaxies in the universe up to z = 8 is (2 trillion), almost a factor of 10 higher than would be seen in an all sky survey at Hubble Ultra-Deep Field depth. We discuss the implications for these results for galaxy evolution, as well as compare our results with the latest models of galaxy formation. These results also reveal that the cosmic background light in the optical and near-infrared likely arise from these unobserved faint galaxies. We also show how these results solve the question of why the sky at night is dark, otherwise known as Olbers' paradox.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/H447F3M5/Conselice et al. - 2016 - THE EVOLUTION OF GALAXY NUMBER DENSITY AT z  8 AND ITS IMPLICATIONS.pdf}
}

@misc{conteThermodynamicComputing2019,
  title = {Thermodynamic {{Computing}}},
  author = {Conte, Tom and DeBenedictis, Erik and Ganesh, Natesh and Hylton, Todd and Strachan, John Paul and Williams, R. Stanley and Alemi, Alexander and Altenberg, Lee and Crooks, Gavin and Crutchfield, James and {del Rio}, Lidia and Deutsch, Josh and DeWeese, Michael and Douglas, Khari and Esposito, Massimiliano and Frank, Michael and Fry, Robert and Harsha, Peter and Hill, Mark and Kello, Christopher and Krichmar, Jeff and Kumar, Suhas and Liu, Shih-Chii and Lloyd, Seth and Marsili, Matteo and Nemenman, Ilya and Nugent, Alex and Packard, Norman and Randall, Dana and Sadowski, Peter and Santhanam, Narayana and Shaw, Robert and Stieg, Adam and Stopnitzky, Elan and Teuscher, Christof and Watkins, Chris and Wolpert, David and Yang, Joshua and Yufik, Yan},
  year = {2019},
  month = nov,
  number = {arXiv:1911.01968},
  eprint = {1911.01968},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.01968},
  urldate = {2024-02-09},
  abstract = {The hardware and software foundations laid in the first half of the 20th Century enabled the computing technologies that have transformed the world, but these foundations are now under siege. The current computing paradigm, which is the foundation of much of the current standards of living that we now enjoy, faces fundamental limitations that are evident from several perspectives. In terms of hardware, devices have become so small that we are struggling to eliminate the effects of thermodynamic fluctuations, which are unavoidable at the nanometer scale. In terms of software, our ability to imagine and program effective computational abstractions and implementations are clearly challenged in complex domains. In terms of systems, currently five percent of the power generated in the US is used to run computing systems - this astonishing figure is neither ecologically sustainable nor economically scalable. Economically, the cost of building next-generation semiconductor fabrication plants has soared past \$10 billion. All of these difficulties - device scaling, software complexity, adaptability, energy consumption, and fabrication economics - indicate that the current computing paradigm has matured and that continued improvements along this path will be limited. If technological progress is to continue and corresponding social and economic benefits are to continue to accrue, computing must become much more capable, energy efficient, and affordable. We propose that progress in computing can continue under a united, physically grounded, computational paradigm centered on thermodynamics. Herein we propose a research agenda to extend these thermodynamic foundations into complex, non-equilibrium, self-organizing systems and apply them holistically to future computing systems that will harness nature's innate computational capacity. We call this type of computing "Thermodynamic Computing" or TC.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Emerging Technologies},
  file = {/Users/nobr/Zotero/storage/TE4I3DPG/Conte et al. - 2019 - Thermodynamic Computing.pdf;/Users/nobr/Zotero/storage/Z9IBJD3Z/1911.html}
}

@article{conti2017,
  title = {Improving {{Exploration}} in {{Evolution Strategies}} for {{Deep Reinforcement Learning}} via a {{Population}} of {{Novelty-Seeking Agents}}},
  author = {Conti, Edoardo and Madhavan, Vashisht and Such, F. and Lehman, J. and Stanley, Kenneth O. and Clune, J.},
  year = {2017},
  month = dec,
  journal = {ArXiv},
  urldate = {2024-01-02},
  abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
  file = {/Users/nobr/Zotero/storage/ZPVLQZEM/Conti et al. - 2017 - Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of No.pdf}
}

@misc{conway2006,
  title = {The {{Free Will Theorem}}},
  author = {Conway, John and Kochen, Simon},
  year = {2006},
  month = apr,
  journal = {arXiv.org},
  doi = {10.1007/s10701-006-9068-6},
  urldate = {2024-02-25},
  abstract = {On the basis of three physical axioms, we prove that if the choice of a particular type of spin 1 experiment is not a function of the information accessible to the experimenters, then its outcome is equally not a function of the information accessible to the particles. We show that this result is robust, and deduce that neither hidden variable theories nor mechanisms of the GRW type for wave function collapse can be made relativistic. We also establish the consistency of our axioms and discuss the philosophical implications.},
  howpublished = {https://arxiv.org/abs/quant-ph/0604079v1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JJ4SL3I5/Conway and Kochen - 2006 - The Free Will Theorem.pdf}
}

@article{cooper2010,
  title = {Predicting Protein Structures with a Multiplayer Online Game},
  author = {Cooper, Seth and Khatib, Firas and Treuille, Adrien and Barbero, Janos and Lee, Jeehyung and Beenen, Michael and {Leaver-Fay}, Andrew and Baker, David and Popovi{\'c}, Zoran and Players, Foldit},
  year = {2010},
  month = aug,
  journal = {Nature 2010 466:7307},
  volume = {466},
  number = {7307},
  pages = {756--760},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature09304},
  urldate = {2023-03-22},
  abstract = {A natural polypeptide chain can fold into a native protein in microseconds, but predicting such stable three-dimensional structure from any given amino-acid sequence and first physical principles remains a formidable computational challenge. Aiming to recruit human visual and strategic powers to the task, Seth Cooper, David Baker and colleagues turned their 'Rosetta' structure-prediction algorithm into an online multiplayer game called Foldit, in which thousands of non-scientists competed and collaborated to produce a rich set of new algorithms and search strategies for protein structure refinement. The work shows that even computationally complex scientific problems can be effectively crowd-sourced using interactive multiplayer games. Predicting the structure of a folded protein from first principles for any given amino-acid sequence remains a formidable computational challenge. To recruit human abilities to the task, these authors turned their Rosetta structure prediction algorithm into an online multiplayer game in which thousands of non-scientists competed and collaborated to produce new algorithms and search strategies for protein structure refinement. This shows that computationally complex problems can be effectively 'crowd-sourced' through interactive multiplayer games. People exert large amounts of problem-solving effort playing computer games. Simple image- and text-recognition tasks have been successfully `crowd-sourced' through games1,2,3, but it is not clear if more complex scientific problems can be solved with human-directed computing. Protein structure prediction is one such problem: locating the biologically relevant native conformation of a protein is a formidable computational challenge given the very large size of the search space. Here we describe Foldit, a multiplayer online game that engages non-scientists in solving hard prediction problems. Foldit players interact with protein structures using direct manipulation tools and user-friendly versions of algorithms from the Rosetta structure prediction methodology4, while they compete and collaborate to optimize the computed energy. We show that top-ranked Foldit players excel at solving challenging structure refinement problems in which substantial backbone rearrangements are necessary to achieve the burial of hydrophobic residues. Players working collaboratively develop a rich assortment of new strategies and algorithms; unlike computational approaches, they explore not only the conformational space but also the space of possible search strategies. The integration of human visual problem-solving and strategy development capabilities with traditional computational algorithms through interactive multiplayer games is a powerful new approach to solving computationally-limited scientific problems.},
  pmid = {20686574},
  keywords = {Computational science,Protein structure predictions,Structural biology},
  file = {/Users/nobr/Zotero/storage/TDE66JC6/Cooper et al. - 2010 - Predicting protein structures with a multiplayer o.pdf}
}

@misc{cosentinoReasoningLargeLanguage2024,
  title = {Reasoning in {{Large Language Models}}: {{A Geometric Perspective}}},
  shorttitle = {Reasoning in {{Large Language Models}}},
  author = {Cosentino, Romain and Shekkizhar, Sarath},
  year = {2024},
  month = jul,
  number = {arXiv:2407.02678},
  eprint = {2407.02678},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2407.02678},
  urldate = {2024-07-07},
  abstract = {The advancement of large language models (LLMs) for real-world applications hinges critically on enhancing their reasoning capabilities. In this work, we explore the reasoning abilities of large language models (LLMs) through their geometrical understanding. We establish a connection between the expressive power of LLMs and the density of their self-attention graphs. Our analysis demonstrates that the density of these graphs defines the intrinsic dimension of the inputs to the MLP blocks. We demonstrate through theoretical analysis and toy examples that a higher intrinsic dimension implies a greater expressive capacity of the LLM. We further provide empirical evidence linking this geometric framework to recent advancements in methods aimed at enhancing the reasoning capabilities of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/F2KMJXT2/Cosentino and Shekkizhar - 2024 - Reasoning in Large Language Models A Geometric Perspective.pdf}
}

@book{cover2006,
  title = {Elements of Information Theory},
  author = {Cover, T. M. and Thomas, Joy A.},
  year = {2006},
  edition = {2nd ed},
  publisher = {Wiley-Interscience},
  address = {Hoboken, N.J},
  isbn = {978-0-471-24195-9},
  lccn = {Q360 .C68 2006},
  keywords = {Information theory},
  annotation = {OCLC: ocm59879802},
  file = {/Users/nobr/Zotero/storage/WY22UBZV/Cover and Thomas - 2006 - ELEMENTS OF INFORMATION THEORY.pdf}
}

@article{cranmer2020,
  title = {The Frontier of Simulation-Based Inference},
  author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
  year = {2020},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  pages = {30055--30062},
  doi = {10.1073/pnas.1912789117},
  file = {/Users/nobr/Zotero/storage/L5Q2RSA3/Cranmer et al. - 2020 - The frontier of simulation-based inference.pdf}
}

@article{crawford2019,
  title = {Anatomy of an {{AI System}}},
  author = {Crawford, Kate and Joler, Vladan},
  year = {2019},
  month = dec,
  journal = {Virtual Creativity},
  volume = {9},
  number = {1},
  pages = {117--120},
  issn = {2397-9704},
  doi = {10.1386/vcr_00008_7},
  urldate = {2023-12-21},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2QPDMKVW/Crawford and Joler - 2019 - Anatomy of an AI System.pdf}
}

@article{creanza2017,
  title = {Cultural Evolutionary Theory: {{How}} Culture Evolves and Why It Matters},
  author = {Creanza, Nicole and Kolodny, Oren and Feldman, Marcus W.},
  year = {2017},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {30},
  pages = {7782--7789},
  doi = {10.1073/pnas.1620732114},
  file = {/Users/nobr/Zotero/storage/TQLVJNCR/Creanza et al. - 2017 - Cultural evolutionary theory How culture evolves and why it matters.pdf}
}

@article{crichton2024,
  title = {A {{Core Calculus}} for {{Documents}}},
  author = {Crichton, Will and Krishnamurthi, Shriram},
  year = {2024},
  volume = {8},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/643JYFT9/Crichton and Krishnamurthi - A Core Calculus for Documents.pdf}
}

@article{crosby2019,
  title = {The {{Animal-AI Olympics}}},
  author = {Crosby, Matthew and Beyret, Benjamin and Halina, Marta},
  year = {2019},
  month = may,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {5},
  pages = {257--257},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0050-3},
  urldate = {2024-12-03},
  abstract = {A new competition presents AI agents with cognition challenges to test their animal intelligence.},
  copyright = {2019 Springer Nature Limited},
  langid = {english},
  keywords = {Animal behaviour,Computer science},
  file = {/Users/nobr/Zotero/storage/Q8GG2YRR/Crosby et al. - 2019 - The Animal-AI Olympics.pdf}
}

@misc{csefalvay2023,
  title = {Self-{{Compressing Neural Networks}}},
  author = {Cs{\'e}falvay, Szabolcs and Imber, James},
  year = {2023},
  month = jan,
  number = {arXiv:2301.13142},
  eprint = {2301.13142},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2301.13142},
  urldate = {2024-08-05},
  abstract = {This work focuses on reducing neural network size, which is a major driver of neural network execution time, power consumption, bandwidth, and memory footprint. A key challenge is to reduce size in a manner that can be exploited readily for efficient training and inference without the need for specialized hardware. We propose Self-Compression: a simple, general method that simultaneously achieves two goals: (1) removing redundant weights, and (2) reducing the number of bits required to represent the remaining weights. This is achieved using a generalized loss function to minimize overall network size. In our experiments we demonstrate floating point accuracy with as few as 3\% of the bits and 18\% of the weights remaining in the network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/BTJ7K76D/Cséfalvay and Imber - 2023 - Self-Compressing Neural Networks.pdf}
}

@article{cuevas2013,
  title = {A Swarm Optimization Algorithm Inspired in the Behavior of the Social-Spider},
  author = {Cuevas, Erik and Cienfuegos, Miguel and Zald{\'i}var, Daniel and {P{\'e}rez-Cisneros}, Marco},
  year = {2013},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {40},
  number = {16},
  pages = {6374--6384},
  issn = {09574174},
  doi = {10.1016/j.eswa.2013.05.041},
  urldate = {2023-11-28},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/FX87NIBT/Cuevas et al. - 2013 - A swarm optimization algorithm inspired in the behavior of the social-spider.pdf;/Users/nobr/Zotero/storage/SW47W2R5/A swarm optimization algorithm inspired in the behavior of -- Cuevas, Erik\; Cienfuegos, Miguel\; Zaldívar, Daniel\; -- Expert Systems with Applications, -- 10.1016_j.eswa.2013.05.041 -- cc19addff3a21190b8.pdf}
}

@article{cully2015,
  title = {Robots That Can Adapt like Animals},
  author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  eprint = {1407.3501},
  primaryclass = {cs, q-bio},
  pages = {503--507},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14422},
  urldate = {2024-05-14},
  abstract = {As robots leave the controlled environments of factories to autonomously function in more complex, natural environments, they will have to respond to the inevitable fact that they will become damaged. However, while animals can quickly adapt to a wide variety of injuries, current robots cannot "think outside the box" to find a compensatory behavior when damaged: they are limited to their pre-specified self-sensing abilities, can diagnose only anticipated failure modes, and require a pre-programmed contingency plan for every type of potential damage, an impracticality for complex robots. Here we introduce an intelligent trial and error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans. Before deployment, a robot exploits a novel algorithm to create a detailed map of the space of high-performing behaviors: This map represents the robot's intuitions about what behaviors it can perform and their value. If the robot is damaged, it uses these intuitions to guide a trial-and-error learning algorithm that conducts intelligent experiments to rapidly discover a compensatory behavior that works in spite of the damage. Experiments reveal successful adaptations for a legged robot injured in five different ways, including damaged, broken, and missing legs, and for a robotic arm with joints broken in 14 different ways. This new technique will enable more robust, effective, autonomous robots, and suggests principles that animals may use to adapt to injury.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/BUDS5Z3J/Cully et al. - 2015 - Robots that can adapt like animals.pdf}
}

@article{cunden2020,
  title = {Multi-Level Constraints Wind Farms Siting for a Complex Terrain in a Tropical Region Using {{MCDM}} Approach Coupled with {{GIS}}},
  author = {Cunden, Tyagaraja S.M. and Doorga, Jay and Lollchund, Michel R. and Rughooputh, Soonil D.D.V.},
  year = {2020},
  month = nov,
  journal = {Energy},
  volume = {211},
  pages = {118533},
  issn = {03605442},
  doi = {10.1016/j.energy.2020.118533},
  urldate = {2023-07-15},
  abstract = {This paper presents a Multi-Criteria Decision Making (MCDM) process together with Geographic Information System (GIS) to analyse multiple constraints that affect the siting of wind farms. Firstly, exclusion zones are identified and are removed from the potential wind farm placement regions of the wind resource map layer. A set of nine selected evaluation criteria are weighted using the Analytical Hierarchy Process (AHP) in order to quantify their impacts on the placements of the wind farms. Constraints such as buffer zones and suitability scores are developed. Using these constraints, raster layers including the proximity to road network and high voltage transmission lines are created. Theses layers are then superimposed on raster layers comprising of high-resolution terrain DEM and raster dataset for wind speed and direction. The combination of the raster layers is done using the Weighted Linear Combination (WLC) technique to yield a wind resource map highlighting the regions for wind farms placements, classified according to their potential wind power density. As a case study, the methodology is applied to the island of Mauritius which has a complex topography.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/MXFN2PXX/Cunden et al. - 2020 - Multi-level constraints wind farms siting for a co.pdf}
}

@article{cybenko1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals, and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  urldate = {2024-11-25},
  abstract = {In this paper we demonstrate that finite linear combinations of com positions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/F3F7T9ZA/Cybenko - 1989 - Approximation by superpositions of a sigmoidal function.pdf}
}

@article{dado2022,
  title = {Hyperrealistic Neural Decoding for Reconstructing Faces from {{fMRI}} Activations via the {{GAN}} Latent Space},
  author = {Dado, Thirza and G{\"u}{\c c}l{\"u}t{\"u}rk, Ya{\u g}mur and Ambrogioni, Luca and Ras, Gabri{\"e}lle and Bosch, Sander and Van Gerven, Marcel and G{\"u}{\c c}l{\"u}, Umut},
  year = {2022},
  month = jan,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {141},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-03938-w},
  urldate = {2023-05-18},
  abstract = {Abstract             Neural decoding can be conceptualized as the problem of mapping brain responses back to sensory stimuli via a feature space. We introduce (i) a novel experimental paradigm that uses well-controlled yet highly naturalistic stimuli with a priori known feature representations and (ii) an implementation thereof for HYPerrealistic reconstruction of PERception (HYPER) of faces from brain recordings. To this end, we embrace the use of generative adversarial networks (GANs) at the earliest step of our neural decoding pipeline by acquiring fMRI data as participants perceive face images synthesized by the generator network of a GAN. We show that the latent vectors used for generation effectively capture the same defining stimulus properties as the fMRI measurements. As such, these latents (conditioned on the GAN) are used as the in-between feature representations underlying the perceived images that can be predicted in neural decoding for (re-)generation of the originally perceived stimuli, leading to the most accurate reconstructions of perception to date.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SE2VEPZW/Dado et al. - 2022 - Hyperrealistic neural decoding for reconstructing .pdf}
}

@article{dalvi2004,
  title = {Adversarial Classification},
  author = {Dalvi, Nilesh and Domingos, Pedro and {Mausam} and Sanghai, Sumit and Verma, Deepak},
  year = {2004},
  pages = {99--108},
  doi = {10.1145/1014052.1014066},
  file = {/Users/nobr/Zotero/storage/7C2DKITY/Dalvi et al. - 2004 - Adversarial classification.pdf}
}

@book{dans2023,
  title = {Mandate for Leadership: The Conservative Promise 2025},
  shorttitle = {Mandate for Leadership},
  editor = {Dans, Paul and Groves, Steven},
  year = {2023},
  publisher = {The Heritage Foundation},
  address = {Washington, DC},
  abstract = {The comprehensive policy guide for a new conservative President, offering specific reforms and proposals for Cabinet departments and federal agencies, pulled from the expertise of the entire conservative movement},
  collaborator = {Roberts, Kevin},
  langid = {english},
  annotation = {OCLC: 1444085205},
  file = {/Users/nobr/Zotero/storage/3ZZKMEIW/Dans and Groves - 2023 - Mandate for leadership the conservative promise 2025.pdf}
}

@misc{dantasAsaPyPythonLibrary2023,
  title = {{{AsaPy}}: {{A Python Library}} for {{Aerospace Simulation Analysis}}},
  shorttitle = {{{AsaPy}}},
  author = {Dantas, Joao P. A. and Silva, Samara R. and Gomes, Vitor C. F. and Costa, Andre N. and Samersla, Adrisson R. and Geraldo, Diego and Maximo, Marcos R. O. A. and Yoneyama, Takashi},
  year = {2023},
  month = jul,
  number = {arXiv:2310.00001},
  eprint = {2310.00001},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.00001},
  urldate = {2024-01-15},
  abstract = {AsaPy is a custom-made Python library designed to simplify and optimize the analysis of simulation data. It offers a range of features, including the design of experiment methods, statistical analysis techniques, machine learning algorithms, and data visualization tools. AsaPy's flexibility and customizability make it a viable solution for engineers and researchers who need to quickly gain insights into constructive simulations. AsaPy is built on top of popular scientific computing libraries, ensuring high performance and scalability. In this work, we provide an overview of the key features and capabilities of AsaPy, followed by an exposition of its architecture and demonstrations of its effectiveness through some use cases applied in military operational simulations. We also evaluate how other simulation tools deal with data science, highlighting AsaPy's strengths and advantages. Finally, we discuss potential use cases and applications of AsaPy and outline future directions for the development and improvement of the library.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software},
  file = {/Users/nobr/Zotero/storage/337NPVHL/Dantas et al. - 2023 - AsaPy A Python Library for Aerospace Simulation Analysis.pdf;/Users/nobr/Zotero/storage/N7F2BF7K/2310.html}
}

@misc{dantasASASimulationEnvironment2022,
  title = {{{ASA}}: {{A Simulation Environment}} for {{Evaluating Military Operational Scenarios}}},
  shorttitle = {{{ASA}}},
  author = {Dantas, Joao P. A. and Costa, Andre N. and Gomes, Vitor C. F. and Kuroswiski, Andre R. and Medeiros, Felipe L. L. and Geraldo, Diego},
  year = {2022},
  month = jun,
  number = {arXiv:2207.12084},
  eprint = {2207.12084},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-15},
  abstract = {The Aerospace Simulation Environment (Ambiente de Simula{\textbackslash}c\{c\}{\textbackslash}{\textasciitilde}ao Aeroespacial -- ASA in Portuguese) is a custom-made object-oriented simulation framework developed mainly in C++ that enables the modeling and simulation of military operational scenarios to support the development of tactics and procedures in the aerospace context for the Brazilian Air Force. This work describes the ASA framework, bringing its distributed architecture for managing multiple simulation machines, a data analysis platform for post-processing simulation data, the capability of loading models at simulation runtime, and a batch mode execution platform to perform multiple independent executions simultaneously. In addition, we present a list of recent works using the ASA framework as a simulation tool in the air combat context.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing},
  file = {/Users/nobr/Zotero/storage/HGKQU5AG/Dantas et al. - 2022 - ASA A Simulation Environment for Evaluating Military Operational Scenarios.pdf;/Users/nobr/Zotero/storage/3CF7TWDG/2207.html}
}

@misc{dantasAutonomousAgentVisual2023,
  title = {Autonomous {{Agent}} for {{Beyond Visual Range Air Combat}}: {{A Deep Reinforcement Learning Approach}}},
  shorttitle = {Autonomous {{Agent}} for {{Beyond Visual Range Air Combat}}},
  author = {Dantas, Joao P. A. and Maximo, Marcos R. O. A. and Yoneyama, Takashi},
  year = {2023},
  month = apr,
  number = {arXiv:2304.09669},
  eprint = {2304.09669},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-14},
  abstract = {This work contributes to developing an agent based on deep reinforcement learning capable of acting in a beyond visual range (BVR) air combat simulation environment. The paper presents an overview of building an agent representing a high-performance fighter aircraft that can learn and improve its role in BVR combat over time based on rewards calculated using operational metrics. Also, through self-play experiments, it expects to generate new air combat tactics never seen before. Finally, we hope to examine a real pilot's ability, using virtual simulation, to interact in the same environment with the trained agent and compare their performances. This research will contribute to the air combat training context by developing agents that can interact with real pilots to improve their performances in air defense missions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/3SN3YKJU/Dantas et al. - 2023 - Autonomous Agent for Beyond Visual Range Air Combat A Deep Reinforcement Learning Approach.pdf;/Users/nobr/Zotero/storage/I2AFYI9N/2304.html}
}

@book{danziger2008,
  title = {Naming the Mind: How Psychology Found Its Language},
  shorttitle = {Naming the Mind},
  author = {Danziger, Kurt},
  year = {2008},
  publisher = {Sage Publ},
  address = {London},
  isbn = {978-0-8039-7762-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/KDFYNHQZ/Danziger - 2008 - Naming the mind how psychology found its language.pdf}
}

@techreport{dasgupta_climate,
  title = {A Climate Finance Initiative to Achieve the Paris Agreement and Strengthen Sustainable Development},
  author = {Dasgupta, Dipak and Hourcade, Jean Charles and Nafo, Seyni}
}

@article{dasilvacastanheira2021,
  title = {Brief Segments of Neurophysiological Activity Enable Individual Differentiation},
  author = {{da Silva Castanheira}, Jason and Orozco Perez, Hector Domingo and Misic, Bratislav and Baillet, Sylvain},
  year = {2021},
  month = sep,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5713},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25895-8},
  urldate = {2023-05-13},
  abstract = {Large, openly available datasets and current analytic tools promise the emergence of population neuroscience. The considerable diversity in personality traits and behaviour between individuals is reflected in the statistical variability of neural data collected in such repositories. Recent studies with functional magnetic resonance imaging (fMRI) have concluded that patterns of resting-state functional connectivity can both successfully distinguish individual participants within a cohort and predict some individual traits, yielding the notion of an individual's neural fingerprint. Here, we aim to clarify the neurophysiological foundations of individual differentiation from features of the rich and complex dynamics of resting-state brain activity using magnetoencephalography (MEG) in 158 participants. We show that akin to fMRI approaches, neurophysiological functional connectomes enable the differentiation of individuals, with rates similar to those seen with fMRI. We also show that individual differentiation is equally successful from simpler measures of the spatial distribution of neurophysiological spectral signal power. Our data further indicate that differentiation can be achieved from brain recordings as short as 30\,seconds, and that it is robust over time: the neural fingerprint is present in recordings performed weeks after their baseline reference data was collected. This work, thus, extends the notion of a~neural or brain fingerprint to fast and large-scale resting-state electrophysiological dynamics.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Neurophysiology,Neuroscience},
  file = {/Users/nobr/Zotero/storage/AVA57IHK/da Silva Castanheira et al. - 2021 - Brief segments of neurophysiological activity enab.pdf}
}

@article{davies2021,
  title = {Advancing Mathematics by Guiding Human Intuition with {{AI}}},
  author = {Davies, Alex and Veli{\v c}kovi{\'c}, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma{\v s}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh{\'a}sz, Andr{\'a}s and Lackenby, Marc and Williamson, Geordie and Hassabis, Demis and Kohli, Pushmeet},
  year = {2021},
  month = dec,
  journal = {Nature},
  volume = {600},
  number = {7887},
  pages = {70--74},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04086-x},
  urldate = {2023-04-26},
  abstract = {The practice of mathematics involves discovering patterns and using these to formulate and prove conjectures, resulting in theorems. Since the 1960s, mathematicians have used computers to assist in the discovery of patterns and formulation of conjectures1, most famously in the Birch and Swinnerton-Dyer conjecture2, a Millennium Prize Problem3. Here we provide examples of new fundamental results in pure mathematics that have been discovered with the assistance of machine learning---demonstrating a method by which machine learning can aid mathematicians in discovering new conjectures and theorems. We propose a process of using machine learning to discover potential patterns and relations between mathematical objects, understanding them with attribution techniques and using these observations to guide intuition and propose conjectures. We outline this machine-learning-guided framework and demonstrate its successful application to current research questions in distinct areas of pure mathematics, in each case showing how it led to meaningful mathematical contributions on important open problems: a new connection between the algebraic and geometric structure of knots, and a candidate algorithm predicted by the combinatorial invariance conjecture for symmetric groups4. Our work may serve as a model for collaboration between the fields of mathematics and artificial intelligence (AI) that can achieve surprising results by leveraging the respective strengths of mathematicians and machine learning.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computer science,Pure mathematics,Statistics},
  file = {/Users/nobr/Zotero/storage/TAAZ4YGZ/Davies et al. - 2021 - Advancing mathematics by guiding human intuition w.pdf}
}

@book{dawson2006,
  title = {Logical Dilemmas: The Life and Work of {{Kurt G{\"o}del}}},
  shorttitle = {Logical Dilemmas},
  author = {Dawson, John W. and G{\"o}del, Kurt},
  year = {2006},
  edition = {Reprint},
  publisher = {Peters},
  address = {Wellesley, Mass},
  isbn = {978-1-56881-256-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/AB6HGF6Q/Dawson and Gödel - 2006 - Logical dilemmas the life and work of Kurt Gödel.pdf}
}

@book{dayan2001,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, L. F.},
  year = {2001},
  series = {Computational Neuroscience},
  publisher = {Massachusetts Institute of Technology Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-04199-7},
  lccn = {QP363.3 .D39 2001},
  keywords = {Computational neuroscience,Computer simulation,Human information processing,Neural networks (Neurobiology)},
  file = {/Users/nobr/Zotero/storage/S3YLBVH7/Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathematical modeling of neural systems.pdf}
}

@misc{dean2017companies,
  title = {These 100 Companies Are to Blame for 71\% of the World's Greenhouse Gas Emissions},
  author = {Dean, Signe},
  year = {2017},
  month = jul
}

@misc{deb2025,
  title = {{{TopoNets}}: {{High Performing Vision}} and {{Language Models}} with {{Brain-Like Topography}}},
  shorttitle = {{{TopoNets}}},
  author = {Deb, Mayukh and Deb, Mainak and Murty, N. Apurva Ratan},
  year = {2025},
  month = jan,
  number = {arXiv:2501.16396},
  eprint = {2501.16396},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.16396},
  urldate = {2025-03-03},
  abstract = {Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present TopoLoss, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively TopoNets. TopoNets are the highest-performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain's visual and language cortices. Together, this work establishes a robust and generalizable framework for integrating topography into leading model architectures, advancing the development of high-performing models that more closely emulate the computational strategies of the human brain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/WKC5AFC6/Deb et al. - 2025 - TopoNets High Performing Vision and Language Models with Brain-Like Topography.pdf}
}

@misc{decreto2010,
  title = {Decree No. 7,390 of December 9, 2010},
  author = {{Government of Brazil}},
  year = {2010},
  month = dec
}

@misc{deepmind2020jax,
  title = {The {{DeepMind JAX Ecosystem}}},
  author = {{DeepMind} and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi{\'c}, Milo{\v s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  year = {2020}
}

@misc{deepseekai2024deepseekv3technicalreport,
  title = {{{DeepSeek-V3}} Technical Report},
  author = {{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu, Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Wang, T. and Yun, Tao and Pei, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhu, Y. X. and Zhang, Yang and Xu, Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu, Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Wu, Z. F. and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang, Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao, Ziyi and Pan, Zizheng},
  year = {2024},
  eprint = {2412.19437},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/Z8T29QW5/DeepSeek-AI et al. - 2024 - DeepSeek-V3 technical report.pdf}
}

@article{degrave2022,
  title = {Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning},
  author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and De Las Casas, Diego and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  year = {2022},
  month = feb,
  journal = {Nature},
  volume = {602},
  number = {7897},
  pages = {414--419},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-04301-9},
  urldate = {2023-12-05},
  abstract = {Abstract                            Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak {\`a} Configuration Variable               1,2               , including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and `snowflake' configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained `droplets' on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XNI6EISF/Degrave et al. - 2022 - Magnetic control of tokamak plasmas through deep reinforcement learning.pdf}
}

@inproceedings{dehghani2022,
  title = {{{SCENIC}}: {{A JAX Library}} for {{Computer Vision Research}} and {{Beyond}}},
  shorttitle = {{{SCENIC}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Dehghani, Mostafa and Gritsenko, Alexey and Arnab, Anurag and Minderer, Matthias and Tay, Yi},
  year = {2022},
  month = jun,
  pages = {21361--21366},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.02070},
  urldate = {2025-08-07},
  abstract = {SCENIC is an open-source1 JAX library with a focus on transformer-based models for computer vision research and beyond. The goal of this toolkit is to facilitate rapid experimentation, prototyping, and research of new architectures and models. SCENIC supports a diverse range of tasks (e.g., classification, segmentation, detection) and facilitates working on multi-modal problems, along with GPU/TPU support for large-scale, multi-host and multi-device training. SCENIC also offers optimized implementations of stateof-the-art research models spanning a wide range of modalities. SCENIC has been successfully used for numerous projects and published papers and continues serving as the library of choice for rapid prototyping and publication of new research ideas.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PB58GHEH/Dehghani et al. - 2022 - SCENIC A JAX Library for Computer Vision Research and Beyond.pdf}
}

@incollection{dejonge2019,
  title = {The {{Challenge}} of {{Negotiation}} in the {{Game}} of {{Diplomacy}}},
  booktitle = {Agreement {{Technologies}}},
  author = {De Jonge, Dave and Baarslag, Tim and Aydo{\u g}an, Reyhan and Jonker, Catholijn and Fujita, Katsuhide and Ito, Takayuki},
  editor = {Lujak, Marin},
  year = {2019},
  volume = {11327},
  pages = {100--114},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-17294-7_8},
  urldate = {2023-11-12},
  abstract = {The game of Diplomacy has been used as a test case for complex automated negotiations for a long time, but to date very few successful negotiation algorithms have been implemented for this game. We have therefore decided to include a Diplomacy tournament within the annual Automated Negotiating Agents Competition (ANAC). In this paper we present the setup and the results of the ANAC 2017 Diplomacy Competition and the ANAC 2018 Diplomacy Challenge. We observe that none of the negotiation algorithms submitted to these two editions have been able to significantly improve the performance over a non-negotiating baseline agent. We analyze these algorithms and discuss why it is so hard to write successful negotiation algorithms for Diplomacy. Finally, we provide experimental evidence that, despite these results, coalition formation and coordination do form essential elements of the game.},
  isbn = {978-3-030-17293-0 978-3-030-17294-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3U9ADNWM/De Jonge et al. - 2019 - The Challenge of Negotiation in the Game of Diplomacy.pdf}
}

@misc{delacruz2020,
  title = {Reconfigurable {{Behavior Trees}}: {{Towards}} an {{Executive Framework Meeting High-level Decision Making}} and {{Control Layer Features}}},
  shorttitle = {Reconfigurable {{Behavior Trees}}},
  author = {{de la Cruz}, Pilar and Piater, Justus and Saveriano, Matteo},
  year = {2020},
  month = aug,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Behavior Trees (BTs) constitute a widespread artificial intelligence tool that has been successfully adopted in robotics. Their advantages include simplicity, modularity, and reusability of code. However, Behavior Trees remain a highlevel decision making engine; control features cannot easily be integrated. This paper proposes Reconfigurable Behavior Trees (RBTs), an extension of the traditional BTs that incorporates sensed information coming from the robotic environment in the decision making process. We endow RBTs with continuous sensory data that permits the online monitoring of the task execution. The resulting stimulus-driven architecture is capable of dynamically handling changes in the executive context while keeping the execution time low. The proposed framework is evaluated on a set of robotic experiments. The results show that RBTs are a promising approach for robotic task representation, monitoring, and execution.},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/LAIGXP22/de la Cruz et al. - 2020 - Reconfigurable Behavior Trees Towards an Executive Framework Meeting High-level Decision Making and.pdf}
}

@article{delamare2005,
  title = {Strategies to Improve the Performance of Very Low Bit Rate Speech Coders and Application to a Variable Rate 1.2 Kb/s Codec},
  author = {De Lamare, R.C. and Alcaim, A.},
  year = {2005},
  journal = {IEE Proceedings - Vision, Image, and Signal Processing},
  volume = {152},
  number = {1},
  pages = {74},
  issn = {1350245X},
  doi = {10.1049/ip-vis:20051189},
  urldate = {2023-08-28},
  abstract = {This paper presents several strategies to improve the performance of very low bit rate speech coders and describes a speech codec that incorporates these strategies and operates at an average bit rate of 1.2 kb/s. The encoding algorithm is based on several improvements in a mixed multiband excitation (MMBE) linear predictive coding (LPC) structure. A switched-predictive vector quantiser technique that outperforms previously reported schemes is adopted to encode the LSF parameters. Spectral and sound specific low rate models are used in order to achieve high quality speech at low rates. An MMBE approach with three sub-bands is employed to encode voiced frames, while fricatives and stops modelling and synthesis techniques are used for unvoiced frames. This strategy is shown to provide good quality synthesised speech, at a bit rate of only 0.4 kb/s for unvoiced frames. To reduce coding noise and improve decoded speech, spectral envelope restoration combined with noise reduction (SERNR) postfilter is used. The contributions of the techniques described in this paper are separately assessed and then combined in the design of a low bit rate codec that is evaluated against the North American Mixed Excitation Linear Prediction (MELP) coder. The performance assessment is carried out in terms of the spectral distortion of LSF quantisation, mean opinion score (MOS), A/B comparison tests and the ITU-T P.862 perceptual evaluation of speech quality (PESQ) standard. Assessment results show that the improved methods for LSF quantisation, sound specific modelling and synthesis and the new postfiltering approach can significantly outperform previously reported techniques. Further results also indicate that a system combining the proposed improvements and operating at 1.2 kb/s, is comparable (slightly outperforming) a MELP coder operating at 2.4 kb/s. For tandem connection situations, the proposed system is clearly superior to the MELP coder.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/B5SCSSGG/De Lamare and Alcaim - 2005 - Strategies to improve the performance of very low .pdf}
}

@book{deleuze2007,
  title = {Two Regimes of Madness: Texts and Interviews 1975 - 1995},
  shorttitle = {Two Regimes of Madness},
  author = {Deleuze, Gilles and Hodges, Am and Deleuze, Gilles},
  editor = {Lapoujade, David},
  year = {2007},
  edition = {2. Dr},
  publisher = {Semiotext(e)},
  address = {New York},
  isbn = {978-1-58435-062-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PK8X8FA7/Deleuze et al. - 2007 - Two regimes of madness texts and interviews 1975 - 1995.pdf}
}

@article{delvalle2021,
  title = {Translating {{New Synthetic Biology Advances}} for {{Biosensing Into}} the {{Earth}} and {{Environmental Sciences}}},
  author = {Del Valle, Ilenne and Fulk, Emily M. and Kalvapalle, Prashant and Silberg, Jonathan J. and Masiello, Caroline A. and Stadler, Lauren B.},
  year = {2021},
  month = feb,
  journal = {Frontiers in Microbiology},
  volume = {11},
  pages = {618373},
  issn = {1664-302X},
  doi = {10.3389/fmicb.2020.618373},
  urldate = {2023-11-09},
  abstract = {The rapid diversification of synthetic biology tools holds promise in making some classically hard-to-solve environmental problems tractable. Here we review longstanding problems in the Earth and environmental sciences that could be addressed using engineered microbes as micron-scale sensors (biosensors). Biosensors can offer new perspectives on open questions, including understanding microbial behaviors in heterogeneous matrices like soils, sediments, and wastewater systems, tracking cryptic element cycling in the Earth system, and establishing the dynamics of microbe-microbe, microbe-plant, and microbe-material interactions. Before these new tools can reach their potential, however, a suite of biological parts and microbial chassis appropriate for environmental conditions must be developed by the synthetic biology community. This includes diversifying sensing modules to obtain information relevant to environmental questions, creating output signals that allow dynamic reporting from hard-to-image environmental materials, and tuning these sensors so that they reliably function long enough to be useful for environmental studies. Finally, ethical questions related to the use of synthetic biosensors in environmental applications are discussed.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/MGFKD9U9/Del Valle et al. - 2021 - Translating New Synthetic Biology Advances for Biosensing Into the Earth and Environmental Sciences.pdf}
}

@article{demontjoye2013,
  title = {Unique in the {{Crowd}}: {{The}} Privacy Bounds of Human Mobility},
  shorttitle = {Unique in the {{Crowd}}},
  author = {De Montjoye, Yves-Alexandre and Hidalgo, C{\'e}sar A. and Verleysen, Michel and Blondel, Vincent D.},
  year = {2013},
  month = mar,
  journal = {Scientific Reports},
  volume = {3},
  number = {1},
  pages = {1376},
  issn = {2045-2322},
  doi = {10.1038/srep01376},
  urldate = {2023-07-10},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/8GMFG23U/De Montjoye et al. - 2013 - Unique in the Crowd The privacy bounds of human m.pdf}
}

@article{deng2023,
  title = {Harmonized and {{Open Energy Dataset}} for {{Modeling}} a {{Highly Renewable Brazilian Power System}}},
  author = {Deng, Ying and Cao, Karl-Ki{\^e}n and Hu, Wenxuan and Stegen, Ronald and {von Krbek}, Kai and Soria, Rafael and Rochedo, Pedro Rua Rodriguez and Jochem, Patrick},
  year = {2023},
  month = feb,
  journal = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {103},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-023-01992-9},
  urldate = {2023-07-17},
  abstract = {Improvements in modelling energy systems of populous emerging economies are highly decisive for a successful global energy transition. The models used--increasingly open source--still need more appropriate open data. As an illustrative example, we take the Brazilian energy system, which has great potential for renewable energy resources but still relies heavily on fossil fuels. We provide a comprehensive open dataset for scenario analyses, which can be directly used with the popular open energy system model PyPSA and other modelling frameworks. It includes three categories: (1) time series data of variable renewable potentials, electricity load profiles, inflows for the hydropower plants, and cross-border electricity exchanges; (2) geospatial data on the administrative division of the Brazilian federal states; (3) tabular data, which contains power plant data with installed and planned generation capacities, aggregated grid network topology, biomass thermal plant potential, as well as scenarios of energy demand. Our dataset could enable further global or country-specific energy system studies based on open data relevant to decarbonizing Brazil's energy system.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Energy management,Energy modelling,Energy policy},
  file = {/Users/nobr/Zotero/storage/YN42GSFR/Deng et al. - 2023 - Harmonized and Open Energy Dataset for Modeling a .pdf}
}

@inproceedings{deng2024,
  title = {Can {{LLMs Implicitly Learn Numeric Parameter Constraints}} in {{Data Science APIs}}?},
  booktitle = {The {{Thirty-eighth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Deng, Yinlin and Xia, Chunqiu Steven and Cao, Zhezhen and Li, Meiziniu and Zhang, Lingming},
  year = {2024},
  month = nov,
  urldate = {2025-03-03},
  abstract = {Data science (DS) programs, typically built on popular DS libraries (such as PyTorch and NumPy) with thousands of APIs, serve as the cornerstone for various mission-critical domains such as financial systems, autonomous driving software, and coding assistants. Recently, large language models (LLMs) have been widely applied to generate DS programs across diverse scenarios, such as assisting users for DS programming or detecting critical vulnerabilities in DS frameworks. Such applications have all operated under the assumption, that LLMs can implicitly model the numerical parameter constraints in DS library APIs and produce valid code. However, this assumption has not been rigorously studied in the literature. In this paper, we empirically investigate the proficiency of LLMs to handle these implicit numerical constraints when generating DS programs. We studied 28 widely used APIs from PyTorch and NumPy, and scrutinized the LLMs' generation performance in different levels of granularity: full programs, all parameters, and individual parameters of a single API. We evaluated both state-of-the-art open-source and closed-source models. The results show that LLMs are great at generating simple DS programs, particularly those that follow common patterns seen in training data. However, as we increase the difficulty by providing more complex/unusual inputs, the performance of LLMs drops significantly. We also observe that GPT-4-Turbo can sustain much higher performance overall, but still cannot handle arithmetic API constraints well. In summary, while LLMs exhibit the ability to memorize common patterns of popular DS API usage through massive training, they overall lack genuine comprehension of the underlying numerical constraints.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GP2289QP/Deng et al. - 2024 - Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs.pdf}
}

@misc{denton2015,
  title = {Deep {{Generative Image Models}} Using a {{Laplacian Pyramid}} of {{Adversarial Networks}}},
  author = {Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob},
  year = {2015},
  month = jun,
  journal = {arXiv.org},
  urldate = {2023-05-08},
  abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40\% of the time, compared to 10\% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
  howpublished = {https://arxiv.org/abs/1506.05751v1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GJV4DRWU/Denton et al. - 2015 - Deep Generative Image Models using a Laplacian Pyr.pdf}
}

@article{dev2018,
  title = {Closed {{Form Word Embedding Alignment}}},
  author = {Dev, Sunipa and Hassan, Safia and Phillips, Jeff M.},
  year = {2018},
  month = jun,
  eprint = {1806.01330},
  abstract = {We develop a family of techniques to align word embeddings which are derived from different source datasets or created using different mechanisms (e.g., GloVe or word2vec). Our methods are simple and have a closed form to optimally rotate, translate, and scale to minimize root mean squared errors or maximize the average cosine similarity between two embeddings of the same vocabulary into the same dimensional space. Our methods extend approaches known as Absolute Orientation, which are popular for aligning objects in three-dimensions, and generalize an approach by Smith etal (ICLR 2017). We prove new results for optimal scaling and for maximizing cosine similarity. Then we demonstrate how to evaluate the similarity of embeddings from different sources or mechanisms, and that certain properties like synonyms and analogies are preserved across the embeddings and can be enhanced by simply aligning and averaging ensembles of embeddings.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/FXRYNU3R/Dev et al. - 2018 - Closed Form Word Embedding Alignment.pdf}
}

@book{devaney2003,
  title = {An Introduction to Chaotic Dynamical Systems},
  author = {Devaney, Robert L.},
  year = {2003},
  series = {Studies in {{Nonlinearity}}},
  edition = {2nd ed., paperback ed},
  publisher = {Westview Press},
  address = {Boulder, Colo},
  isbn = {978-0-8133-4085-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/WVR78J7F/Devaney - 2003 - An introduction to chaotic dynamical systems.pdf}
}

@misc{devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-06-11},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/7JZUSEZI/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/nobr/Zotero/storage/SEVDRXYZ/1810.html}
}

@article{devore2006,
  title = {A {{Modern Introduction}} to {{Probability}} and {{Statistics}}: {{Understanding Why}} and {{How}}},
  shorttitle = {A {{Modern Introduction}} to {{Probability}} and {{Statistics}}},
  author = {Devore, Jay},
  year = {2006},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {101},
  number = {473},
  pages = {393--394},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2006.s72},
  urldate = {2024-02-14},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/NTRPVGKI/Devore - 2006 - A Modern Introduction to Probability and Statistics Understanding Why and How.pdf}
}

@article{dhariwal2020,
  title = {Jukebox: {{A Generative Model}} for {{Music}}},
  author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  year = {2020},
  month = apr,
  eprint = {2005.00341},
  urldate = {2023-03-22},
  abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/26HURNVR/full-text.pdf}
}

@article{dharna,
  title = {Quality-{{Diversity Self-Play}}: {{Open-Ended Strategy Innovation}} via {{Foundation Models}}},
  author = {Dharna, Aaron and Lu, Cong and Clune, Jeff},
  abstract = {Multi-agent dynamics have powered innovation from time immemorial, such as scientific innovations during the space race or predator-prey dynamics in the natural world. The resulting landscape of interacting agents is a continually changing, interconnected, and complex mosaic of opportunities for innovation. Yet, training innovative and adaptive artificial agents remains challenging. Self-Play algorithms bootstrap the complexity of their solutions by automatically generating a curriculum. Recent work has demonstrated the power of foundation models (FMs) as intelligent and efficient search operators. In this paper, we investigate whether combining the human-like priors and extensive knowledge embedded in FMs with multi-agent race dynamics can lead to rapid policy innovation in open-ended Self-Play algorithms. We propose a novel algorithm, Quality-Diversity Self-Play (QDSP) that explores diverse and high-performing strategies in interacting (here, competing) populations. We evaluate QDSP in a two-player asymmetric pursuer-evader simulation with code-based policies and show that QDSP surpasses high-performing human-designed policies. Furthermore, QDSP discovers better policies than those from quality-only or diversity-only Self-Play algorithms. Since QDSP explores new code-based strategies, the discovered policies come from many distinct subfields of computer science and control, including reinforcement learning, heuristic search, model predictive control, tree search, and machine learning approaches. Combining multi-agent dynamics with the knowledge of FMs demonstrates a powerful new approach to efficiently create a Cambrian explosion of diverse, performant, and complex strategies in multi-agent settings.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BIMGYDR9/Dharna et al. - Quality-Diversity Self-Play Open-Ended Strategy Innovation via Foundation Models.pdf}
}

@article{dharnaa,
  title = {Quality-{{Diversity Self-Play}}: {{Open-Ended Strategy Innovation}} via {{Foundation Models}}},
  author = {Dharna, Aaron and Lu, Cong and Clune, Jeff},
  abstract = {Multi-agent dynamics have powered innovation from time immemorial, such as scientific innovations during the space race or predator-prey dynamics in the natural world. The resulting landscape of interacting agents is a continually changing, interconnected, and complex mosaic of opportunities for innovation. Yet, training innovative and adaptive artificial agents remains challenging. Self-Play algorithms bootstrap the complexity of their solutions by automatically generating a curriculum. Recent work has demonstrated the power of foundation models (FMs) as intelligent and efficient search operators. In this paper, we investigate whether combining the human-like priors and extensive knowledge embedded in FMs with multi-agent race dynamics can lead to rapid policy innovation in open-ended Self-Play algorithms. We propose a novel algorithm, Quality-Diversity Self-Play (QDSP) that explores diverse and high-performing strategies in interacting (here, competing) populations. We evaluate QDSP in a two-player asymmetric pursuer-evader simulation with code-based policies and show that QDSP surpasses high-performing human-designed policies. Furthermore, QDSP discovers better policies than those from quality-only or diversity-only Self-Play algorithms. Since QDSP explores new code-based strategies, the discovered policies come from many distinct subfields of computer science and control, including reinforcement learning, heuristic search, model predictive control, tree search, and machine learning approaches. Combining multi-agent dynamics with the knowledge of FMs demonstrates a powerful new approach to efficiently create a Cambrian explosion of diverse, performant, and complex strategies in multi-agent settings.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/V4QISR7Z/Dharna et al. - Quality-Diversity Self-Play Open-Ended Strategy Innovation via Foundation Models.pdf}
}

@article{dharnab,
  title = {Quality-{{Diversity Self-Play}}: {{Open-Ended Strategy Innovation}} via {{Foundation Models}}},
  author = {Dharna, Aaron and Lu, Cong and Clune, Jeff},
  abstract = {Multi-agent dynamics have powered innovation from time immemorial, such as scientific innovations during the space race or predator-prey dynamics in the natural world. The resulting landscape of interacting agents is a continually changing, interconnected, and complex mosaic of opportunities for innovation. Yet, training innovative and adaptive artificial agents remains challenging. Self-Play algorithms bootstrap the complexity of their solutions by automatically generating a curriculum. Recent work has demonstrated the power of foundation models (FMs) as intelligent and efficient search operators. In this paper, we investigate whether combining the human-like priors and extensive knowledge embedded in FMs with multi-agent race dynamics can lead to rapid policy innovation in open-ended Self-Play algorithms. We propose a novel algorithm, Quality-Diversity Self-Play (QDSP) that explores diverse and high-performing strategies in interacting (here, competing) populations. We evaluate QDSP in a two-player asymmetric pursuer-evader simulation with code-based policies and show that QDSP surpasses high-performing human-designed policies. Furthermore, QDSP discovers better policies than those from quality-only or diversity-only Self-Play algorithms. Since QDSP explores new code-based strategies, the discovered policies come from many distinct subfields of computer science and control, including reinforcement learning, heuristic search, model predictive control, tree search, and machine learning approaches. Combining multi-agent dynamics with the knowledge of FMs demonstrates a powerful new approach to efficiently create a Cambrian explosion of diverse, performant, and complex strategies in multi-agent settings.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HQL7YVPV/Dharna et al. - Quality-Diversity Self-Play Open-Ended Strategy Innovation via Foundation Models.pdf}
}

@misc{dialogo_chino2018,
  title = {Owner of {{Brazil}}'s Largest Coal Reserves, {{Rio Grande}} Do {{Sul}} Waves to {{China}} in Search of Investors},
  author = {Reed, Sarita and Fontana, Vinicius},
  year = {2018},
  month = dec
}

@book{diamond2005,
  title = {Collapse: How Societies Choose to Fail or Succeed},
  shorttitle = {Collapse},
  author = {Diamond, Jared M.},
  year = {2005},
  publisher = {Viking},
  address = {New York},
  isbn = {978-0-670-03337-9},
  langid = {english},
  lccn = {HN13 .D5 2005},
  keywords = {Case studies,Environmental policy,Social change,Social history},
  file = {/Users/nobr/Zotero/storage/CKUX4TRE/Diamond - 2005 - Collapse how societies choose to fail or succeed.pdf}
}

@misc{dibernardo2021,
  title = {Latin Writing Styles Analysis with {{Machine Learning}}: {{New}} Approach to Old Questions},
  shorttitle = {Latin Writing Styles Analysis with {{Machine Learning}}},
  author = {Di Bernardo, Arianna and Poetto, Simone and Sillano, Pietro and Villata, Beatrice and S{\'o}jka, Weronika and {Pi{\k e}tka-Danilewicz}, Zofia and Pranke, Piotr},
  year = {2021},
  month = sep,
  number = {arXiv:2109.00601},
  eprint = {2109.00601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.00601},
  urldate = {2023-06-11},
  abstract = {In the Middle Ages texts were learned by heart and spread using oral means of communication from generation to generation. Adaptation of the art of prose and poems allowed keeping particular descriptions and compositions characteristic for many literary genres. Taking into account such a specific construction of literature composed in Latin, we can search for and indicate the probability patterns of familiar sources of specific narrative texts. Consideration of Natural Language Processing tools allowed us the transformation of textual objects into numerical ones and then application of machine learning algorithms to extract information from the dataset. We carried out the task consisting of the practical use of those concepts and observation to create a tool for analyzing narrative texts basing on open-source databases. The tool focused on creating specific search tools resources which could enable us detailed searching throughout the text. The main objectives of the study take into account finding similarities between sentences and between documents. Next, we applied machine learning algorithms on chosen texts to calculate specific features of them (for instance authorship or centuries) and to recognize sources of anonymous texts with a certain percentage.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/KZN766LA/Di Bernardo et al. - 2021 - Latin writing styles analysis with Machine Learnin.pdf;/Users/nobr/Zotero/storage/DLTY9PSZ/2109.html}
}

@misc{digiovanniSurveySelfPlayReinforcement2021,
  title = {Survey of {{Self-Play}} in {{Reinforcement Learning}}},
  author = {DiGiovanni, Anthony and Zell, Ethan C.},
  year = {2021},
  month = jul,
  number = {arXiv:2107.02850},
  eprint = {2107.02850},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.02850},
  urldate = {2023-12-30},
  abstract = {In reinforcement learning (RL), the term self-play describes a kind of multi-agent learning (MAL) that deploys an algorithm against copies of itself to test compatibility in various stochastic environments. As is typical in MAL, the literature draws heavily from well-established concepts in classical game theory and so this survey quickly reviews some fundamental concepts. In what follows, we present a brief survey of self-play literature, its major themes, criteria, and techniques, and then conclude with an assessment of current shortfalls of the literature as well as suggestions for future directions.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Science and Game Theory},
  file = {/Users/nobr/Zotero/storage/YP7G9UPN/DiGiovanni and Zell - 2021 - Survey of Self-Play in Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/S53WJU8B/2107.html}
}

@article{dinan2022,
  title = {Human-Level Play in the Game of {{{\emph{Diplomacy}}}} by Combining Language Models with Strategic Reasoning},
  author = {Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and Jacob, Athul Paul and Komeili, Mojtaba and Konath, Karthik and Kwon, Minae and Lerer, Adam and Lewis, Mike and Miller, Alexander H. and Mitts, Sasha and Renduchintala, Adithya and Roller, Stephen and Rowe, Dirk and Shi, Weiyan and Spisak, Joe and Wei, Alexander and Wu, David and Zhang, Hugh and Zijlstra, Markus},
  year = {2022},
  month = dec,
  journal = {Science},
  volume = {378},
  number = {6624},
  pages = {1067--1074},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.ade9097},
  urldate = {2023-11-06},
  abstract = {Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in               Diplomacy               , a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players' beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online               Diplomacy               league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game.                        ,              AI masters Diplomacy                            The game               Diplomacy               has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker,               Diplomacy               cannot be solved purely through self-play; it requires the development of an agent to understand other players' motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR)               et al               . developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online               Diplomacy               league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. ---YS                        ,                             Artificial intelligence demonstrates human-level performance in the strategic board game               Diplomacy               .},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/AT7AFDXW/Meta Fundamental AI Research Diplomacy Team (FAIR)† et al. - 2022 - Human-level play in the game of Diplomacy by combining language models with strategic reasoni.pdf}
}

@article{dixon,
  title = {From {{Superconductors}} to {{Supercolliders}}},
  author = {Dixon, Lance},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6AQDM6AA/Dixon - From Superconductors to Supercolliders.pdf}
}

@article{domokos2024,
  title = {Soft Cells and the Geometry of Seashells},
  author = {Domokos, G{\'a}bor and Goriely, Alain and Horv{\'a}th, {\'A}kos G and Reg{\H o}s, Krisztina},
  editor = {Harris, Michael},
  year = {2024},
  month = sep,
  journal = {PNAS Nexus},
  volume = {3},
  number = {9},
  pages = {pgae311},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae311},
  urldate = {2024-10-17},
  abstract = {A central problem of geometry is the tiling of space with simple structures. The classical solutions, such as triangles, squares, and hexagons in the plane and cubes and other polyhedra in three-dimensional space are built with sharp corners and flat faces. However, many tilings in Nature are characterized by shapes with curved edges, nonflat faces, and few, if any, sharp corners. An important question is then to relate prototypical sharp tilings to softer natural shapes. Here, we solve this problem by introducing a new class of shapes, the soft cells, minimizing the number of sharp corners and filling space as soft tilings. We prove that an infinite class of polyhedral tilings can be smoothly deformed into soft tilings and we construct the soft versions of all Dirichlet--Voronoi cells associated with point lattices in two and three dimensions. Remarkably, these ideal soft shapes, born out of geometry, are found abundantly in nature, from cells to shells.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/EQM4W4HW/pgae311.pdf}
}

@misc{donatellaThermodynamicNaturalGradient2024,
  title = {Thermodynamic {{Natural Gradient Descent}}},
  author = {Donatella, Kaelan and Duffield, Samuel and Aifer, Maxwell and Melanson, Denis and Crooks, Gavin and Coles, Patrick J.},
  year = {2024},
  month = may,
  number = {arXiv:2405.13817},
  eprint = {2405.13817},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {Second-order training methods have better convergence properties than gradient descent but are rarely used in practice for large-scale training due to their computational overhead. This can be viewed as a hardware limitation (imposed by digital computers). Here we show that natural gradient descent (NGD), a second-order method, can have a similar computational complexity per iteration to a first-order method, when employing appropriate hardware. We present a new hybrid digitalanalog algorithm for training neural networks that is equivalent to NGD in a certain parameter regime but avoids prohibitively costly linear system solves. Our algorithm exploits the thermodynamic properties of an analog system at equilibrium, and hence requires an analog thermodynamic computer. The training occurs in a hybrid digital-analog loop, where the gradient and Fisher information matrix (or any other positive semi-definite curvature matrix) are calculated at given time intervals while the analog dynamics take place. We numerically demonstrate the superiority of this approach over state-of-the-art digital first- and second-order training methods on classification tasks and language model fine-tuning tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/NE9FKX8W/Donatella et al. - 2024 - Thermodynamic Natural Gradient Descent.pdf}
}

@article{dong2023,
  title = {Curiosity-Tuned Experience Replay for Wargaming Decision Modeling without Reward-Engineering},
  author = {Dong, Liwei and Li, Ni and Gong, Guanghong},
  year = {2023},
  month = dec,
  journal = {Simulation Modelling Practice and Theory},
  volume = {129},
  pages = {102842},
  issn = {1569190X},
  doi = {10.1016/j.simpat.2023.102842},
  urldate = {2023-11-12},
  abstract = {Reinforcement Learning (RL) has become a promising technique to deal with the tough decision modeling problem in the wargaming field. However, to deploy current RL algorithms requires reward-engineering scenario by scenario, which is laborious for massive wargaming scenarios. To tackle this issue, this paper proposes an improved RL method, curiosity-tuned experience replay (CTER), which allows the RL-driven decision model to achieve a relatively effective policy under the sparse reward. CTER uses the curiosity mechanism to regulate the three critical procedures during learning with experience replay: the exploration, storage, and revisitation of the experi\- ences. Based on the prediction-based curiosity, CTER generates an intrinsic reward to fill the sparse reward space, and further provides an adaptive exploration strategy to collect more informative experiences. Moreover, CTER develops a novel prioritized replay and memory updating mechanism to reuse experiences more efficiently. Through the systematic evaluation and comparison on typical game tasks and wargaming tasks, CTER shows its effectiveness and generalization in different scenarios without reward-engineering. Especially, the policy perfor\- mance of CTER-based RL with the sparse reward is almost equivalent to that of ordinary RL with dense engineered rewards. Our work may offer a relatively universal approach for wargaming decision modeling, which can free the RL-based decision modelers from the laborious rewardengineering.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/I3JHA3MX/Dong et al. - 2023 - Curiosity-tuned experience replay for wargaming decision modeling without reward-engineering.pdf}
}

@article{dong2023a,
  title = {Accelerating Wargaming Reinforcement Learning by Dynamic Multi-Demonstrator Ensemble},
  author = {Dong, Liwei and Li, Ni and Yuan, Haitao and Gong, Guanghong},
  year = {2023},
  month = nov,
  journal = {Information Sciences},
  volume = {648},
  pages = {119534},
  issn = {00200255},
  doi = {10.1016/j.ins.2023.119534},
  urldate = {2023-12-18},
  abstract = {Deep Reinforcement Learning (DRL) has become a promising technique to deal with tough wargaming decision-making problems. However, DRL suffers an inherent problem of low learning efficiency and it often requires massive cost of training steps, which may be alleviated with expert demonstrations in wargaming domains. Most learning methods with demonstrations generally treat the demonstration data from different expert demonstrators without distinction. Besides, a more appropriate and effective mechanism is highly needed to control sampling balance of expert-generated demonstration samples and agent-generated interaction ones. To tackle the two issues, this work proposes an improved approach to leverage expert demonstrations to further accelerate DRL. It innovatively extracts inherent diversity in multiple demonstrators by pretraining agents individually from multiple demonstration sources, thereby producing a strong and initial ensemble model. In addition, a novel technique to evaluate the learning importance of each demonstrator is designed to dynamically tune sampling ratios of learning data in a more adaptive and effective manner. Through the evaluation on several classic game tasks and a typical wargaming scenario, our method shows superior performance over several state-of-the-art methods and significantly raises DRL's efficiency for typical wargaming decision-making applications.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SA3JI2R4/Dong et al. - 2023 - Accelerating wargaming reinforcement learning by dynamic multi-demonstrator ensemble.pdf}
}

@misc{dong2024,
  title = {{{FAN}}: {{Fourier Analysis Networks}}},
  shorttitle = {{{FAN}}},
  author = {Dong, Yihong and Li, Ge and Tao, Yongding and Jiang, Xue and Zhang, Kechi and Li, Jia and Su, Jing and Zhang, Jun and Xu, Jingjing},
  year = {2024},
  month = nov,
  number = {arXiv:2410.02675},
  eprint = {2410.02675},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2410.02675},
  urldate = {2025-01-08},
  abstract = {Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is a crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving a more accurate expression and prediction of periodic patterns. As a promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/56F6PBNR/Dong et al. - 2024 - FAN Fourier Analysis Networks.pdf}
}

@techreport{dorkenwald2023,
  type = {Preprint},
  title = {Neuronal Wiring Diagram of an Adult Brain},
  author = {Dorkenwald, Sven and Matsliah, Arie and Sterling, Amy R and Schlegel, Philipp and Yu, Szi-chieh and McKellar, Claire E. and Lin, Albert and Costa, Marta and Eichler, Katharina and Yin, Yijie and Silversmith, Will and {Schneider-Mizell}, Casey and Jordan, Chris S. and Brittain, Derrick and Halageri, Akhilesh and Kuehner, Kai and Ogedengbe, Oluwaseun and Morey, Ryan and Gager, Jay and Kruk, Krzysztof and Perlman, Eric and Yang, Runzhe and Deutsch, David and Bland, Doug and Sorek, Marissa and Lu, Ran and Macrina, Thomas and Lee, Kisuk and Bae, J. Alexander and Mu, Shang and Nehoran, Barak and Mitchell, Eric and Popovych, Sergiy and Wu, Jingpeng and Jia, Zhen and Castro, Manuel and Kemnitz, Nico and Ih, Dodam and Bates, Alexander Shakeel and Eckstein, Nils and Funke, Jan and Collman, Forrest and Bock, Davi D. and Jefferis, Gregory S.X.E. and Seung, H. Sebastian and Murthy, Mala and {the FlyWire Consortium}},
  year = {2023},
  month = jun,
  institution = {Neuroscience},
  doi = {10.1101/2023.06.27.546656},
  urldate = {2023-08-28},
  abstract = {Connections between neurons can be mapped by acquiring and analyzing electron microscopic (EM) brain images. In recent years, this approach has been applied to chunks of brains to reconstruct local connectivity maps that are highly informative, yet inadequate for understanding brain function more globally. Here, we present the first neuronal wiring diagram of a whole adult brain, containing 5{\texttimes}107 chemical synapses between {\textasciitilde}130,000 neurons reconstructed from a female Drosophila melanogaster. The resource also incorporates annotations of cell classes and types, nerves, hemilineages, and predictions of neurotransmitter identities. Data products are available by download, programmatic access, and interactive browsing and made interoperable with other fly data resources. We show how to derive a projectome, a map of projections between regions, from the connectome. We demonstrate the tracing of synaptic pathways and the analysis of information flow from inputs (sensory and ascending neurons) to outputs (motor, endocrine, and descending neurons), across both hemispheres, and between the central brain and the optic lobes. Tracing from a subset of photoreceptors all the way to descending motor pathways illustrates how structure can uncover putative circuit mechanisms underlying sensorimotor behaviors. The technologies and open ecosystem of the FlyWire Consortium set the stage for future large-scale connectome projects in other species.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TP3VHIHG/Dorkenwald et al. - 2023 - Neuronal wiring diagram of an adult brain.pdf}
}

@misc{du2023,
  title = {Improving {{Factuality}} and {{Reasoning}} in {{Language Models}} through {{Multiagent Debate}}},
  author = {Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B. and Mordatch, Igor},
  year = {2023},
  month = may,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such "society of minds" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding. Project website at https://composable-models.github.io/llm\_debate/.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/CU7ZPBBC/Du et al. - 2023 - Improving Factuality and Reasoning in Language Models through Multiagent Debate.pdf}
}

@article{duan2013,
  title = {Multiple {{UCAVs}} Cooperative Air Combat Simulation Platform Based on {{PSO}}, {{ACO}}, and Game Theory},
  author = {Duan, Haibin and Wei, Xingxing and Dong, Zhuoning},
  year = {2013},
  month = nov,
  journal = {IEEE Aerospace and Electronic Systems Magazine},
  volume = {28},
  number = {11},
  pages = {12--19},
  issn = {0885-8985},
  doi = {10.1109/MAES.2013.6678487},
  urldate = {2024-01-14},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DWJ7GMC7/Duan et al. - 2013 - Multiple UCAVs cooperative air combat simulation platform based on PSO, ACO, and game theory.pdf}
}

@article{dubova2022,
  title = {Building Human-like Communicative Intelligence: {{A}} Grounded Perspective},
  shorttitle = {Building Human-like Communicative Intelligence},
  author = {Dubova, Marina},
  year = {2022},
  month = mar,
  journal = {Cognitive Systems Research},
  volume = {72},
  pages = {63--79},
  issn = {13890417},
  doi = {10.1016/j.cogsys.2021.12.002},
  urldate = {2023-11-12},
  abstract = {Modern Artificial Intelligence (AI) systems excel at diverse tasks, from image classification to strategy games, even outperforming humans in many of these domains. After making astounding progress in language learning in the recent decade, AI systems, however, seem to approach the ceiling that does not reflect important aspects of human communicative capacities. Unlike human learners, communicative AI systems often fail to systematically generalize to new data, suffer from sample inefficiency, fail to capture common-sense semantic knowledge, and do not translate to real-world communicative situations. Cognitive Science offers several insights on how AI could move forward from this point. This paper aims to: (1) suggest that the dominant cognitively-inspired AI directions, based on nativist and symbolic paradigms, lack necessary substantiation and concreteness to guide progress in modern AI, and (2) articulate an alternative, ``grounded'', perspective on AI advancement, inspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research. I review results on 4E research lines in Cognitive Science to distinguish the main aspects of naturalistic learning conditions that play causal roles for human language development. I then use this analysis to propose a list of concrete, implementable components for building ``grounded'' linguistic intelligence. These components include embodying machines in a percep\- tion--action cycle, equipping agents with active exploration mechanisms so they can build their own curriculum, allowing agents to gradually develop motor abilities to promote piecemeal language development, and endowing the agents with adaptive feedback from their physical and social environment. I hope that these ideas can direct AI research towards building machines that develop human-like language abilities through their experiences with the world.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/VHG89XZM/Dubova - 2022 - Building human-like communicative intelligence A grounded perspective.pdf}
}

@misc{dupontEquivariantNeuralRendering2020,
  title = {Equivariant {{Neural Rendering}}},
  author = {Dupont, Emilien and Bautista, Miguel Angel and Colburn, Alex and Sankar, Aditya and Guestrin, Carlos and Susskind, Josh and Shan, Qi},
  year = {2020},
  month = dec,
  number = {arXiv:2006.07630},
  eprint = {2006.07630},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {We propose a framework for learning neural scene representations directly from images, without 3D supervision. Our key insight is that 3D structure can be imposed by ensuring that the learned representation transforms like a real 3D scene. Specifically, we introduce a loss which enforces equivariance of the scene representation with respect to 3D transformations. Our formulation allows us to infer and render scenes in real time while achieving comparable results to models requiring minutes for inference. In addition, we introduce two challenging new datasets for scene representation and neural rendering, including scenes with complex lighting and backgrounds. Through experiments, we show that our model achieves compelling results on these datasets as well as on standard ShapeNet benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/EGD542T4/Dupont et al. - 2020 - Equivariant Neural Rendering.pdf}
}

@article{dwork2013,
  title = {The {{Algorithmic Foundations}} of {{Differential Privacy}}},
  author = {Dwork, Cynthia and Roth, Aaron},
  year = {2013},
  journal = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume = {9},
  number = {3-4},
  pages = {211--407},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000042},
  urldate = {2023-11-09},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/G8Y6S6SE/Dwork and Roth - 2013 - The Algorithmic Foundations of Differential Privacy.pdf}
}

@inproceedings{ebrahimi2020,
  title = {How {{Can Self-Attention Networks Recognize Dyck-n Languages}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Ebrahimi, Javid and Gelda, Dhruv and Zhang, Wei},
  editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
  year = {2020},
  month = nov,
  pages = {4301--4306},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.findings-emnlp.384},
  urldate = {2024-06-02},
  abstract = {We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol (SA+) and one without (SA-). Our results show that SA+ is able to generalize to longer sequences and deeper dependencies. For D2, we find that SA- completely breaks down on long sequences whereas the accuracy of SA+ is 58.82\%. We find attention maps learned by SA+ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn hierarchies without recursion.},
  file = {/Users/nobr/Zotero/storage/P7KHQCYW/Ebrahimi et al. - 2020 - How Can Self-Attention Networks Recognize Dyck-n Languages.pdf}
}

@misc{ecb_spending,
  title = {{{ECB}} Spending Report},
  author = {{Reuters}},
  year = {2018},
  month = dec
}

@misc{economic_times2019,
  title = {Rooftop Solar Projects Must for {{India}} to Meet 175 {{GW}} Renewable Energy Goal: {{Report}}},
  author = {{The Economic Times}},
  year = {2019},
  month = may
}

@article{economist2012carbon,
  title = {Carbon Markets: {{Complete}} Disaster in the Making},
  author = {{The Economist}},
  year = {2012},
  month = sep,
  journal = {The Economist}
}

@article{economist2012complete,
  title = {Complete Disaster in the Making},
  author = {{The Economist}},
  year = {2012},
  month = sep,
  journal = {The Economist}
}

@misc{EEA2017SeaLevel,
  title = {Global and {{European}} Sea Level Rise},
  author = {Agency, European Environment},
  year = {2017},
  howpublished = {https://www.eea.europa.eu/en/analysis/indicators/global-and-european-sea-level-rise}
}

@article{effat2022,
  title = {Geospatial Modeling for Selection of Optimum Sites for Hybrid Solar-Wind Energy in {{Assiut Governorate}}, {{Egypt}}},
  author = {Effat, Hala A. and {El-Zeiny}, Ahmed M.},
  year = {2022},
  month = aug,
  journal = {The Egyptian Journal of Remote Sensing and Space Science},
  volume = {25},
  number = {2},
  pages = {627--637},
  issn = {11109823},
  doi = {10.1016/j.ejrs.2022.03.005},
  urldate = {2023-07-13},
  abstract = {Renewable energy resources are strategic solutions to develop remote urban areas, provide accessibility, meet the growing energy needs, prevent and mitigate adverse environmental impacts of anthropogenic activities. The purpose of the current research paper is to map and designate optimal sites for locating new hybrid solar and wind farms that fulfill suitability criteria for photovoltaic power plants and wind power in Assiut Governorate. SRTM digital elevation model was processed using the ESRI ArcGIS Area solar radiation tool to estimate and map insolation across the landscape on the topographic surface. Same data (SRTM DEM) was used together with the average wind speed map at elevation 50 m to calculate the wind power density. A geospatial model was designed based on three criteria sets covering three themes: first energy resource (climate and topography), second least cost (infrastructure and third, environmental constraints. The Analytic Hierarchy Process, Weighted Linear Combination and Binary overlay methods were applied, resulting in three sub-models. Such were further combined to produce the solar radiation and the wind power suitability indices separately. The solar radiation index values were reclassified; zones with a minimum threshold of 4 kw/m2/day were selected. From such zones, twenty optimum solar sites were selected based on a minimum area threshold. To identify potential wind zones, a minimum of wind power density equivalent to 220 W/m2 was considered. Results reveal that six potential solar sites exist within the potential wind energy zones; such sites were finally selected as candidates for allocation of hybrid solar-wind power stations.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/WY8DSV29/Effat and El-Zeiny - 2022 - Geospatial modeling for selection of optimum sites.pdf}
}

@article{egri2006,
  title = {A {{Compositional Neural-network Solution}} to {{Prime-number Testing}}},
  author = {Egri, L{\'a}szl{\'o} and Shultz, Thomas R},
  year = {2006},
  abstract = {A long-standing difficulty for connectionism has been to implement compositionality, the idea of building a knowledge representation out of components such that the meaning arises from the meanings of the individual components and how they are combined. Here we show how a neural-learning algorithm, knowledge-based cascade-correlation (KBCC), creates a compositional representation of the prime-number concept and uses this representation to decide whether its input n is a prime number or not. KBCC conformed to a basic prime-number testing algorithm by recruiting source networks representing division by prime numbers in order from smallest to largest prime divisor up to {\textsurd}n. KBCC learned how to test prime numbers faster and generalized better to untrained numbers than did similar knowledge-free neural learners. The results demonstrate that neural networks can learn to perform in a compositional manner and underscore the importance of basing learning on existing knowledge.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/nobr/Zotero/storage/HVIJ3RWS/Egri and Shultz - A Compositional Neural-network Solution to Prime-number Testing.pdf}
}

@article{ekaputra2022,
  title = {Emergency {{Shelter Geospatial Location Optimization}} for {{Flood Disaster Condition}}: {{A Review}}},
  shorttitle = {Emergency {{Shelter Geospatial Location Optimization}} for {{Flood Disaster Condition}}},
  author = {Ekaputra, Reza Asriandi and Lee, Changkye and Kee, Seong-Hoon and Yee, Jurng-Jae},
  year = {2022},
  month = sep,
  journal = {Sustainability},
  volume = {14},
  number = {19},
  pages = {12482},
  issn = {2071-1050},
  doi = {10.3390/su141912482},
  urldate = {2023-07-04},
  abstract = {Today, the world is experiencing a tremendous catastrophic disaster that can lead to potential environmental damage. However, awareness of how to deal with this catastrophic situation still remains very low. One of the most critical issues in disaster response is assigning disaster victims to the best emergency shelter location. This article reviews various existing studies to develop a new approach to determining emergency shelter locations. There are four evaluation criteria that are reviewed: optimization objective, decision variable, methodology, and victim identification. From the investigation, there are two major evaluations that can be further developed. In terms of decision variables, most of the previous research applies direct distance (Euclidean Distance) in the analysis process. However, the application of travel distance can represent a real evacuation process. Another interesting point is the victim identification process. Recent research applies grid-based partitioning and administrative-based partitioning. However, this method leads to a bias in the assignment process. This article recommends the application of K-Means clustering method as one of the unsupervised machine learning methods that is rapidly developing in many engineering fields. For better understanding, an example of K-Means clustering application is also provided in this article. Finally, the combination of travel distance and K-Means clustering will be proposed method for any further research.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/QBQ39KTS/Ekaputra et al. - 2022 - Emergency Shelter Geospatial Location Optimization.pdf}
}

@misc{elhage2022,
  title = {Toy {{Models}} of {{Superposition}}},
  author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and {Hatfield-Dodds}, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
  year = {2022},
  month = sep,
  number = {arXiv:2209.10652},
  eprint = {2209.10652},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.10652},
  urldate = {2024-11-06},
  abstract = {Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in "superposition." We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/X49EWTRD/Elhage et al. - 2022 - Toy Models of Superposition.pdf;/Users/nobr/Zotero/storage/MD337T3Q/2209.html}
}

@book{eliade1987,
  title = {The Sacred and the Profane: The Nature of Religion ; [the Groundbreaking Work by One of the Greatest Authorities on Myth, Symbol, and Ritual]},
  shorttitle = {The Sacred and the Profane},
  author = {Eliade, Mircea and Trask, Willard R. and Eliade, Mircea},
  year = {1987},
  series = {A {{Harvest Book}}},
  publisher = {Harcourt, Brace},
  address = {San Diego},
  isbn = {978-0-15-679201-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5TUZ49TE/Eliade et al. - 1987 - The sacred and the profane the nature of religion ; [the groundbreaking work by one of the greatest.pdf}
}

@book{ellenberger1994,
  title = {The Discovery of the Unconscious: The History and Evolution of Dynamic Psychiatry},
  shorttitle = {The Discovery of the Unconscious},
  author = {Ellenberger, Henri F.},
  year = {1994},
  publisher = {Fontana},
  address = {London},
  isbn = {978-0-00-686320-5},
  langid = {english}
}

@misc{ellis2023,
  title = {{{SMACv2}}: {{An Improved Benchmark}} for {{Cooperative Multi-Agent Reinforcement Learning}}},
  shorttitle = {{{SMACv2}}},
  author = {Ellis, Benjamin and Cook, Jonathan and Moalla, Skander and Samvelyan, Mikayel and Sun, Mingfei and Mahajan, Anuj and Foerster, Jakob N. and Whiteson, Shimon},
  year = {2023},
  month = oct,
  number = {arXiv:2212.07489},
  eprint = {2212.07489},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.07489},
  urldate = {2024-02-08},
  abstract = {The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which augments SMACv2 to ensure meaningful partial observability. We show that these changes ensure the benchmark requires the use of *closed-loop* policies. We evaluate state-of-the-art algorithms on SMACv2 and show that it presents significant challenges not present in the original benchmark. Our analysis illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can help benchmark the next generation of MARL methods. Videos of training are available at https://sites.google.com/view/smacv2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/DSFT75P6/Ellis et al. - 2023 - SMACv2 An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/JIPZB6T9/2212.html}
}

@article{eloundou2023,
  title = {{{GPTs}} Are {{GPTs}}: {{An Early Look}} at the {{Labor Market Impact Potential}} of {{Large Language Models}}},
  author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  year = {2023},
  month = mar,
  eprint = {2303.10130},
  abstract = {We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of LLMs, while approximately 19\% of workers may see at least 50\% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15\% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56\% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/CPKW4DRL/document.pdf}
}

@article{erard2023,
  title = {Beyond {{Last Words}}: {{Patterns}} of {{Linguistic}} and {{Interactional Behavior}} in a {{Historical Sample}} of {{Dying Hospital Patients}}},
  shorttitle = {Beyond {{Last Words}}},
  author = {Erard, Michael},
  year = {2023},
  month = feb,
  journal = {OMEGA - Journal of Death and Dying},
  volume = {86},
  number = {3},
  pages = {1089--1107},
  issn = {0030-2228, 1541-3764},
  doi = {10.1177/00302228211000938},
  urldate = {2025-03-03},
  abstract = {Patterns of linguistic and interactional behavior by people at the very end of their lives are not well described, partly because data is difficult to obtain. This paper analyzes descriptions of 486 deaths gathered from 1900 to 1904 in the first-ever clinical study of dying by noted Canadian physician, Sir William Osler. Only 16 patients were noted speaking, and only four canonical last words were reported. The most frequent observation by medical staff was that the deaths were quiet (n {$\frac{1}{4}$} 30), though range of other behaviors were noted (e.g., moaning, delirium, seeming intention to speak). Osler's problematic study left behind data whose analysis is a small step toward empirically characterizing the linguistic and interactional details of a previously under-described phenomena as well as the importance of the social context in which they occur.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IKRH8MJ7/Erard - 2023 - Beyond Last Words Patterns of Linguistic and Interactional Behavior in a Historical Sample of Dying.pdf}
}

@book{euclides2007,
  title = {Euclid's {{Elements}}: All Thirteen Books Complete in One Volume},
  shorttitle = {Euclid's {{Elements}}},
  author = {Euclides and Heath, Thomas L. and Densmore, Dana and Euclides},
  year = {2007},
  publisher = {Green Lion Press},
  address = {Santa Fe, NM},
  isbn = {978-1-888009-18-7},
  langid = {english}
}

@book{euclides2008,
  title = {Euclid's Elements of Geometry: The {{Greek}} Text of {{J}}.{{L}}. {{Heiberg}} (1883 - 1885): From {{Euclidis Elementa}}, Edidit et {{Latine}} Interpretatus Est {{I}}.{{L}}. {{Heiberg}}, in Aedibus {{B}}.{{G}}. {{Teubneri}}, 1883-1885},
  shorttitle = {Euclid's Elements of Geometry},
  author = {Euclides},
  editor = {Fitzpatrick, Richard},
  translator = {Fitzpatrick, Richard},
  year = {2008},
  edition = {Revised and corrected},
  publisher = {s.n},
  address = {s.l},
  isbn = {978-0-615-17984-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZD3LBRW8/Euclides - 2008 - Euclid's elements of geometry the Greek text of J.L. Heiberg (1883 - 1885) from Euclidis Elementa,.pdf}
}

@article{evensen,
  title = {Wargaming {{Evolved}}: {{Methodology}} and {{Best Practices}} for {{Simulation-Supported Wargaming}}},
  author = {Evensen, Per-Idar and Hals{\o}r, Marius and Martinussen, Svein Erlend and Bentsen, Dan Helge},
  year = {2019},
  journal = {artificial intelligence},
  abstract = {When developing and assessing future force structures, wargaming is a key activity for better understanding the strengths and weaknesses of the force structures. Today simulation systems let us create synthetic environments that to a high degree replicate the physical properties of the real world for these wargames. Furthermore, advances in artificial intelligence (AI) and behavior modeling has given us more realistic computer-generated forces (CGF) that can execute battle drills and lower level tactics with a fairly high degree of realism. However, at the higher levels of the chain of command, AI has not yet replaced human leadership, and planning and conducting simulated operations require participation of officers.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7AEGG9UI/Evensen et al. - Wargaming Evolved Methodology and Best Practices for Simulation-Supported Wargaming.pdf}
}

@article{evensen2009,
  title = {Wargaming {{Evolved}}: {{Methodology}} and {{Best Practices}} for {{Simulation-Supported Wargaming}}},
  author = {Evensen, Per-Idar and Hals{\o}r, Marius and Martinussen, Svein Erlend and Bentsen, Dan Helge},
  year = {2009},
  journal = {artificial intelligence},
  abstract = {When developing and assessing future force structures, wargaming is a key activity for better understanding the strengths and weaknesses of the force structures. Today simulation systems let us create synthetic environments that to a high degree replicate the physical properties of the real world for these wargames. Furthermore, advances in artificial intelligence (AI) and behavior modeling has given us more realistic computer-generated forces (CGF) that can execute battle drills and lower level tactics with a fairly high degree of realism. However, at the higher levels of the chain of command, AI has not yet replaced human leadership, and planning and conducting simulated operations require participation of officers.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6QRFTZN4/Evensen et al. - Wargaming Evolved Methodology and Best Practices for Simulation-Supported Wargaming.pdf}
}

@misc{exame2017solar,
  title = {Solar Energy Sector Employed Twice as Many People as the Coal Industry},
  author = {{EXAME Magazine}},
  year = {2017},
  month = feb
}

@misc{exame2018petrobras,
  title = {Since the Strike Began, {{Petrobras}} Has Already Lost {{R}}\$126 Billion in Market Value},
  author = {{EXAME Magazine}},
  year = {2018},
  month = apr
}

@inproceedings{fabien2020,
  title = {{{BertAA}} : {{BERT}} Fine-Tuning for {{Authorship Attribution}}},
  shorttitle = {{{BertAA}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Natural Language Processing}} ({{ICON}})},
  author = {Fabien, Ma{\"e}l and {Villatoro-Tello}, Esau and Motlicek, Petr and Parida, Shantipriya},
  year = {2020},
  month = dec,
  pages = {127--137},
  publisher = {NLP Association of India (NLPAI)},
  address = {Indian Institute of Technology Patna, Patna, India},
  urldate = {2023-06-11},
  abstract = {Identifying the author of a given text can be useful in historical literature, plagiarism detection, or police investigations. Authorship Attribution (AA) has been well studied and mostly relies on a large feature engineering work. More recently, deep learning-based approaches have been explored for Authorship Attribution (AA). In this paper, we introduce BertAA, a fine-tuning of a pre-trained BERT language model with an additional dense layer and a softmax activation to perform authorship classification. This approach reaches competitive performances on Enron Email, Blog Authorship, and IMDb (and IMDb62) datasets, up to 5.3\% (relative) above current state-of-the-art approaches. We performed an exhaustive analysis allowing to identify the strengths and weaknesses of the proposed method. In addition, we evaluate the impact of including additional features (e.g. stylometric and hybrid features) in an ensemble approach, improving the macro-averaged F1-Score by 2.7\% (relative) on average.},
  file = {/Users/nobr/Zotero/storage/YVKB88JY/Fabien et al. - 2020 - BertAA  BERT fine-tuning for Authorship Attributi.pdf}
}

@misc{faldor2024,
  title = {{{OMNI-EPIC}}: {{Open-endedness}} via {{Models}} of Human {{Notions}} of {{Interestingness}} with {{Environments Programmed}} in {{Code}}},
  shorttitle = {{{OMNI-EPIC}}},
  author = {Faldor, Maxence and Zhang, Jenny and Cully, Antoine and Clune, Jeff},
  year = {2024},
  month = oct,
  number = {arXiv:2405.15568},
  eprint = {2405.15568},
  publisher = {arXiv},
  urldate = {2024-10-30},
  abstract = {Open-ended and AI-generating algorithms aim to continuously generate and solve increasingly complex tasks indefinitely, offering a promising path toward more general intelligence. To accomplish this grand vision, learning must occur within a vast array of potential tasks. Existing approaches to automatically generating environments are constrained within manually predefined, often narrow distributions of environment, limiting their ability to create any learning environment. To address this limitation, we introduce a novel framework, OMNI-EPIC, that augments previous work in Open-endedness via Models of human Notions of Interestingness (OMNI) with Environments Programmed in Code (EPIC). OMNI-EPIC leverages foundation models to autonomously generate code specifying the next learnable (i.e., not too easy or difficult for the agent's current skill set) and interesting (e.g., worthwhile and novel) tasks. OMNI-EPIC generates both environments (e.g., an obstacle course) and reward functions (e.g., progress through the obstacle course quickly without touching red objects), enabling it, in principle, to create any simulatable learning task. We showcase the explosive creativity of OMNI-EPIC, which continuously innovates to suggest new, interesting learning challenges. We also highlight how OMNI-EPIC can adapt to reinforcement learning agents' learning progress, generating tasks that are of suitable difficulty. Overall, OMNI-EPIC can endlessly create learnable and interesting environments, further propelling the development of self-improving AI systems and AI-Generating Algorithms. Project website with videos: https://dub.sh/omniepic},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/3BGUJTNS/Faldor et al. - 2024 - OMNI-EPIC Open-endedness via Models of human Notions of Interestingness with Environments Programme.pdf;/Users/nobr/Zotero/storage/LCSLTFGK/2405.html}
}

@article{famm2013,
  title = {A Jump-Start for Electroceuticals},
  author = {Famm, Kristoffer},
  year = {2013},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/URE93TGC/Famm - A jump-start for electroceuticals.pdf}
}

@techreport{fan2022,
  type = {Preprint},
  title = {Impact of the Size of {{CRISPR-Cas}} Gene Cassettes on the Packaging Efficiency and Transduction Titer of Lentiviral Vectors},
  author = {Fan, Minghui and Berkhout, Ben and {Herrera-Carrillo}, Elena},
  year = {2022},
  month = mar,
  institution = {In Review},
  doi = {10.21203/rs.3.rs-1458483/v1},
  urldate = {2024-03-20},
  abstract = {Background CRISPR-Cas technology has revolutionized the use of genome-editing in studies on diverse biological topics. Lentiviral vectors (LVs) have been widely used for the generation of stably transduced cells that durably express the Cas protein and guide RNA (gRNA), the key components of the CRISPR-Cas editing system. This delivery system allows sustained gene expression in dividing and non-dividing cells in vitro and in vivo and LVs can be pseudo-typed with a variety of envelope proteins to establish a broad tropism for a range of target cells. A major challenge on the use of LVs in clinical settings is the requirement to obtain vector stocks of a su ciently high titer. The endonuclease encoded by Streptococcus pyogenes Cas9 (SpCas9) is the most widely used CRISPR endonuclease for genome engineering, but it has an important limitation because of its relatively large size ({$\sim$}4.1 kb coding sequence) that may hamper the delivery by means of viral vector systems. Smaller Cas endonucleases have been developed more recently, but these systems still need to be validated in speci c experimental settings. Results This work aimed to assess the impact of the CRISPR-Cas transgene size on the production of LVs and their ability to transduce cells. We observed that the LV transduction e ciency measured on the SupT1 T cell line dropped dramatically for RNA transcripts approaching 8 kb, e.g. a further increase of 1 kb reduced the transduction e ciency by more than 5-fold. As the number of produced virus particles was not affected, this means that the use of larger transgenes may cause the production of more empty virion particles. Conclusions This study con rmed that the transfer e ciency decreased for CRISPR-Cas genes above a certain size. One of the strategies to avoid the restricted RNA packaging can be the use of smaller CRISPR-Cas systems or the inclusion of truncated versions of the regulatory sequences like the promoters that are needed for e cient expression of the transgenes. This work has implications for the design of effective CRISPR-Cas editing therapies that use viral vectors, LVs in particular.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JGQYUSAS/Fan et al. - 2022 - Impact of the size of CRISPR-Cas gene cassettes on the packaging efficiency and transduction titer o.pdf}
}

@article{faragher2012,
  title = {Understanding the {{Basis}} of the {{Kalman Filter Via}} a {{Simple}} and {{Intuitive Derivation}} [{{Lecture Notes}}]},
  author = {Faragher, Ramsey},
  year = {2012},
  month = sep,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {5},
  pages = {128--132},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2203621},
  urldate = {2025-03-03},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/CLPDUSTS/Faragher - 2012 - Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation [Lecture Notes].pdf}
}

@misc{farisco2024,
  title = {Is Artificial Consciousness Achievable? {{Lessons}} from the Human Brain},
  shorttitle = {Is Artificial Consciousness Achievable?},
  author = {Farisco, Michele and Evers, Kathinka and Changeux, Jean-Pierre},
  year = {2024},
  month = apr,
  number = {arXiv:2405.04540},
  eprint = {2405.04540},
  primaryclass = {cs, q-bio},
  doi = {10.48550/arXiv.2405.04540},
  urldate = {2024-05-19},
  abstract = {We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (structural and architectural) and extrinsic (related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it is theoretically possible that AI research can develop partial or potentially alternative forms of consciousness that is qualitatively different from the human, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word consciousness for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify what is common and what differs in AI conscious processing from full human conscious experience.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/HUNNU4ET/Farisco et al. - 2024 - Is artificial consciousness achievable Lessons from the human brain.pdf}
}

@article{farkatdiogenes2020,
  title = {Overcoming Barriers to Onshore Wind Farm Implementation in {{Brazil}}},
  author = {Farkat Di{\'o}genes, Jamil Ramsi and Coelho Rodrigues, Jos{\'e} and Farkat Di{\'o}genes, Maria Caroline and Claro, Jo{\~a}o},
  year = {2020},
  month = mar,
  journal = {Energy Policy},
  volume = {138},
  pages = {111165},
  issn = {03014215},
  doi = {10.1016/j.enpol.2019.111165},
  urldate = {2023-07-13},
  abstract = {Brazil has been failing to offer the most favorable conditions for the implementation of onshore wind farms, due to the presence of multiple barriers. However, the country has observed a fast and expressive wind energy (WE) diffusion (the installed WE capacity grew 37 times in the last decade). Furthermore, its onshore wind farms have reached impressive capacity factors (with productivity levels much higher than the average around the world) and a very low levelized cost of electricity. This study aims at identifying how wind developers plan onshore wind farms to overcome existing barriers. Based on forty-one interviews with relevant stakeholders of the Brazilian WE sector, the study identified efforts targeted at overcoming twenty-four previously identified bar\- riers. Although most barriers may be overcome directly through developer initiatives, addressing higher level barriers, namely an unstable macroeconomic environment, a poor transmission infrastructure, and inadequate access to capital, depends on government actions.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7THSENT5/Farkat Diógenes et al. - 2020 - Overcoming barriers to onshore wind farm implement.pdf}
}

@article{farooq2016,
  title = {{{{\emph{StarCraft}}}} {{AI Competition}}: {{A Step Toward Human}}-{{Level AI}} for {{Real}}-{{Time Strategy Games}}},
  shorttitle = {{{{\emph{StarCraft}}}} {{AI Competition}}},
  author = {Farooq, Sehar Shahzad and Oh, In-Suk and Kim, Man-Jae and Kim, Kyung Joong},
  year = {2016},
  month = jun,
  journal = {AI Magazine},
  volume = {37},
  number = {2},
  pages = {102--106},
  issn = {0738-4602, 2371-9621},
  doi = {10.1609/aimag.v37i2.2657},
  urldate = {2024-12-04},
  abstract = {This article reviews the two most recent IEEE Conference on Computational Intelligence and Games (CIG) StarCraft Artificial Intelligence (AI) Competitions organized by the authors; these were the fourth and fifth in a series of annual competitions initiated in 2011. StarCraft AI Competitions have been hosted in conjunction with three different events: the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE), CIG, and Student StarCraft AI Tournament (SSCAIT). The purpose of these competitions is to design bots that are able autonomously and successfully to play the StarCraft game by implementing real-time strategies. Recent results reveal the promising use of AI techniques in creating successful AI entries, but there is room for improvement with respect to the bots' ability to adapt and learn to defeat humans and scripted AI bots.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/VHZLAN6D/Farooq et al. - 2016 - StarCraft AI Competition A Step Toward Human‐Level AI for Real‐Time Strategy Games.pdf}
}

@article{farquhar2024,
  title = {Detecting Hallucinations in Large Language Models Using Semantic Entropy},
  author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  year = {2024},
  month = jun,
  journal = {Nature},
  volume = {630},
  number = {8017},
  pages = {625--630},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07421-0},
  urldate = {2024-06-24},
  abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often `hallucinate' false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has~been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations---confabulations---which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computer science,Information technology},
  file = {/Users/nobr/Zotero/storage/2JQHJNAT/Farquhar et al. - 2024 - Detecting hallucinations in large language models using semantic entropy.pdf}
}

@misc{faskowitz2021,
  title = {Edges in {{Brain Networks}}: {{Contributions}} to {{Models}} of {{Structure}} and {{Function}}},
  shorttitle = {Edges in {{Brain Networks}}},
  author = {Faskowitz, Joshua and Betzel, Richard F. and Sporns, Olaf},
  year = {2021},
  month = may,
  number = {arXiv:2105.07069},
  eprint = {2105.07069},
  primaryclass = {q-bio},
  publisher = {arXiv},
  urldate = {2023-05-16},
  abstract = {Network models describe the brain as sets of nodes and edges that represent its distributed organization. So far, most discoveries in network neuroscience have prioritized insights that highlight distinct groupings and specialized functional contributions of network nodes. Importantly, these functional contributions are determined and expressed by the web of their interrelationships, formed by network edges. Here, we underscore the important contributions made by brain network edges for understanding distributed brain organization. Different types of edges represent different types of relationships, including connectivity and similarity among nodes. Adopting a specific definition of edges can fundamentally alter how we analyze and interpret a brain network. Furthermore, edges can associate into collectives and higher-order arrangements, describe time series, and form edge communities that provide insights into brain network topology complementary to the traditional node-centric perspective. Focusing on the edges, and the higher-order or dynamic information they can provide, discloses previously underappreciated aspects of structural and functional network organization.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/KQZK2YPZ/Faskowitz et al. - 2021 - Edges in Brain Networks Contributions to Models o.pdf;/Users/nobr/Zotero/storage/88ZIWI9R/2105.html}
}

@article{fedorenko2024,
  title = {Language Is Primarily a Tool for Communication Rather than Thought},
  author = {Fedorenko, Evelina and Piantadosi, Steven T. and Gibson, Edward A. F.},
  year = {2024},
  month = jun,
  journal = {Nature},
  volume = {630},
  number = {8017},
  pages = {575--586},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-024-07522-w},
  urldate = {2024-08-11},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2TIFEGLG/Fedorenko et al. - 2024 - Language is primarily a tool for communication rather than thought.pdf}
}

@misc{feilong2023,
  title = {A Cortical Surface Template for Human Neuroscience},
  author = {Feilong, Ma and Jiahui, Guo and Gobbini, M. Ida and Haxby, James V.},
  year = {2023},
  month = mar,
  primaryclass = {New Results},
  pages = {2023.03.21.533686},
  publisher = {bioRxiv},
  doi = {10.1101/2023.03.21.533686},
  urldate = {2023-05-12},
  abstract = {Neuroimaging data analysis relies on normalization to standard anatomical templates to resolve macroanatomical differences across brains. Existing human cortical surface templates sample locations unevenly because of distortions introduced by inflation of the folded cortex into a standard shape. Here we present the onavg template, which affords uniform sampling of the cortex. We created the onavg template based on openly-available high-quality structural scans of 1,031 brains---25 times more than existing cortical templates. We optimized the vertex locations based on cortical anatomy, achieving an even distribution. We observed consistently higher multivariate pattern classification accuracies and representational geometry inter-subject correlations based on onavg than on other templates, and onavg only needs 3{\textfractionsolidus}4 as much data to achieve the same performance compared to other templates. The optimized sampling also reduces CPU time across algorithms by 1.3\%--22.4\% due to less variation in the number of vertices in each searchlight.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2K27ZRRQ/Feilong et al. - 2023 - A cortical surface template for human neuroscience.pdf}
}

@article{fernandez2023,
  title = {How {{Large Language Models Will Disrupt Data Management}}},
  author = {Fernandez, Raul Castro and Elmore, Aaron J. and Franklin, Michael J. and Krishnan, Sanjay and Tan, Chenhao},
  year = {2023},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {16},
  number = {11},
  pages = {3302--3309},
  issn = {2150-8097},
  doi = {10.14778/3611479.3611527},
  urldate = {2024-08-22},
  abstract = {Large language models (LLMs), such as GPT-4, are revolutionizing software's ability to understand, process, and synthesize language. The authors of this paper believe that this advance in technology is significant enough to prompt introspection in the data management community, similar to previous technological disruptions such as the advents of the world wide web, cloud computing, and statistical machine learning. We argue that the disruptive influence that LLMs will have on data management will come from two angles. (1) A number of hard database problems, namely, entity resolution, schema matching, data discovery, and query synthesis, hit a ceiling of automation because the system does not fully understand the semantics of the underlying data. Based on large training corpora of natural language, structured data, and code, LLMs have an unprecedented ability to ground database tuples, schemas, and queries in real-world concepts. We will provide examples of how LLMs may completely change our approaches to these problems. (2) LLMs blur the line between predictive models and information retrieval systems with their ability to answer questions. We will present examples showing how large databases and information retrieval systems have complementary functionality.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RJD4734F/p3302-fernandez.pdf}
}

@misc{fernandoPromptbreederSelfReferentialSelfImprovement2023,
  title = {Promptbreeder: {{Self-Referential Self-Improvement Via Prompt Evolution}}},
  shorttitle = {Promptbreeder},
  author = {Fernando, Chrisantha and Banarse, Dylan and Michalewski, Henryk and Osindero, Simon and Rockt{\"a}schel, Tim},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16797},
  eprint = {2309.16797},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.16797},
  urldate = {2023-11-19},
  abstract = {Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furthermore, Promptbreeder is able to evolve intricate task-prompts for the challenging problem of hate speech classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/9N5Z2NYE/Fernando et al. - 2023 - Promptbreeder Self-Referential Self-Improvement Via Prompt Evolution.pdf;/Users/nobr/Zotero/storage/32C4LD8L/2309.html}
}

@misc{ferrante2023,
  title = {Semantic {{Brain Decoding}}: From {{fMRI}} to Conceptually Similar Image Reconstruction of Visual Stimuli},
  shorttitle = {Semantic {{Brain Decoding}}},
  author = {Ferrante, Matteo and Boccato, Tommaso and Toschi, Nicola},
  year = {2023},
  month = mar,
  number = {arXiv:2212.06726},
  eprint = {2212.06726},
  primaryclass = {cs, q-bio},
  doi = {10.48550/arXiv.2212.06726},
  urldate = {2023-05-19},
  abstract = {Brain decoding is a field of computational neuroscience that uses measurable brain activity to infer mental states or internal representations of perceptual inputs. Therefore, we propose a novel approach to brain decoding that also relies on semantic and contextual similarity. We employ an fMRI dataset of natural image vision and create a deep learning decoding pipeline inspired by the existence of both bottom-up and top-down processes in human vision. We train a linear brain-to-feature model to map fMRI activity features to visual stimuli features, assuming that the brain projects visual information onto a space that is homeomorphic to the latent space represented by the last convolutional layer of a pretrained convolutional neural network, which typically collects a variety of semantic features that summarize and highlight similarities and differences between concepts. These features are then categorized in the latent space using a nearest-neighbor strategy, and the results are used to condition a generative latent diffusion model to create novel images. From fMRI data only, we produce reconstructions of visual stimuli that match the original content very well on a semantic level, surpassing the state of the art in previous literature. We evaluate our work and obtain good results using a quantitative semantic metric (the Wu-Palmer similarity metric over the WordNet lexicon, which had an average value of 0.57) and perform a human evaluation experiment that resulted in correct evaluation, according to the multiplicity of human criteria in evaluating image similarity, in over 80\% of the test set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/6WXMW8JC/Ferrante et al. - 2023 - Semantic Brain Decoding from fMRI to conceptually similar image reconstruction of visual stimuli.pdf}
}

@article{ferreira2021,
  title = {Bayesian Spatial and Spatiotemporal Models Based on Multiscale Factorizations},
  author = {Ferreira, Marco A. R.},
  year = {2021},
  month = mar,
  journal = {WIREs Computational Statistics},
  volume = {13},
  number = {2},
  pages = {e1509},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1509},
  urldate = {2023-11-22},
  abstract = {We review the literature on spatial and spatiotemporal models based on spatial multiscale factorizations. Specifically, we review models based on wavelets and Kolaczyk--Huang factorizations for Gaussian and Poisson data. These multiscale models decompose spatial and spatiotemporal datasets into many small components, called multiscale coefficients, at multiple levels of spatial resolution. Then analysis proceeds independently for each multiscale coefficient. After that, aggregation equations are used to coherently combine the analyses from the multiple multiscale coefficients to obtain a statistical analysis at the original resolution level. The computational cost of such analysis grows linearly with sample size. Furthermore, computations for these models are scalable, parallelizable, and fast. Therefore, these multiscale models are tremendously useful for the analysis of massive spatial and spatiotemporal datasets.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/FZPJQKQK/Ferreira - 2021 - Bayesian spatial and spatiotemporal models based on multiscale factorizations.pdf}
}

@article{fey2023,
  title = {Relational {{Deep Learning}}: {{Graph Representation Learning}} on {{Relational Databases}}},
  author = {Fey, Matthias and Hu, Weihua and Huang, Kexin and Lenssen, Jan Eric and Ranjan, Rishabh and Robinson, Joshua and Ying, Rex and You, Jiaxuan and Leskovec, Jure},
  year = {2023},
  abstract = {Much of the world's most valued data is stored in data warehouses, where the data is spread across many tables connected by primary-foreign key relations. However, building machine learning models using this data is both challenging and time consuming. The core problem is that no machine learning method is capable of learning directly on the data spread across multiple relational tables. Current methods can only learn from a single table, so the data must first be joined and aggregated into a single training table, the process known as feature engineering. Here we introduce an end-to-end deep representation learning approach to directly learn on data spread across multiple tables. We name our approach Relational Deep Learning. The core idea is to view relational tables as a heterogeneous graph, with a node for each row in each table, and edges specified by primary-foreign key relations. Message Passing Neural Networks can then automatically learn across multiple tables to extract representations that leverage all input data, without any manual feature engineering. To facilitate research, we also develop RELBENCH, a set of benchmark datasets and an implementation of Relational Deep Learning. The data covers a wide spectrum, from discussions on Stack Exchange to book reviews on the Amazon Product Catalog. Overall, we define a new research area that generalizes graph machine learning and broadens its applicability to a wide set of AI use cases.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZB3E23NZ/Fey et al. - Relational Deep Learning Graph Representation Learning on Relational Databases.pdf}
}

@article{fields2013,
  title = {A {{Gentle Introduction}} to the {{Art}} of {{Mathematics}}, {{Version}} 3.1},
  author = {Fields, Joseph E},
  year = {2013},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SHKXHHMA/Fields - A Gentle Introduction to the Art of Mathematics, Version 3.1.pdf}
}

@article{fiete2007,
  title = {Neural Network Models of Birdsong Production, Learning, and Coding.},
  author = {Fiete, Ila R and Seung, H Sebastian},
  year = {2007},
  abstract = {Birdsong involves motor sequence generation and goal-directed sensorimotor learning, and is controlled by a discrete set of premotor brain nuclei. These features make it an ideal system for theoretical explorations of the neural basis for motor learning and control. We review neural network models of various aspects of song production, in particular the formation of neural sequences to drive song, the learning of the motor map, and the reasons why song may be encoded the way it is by songbird premotor neurons. Our emphasis is on illustrating how theoretical work has contributed to our understanding of the song system and highlighting the resulting predictions for experiment.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SR9HFHE8/Fiete and Seung - 2007 - Neural network models of birdsong production, learning, and coding..pdf}
}

@article{filgueiras2003,
  title = {Wind Energy in {{Brazil}}---Present and Future},
  author = {Filgueiras, Alexandre and Silva, Thelma Maria V.e},
  year = {2003},
  month = oct,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {7},
  number = {5},
  pages = {439--451},
  issn = {13640321},
  doi = {10.1016/S1364-0321(03)00068-6},
  urldate = {2023-07-13},
  abstract = {In Brazil, the power generation is predominantly hydroelectric, corresponding approximately to 91.4\% of the installed capacity. The energy crisis in the Brazilian electric sector meant the end of the centralized generation and the launching of a decentralized generation approach aiming to add to the existing plants through small- and medium power capacity. Such a condition matches the wind energy characteristic profile. In northeastern Brazil is one of the most promising areas for wind exploitation, where the largest wind speed occurs exactly when the rate of flow in Sao Francisco river, which accounts for all the power energy supply of the northeast region, is low. Thus, during this critical period, the wind farms can produce electrical energy, saving the Sao Francisco waters and on top of that with no environmental risk, thus contributing to the overall reduction of CO2 emission in the atmosphere. Because of those factors, the National Electrical Energy Agency (ANEEL), the Brazilian regulatory authority, has approved 77 projects for construction of wind farms, attracting foreign investment, besides the installation of wind turbine industries. The increasing use of the wind energy is prevailing over the absence of a specific legislation for the sector, but the National Congress has already taken some measures such as the act bill no. 2905/2000 and the temporary measure no. 14/2001.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/C6XBY4JL/Filgueiras and Silva - 2003 - Wind energy in Brazil—present and future.pdf}
}

@article{filho2020,
  title = {Transitivity and Degree Assortativity Explained: {{The}} Bipartite Structure of Social Networks},
  shorttitle = {Transitivity and Degree Assortativity Explained},
  author = {Filho, Demival Vasques and O'Neale, Dion R. J.},
  year = {2020},
  month = may,
  journal = {Physical Review E},
  volume = {101},
  number = {5},
  eprint = {1912.03211},
  primaryclass = {physics},
  pages = {052305},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.101.052305},
  urldate = {2023-11-09},
  abstract = {Dynamical processes, such as the diffusion of knowledge, opinions, pathogens, ``fake news'', innovation, and others, are highly dependent on the structure of the social network on which they occur. However, questions on why most social networks present some particular structural features, namely high levels of transitivity and degree assortativity, when compared to other types of networks remain open. First, we argue that every one-mode network can be regarded as a projection of a bipartite network, and show that this is the case using two simple examples solved with the generating functions formalism. Second, using synthetic and empirical data, we reveal how the combination of the degree distribution of both sets of nodes of the bipartite network --- together with the presence of cycles of length four and six --- explains the observed levels of transitivity and degree assortativity in the one-mode projected network. Bipartite networks with top node degrees that display a more right-skewed distribution than the bottom nodes result in highly transitive and degree assortative projections, especially if a large number of small cycles are present in the bipartite structure.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society},
  file = {/Users/nobr/Zotero/storage/MICLGMB2/Filho and O'Neale - 2020 - Transitivity and degree assortativity explained The bipartite structure of social networks.pdf}
}

@misc{fioresi2023,
  title = {Deep {{Learning}} and {{Geometric Deep Learning}}: An Introduction for Mathematicians and Physicists},
  shorttitle = {Deep {{Learning}} and {{Geometric Deep Learning}}},
  author = {Fioresi, R. and Zanchetta, F.},
  year = {2023},
  month = may,
  number = {arXiv:2305.05601},
  eprint = {2305.05601},
  primaryclass = {math-ph},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {In this expository paper we want to give a brief introduction, with few key references for further reading, to the inner functioning of the new and successfull algorithms of Deep Learning and Geometric Deep Learning with a focus on Graph Neural Networks. We go over the key ingredients for these algorithms: the score and loss function and we explain the main steps for the training of a model. We do not aim to give a complete and exhaustive treatment, but we isolate few concepts to give a fast introduction to the subject. We provide some appendices to complement our treatment discussing Kullback-Leibler divergence, regression, Multi-layer Perceptrons and the Universal Approximation Theorem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematical Physics},
  file = {/Users/nobr/Zotero/storage/FNSU7SSR/Fioresi and Zanchetta - 2023 - Deep Learning and Geometric Deep Learning an intr.pdf}
}

@article{fischer2022,
  title = {Spatial Variation in Bicycling Risk Based on Crowdsourced Safety Data},
  author = {Fischer, Jaimy and Sersli, Stephanie and Nelson, Trisalyn and Yu, Hanchen and Laberee, Karen and Zanotto, Moreno and Winters, Meghan},
  year = {2022},
  month = sep,
  journal = {The Canadian Geographer / Le G{\'e}ographe canadien},
  volume = {66},
  number = {3},
  pages = {556--568},
  issn = {0008-3658, 1541-0064},
  doi = {10.1111/cag.12756},
  urldate = {2023-07-04},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/46TEXJ5E/Fischer et al. - 2022 - Spatial variation in bicycling risk based on crowd.pdf}
}

@article{fischl1999,
  title = {High-Resolution Intersubject Averaging and a Coordinate System for the Cortical Surface},
  author = {Fischl, Bruce and Sereno, Martin I. and Tootell, Roger B.H. and Dale, Anders M.},
  year = {1999},
  journal = {Human Brain Mapping},
  volume = {8},
  number = {4},
  pages = {272--284},
  issn = {1065-9471, 1097-0193},
  doi = {10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4},
  urldate = {2024-12-19},
  abstract = {The neurons of the human cerebral cortex are arranged in a highly folded sheet, with the majority of the cortical surface area buried in folds. Cortical maps are typically arranged with a topography oriented parallel to the cortical surface. Despite this unambiguous sheetlike geometry, the most commonly used coordinate systems for localizing cortical features are based on 3-D stereotaxic coordinates rather than on position relative to the 2-D cortical sheet. In order to address the need for a more natural surface-based coordinate system for the cortex, we have developed a means for generating an average folding pattern across a large number of individual subjects as a function on the unit sphere and of nonrigidly aligning each individual with the average. This establishes a spherical surface-based coordinate system that is adapted to the folding pattern of each individual subject, allowing for much higher localization accuracy of structural and functional features of the human brain. Hum. Brain Mapping 8:272--284, 1999.},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RPFLGD2V/Fischl et al. - 1999 - High-resolution intersubject averaging and a coordinate system for the cortical surface.pdf}
}

@article{fizek2018,
  title = {Automated {{State}} of {{Play}}: {{Rethinking Anthropocentric Rules}} of the {{Game}}},
  shorttitle = {Automated {{State}} of {{Play}}},
  author = {Fizek, Sonia},
  year = {2018},
  month = mar,
  journal = {Digital Culture \& Society},
  volume = {4},
  number = {1},
  pages = {201--214},
  issn = {2364-2122, 2364-2114},
  doi = {10.14361/dcs-2018-0112},
  urldate = {2023-08-10},
  abstract = {Automation of play has become an ever more noticeable phenomenon in the domain of video games, expressed by self-playing game worlds, self-acting characters, and non-human agents traversing multiplayer spaces. This article proposes to look at AI-driven non-human play and, what follows, rethink digital games, taking into consideration their cybernetic nature, thus departing from the anthropocentric perspectives dominating the field of Game Studies. A decentralised post-humanist reading, as the author argues, not only allows to rethink digital games and play, but is a necessary condition to critically reflect AI, which due to the fictional character of video games, often plays by very different rules than the so-called ``true'' AI.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/S4IEFYWA/Fizek - 2018 - Automated State of Play Rethinking Anthropocentri.pdf}
}

@book{flaccus2014,
  title = {Argonautica},
  author = {Flaccus, Valerius Flaccus},
  year = {2014},
  publisher = {Cambridge, MA : Harvard University Press},
  urldate = {2024-01-11},
  abstract = {{$<$}?xml version="1.0" encoding="utf-8"?{$>$}Valerius Flaccus, Gaius, Latin poet who flourished in the period ca. 70\&amp;ndash;90 CE, composed in smooth and sometimes obscure style an incomplete epic Argonautica in eight books, on the Quest for the...},
  isbn = {978-0-674-99316-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JAFX6NAH/volume.html}
}

@article{flores-ortega2023,
  title = {Network Efficiency of Spatial Systems with Fractal Morphology: A Geometric Graphs Approach},
  shorttitle = {Network Efficiency of Spatial Systems with Fractal Morphology},
  author = {{Flores-Ortega}, A. C. and {Nicol{\'a}s-Carlock}, J. R. and {Carrillo-Estrada}, J. L.},
  year = {2023},
  month = oct,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {18706},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-45962-y},
  urldate = {2023-11-02},
  abstract = {The functional features of spatial networks depend upon a non-trivial relationship between the topological and physical structure. Here, we explore that relationship for spatial networks with radial symmetry and disordered fractal morphology. Under a geometric graphs approach, we quantify the effectiveness of the exchange of information in the system from center to perimeter and over the entire network structure. We mainly consider two paradigmatic models of disordered fractal formation, the Ballistic Aggregation and Diffusion-Limited Aggregation models, and complementary, the Viscek and Hexaflake fractals, and Kagome and Hexagonal lattices. First, we show that complex tree morphologies provide important advantages over regular configurations, such as an invariant structural cost for different fractal dimensions. Furthermore, although these systems are known to be scale-free in space, they have bounded degree distributions for different values of an euclidean connectivity parameter and, therefore, do not represent ordinary scale-free networks. Finally, compared to regular structures, fractal trees are fragile and overall inefficient as expected, however, we show that this efficiency can become similar to that of a robust hexagonal lattice, at a similar cost, by just considering a very short euclidean connectivity beyond first neighbors.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Complex networks,Statistical physics},
  file = {/Users/nobr/Zotero/storage/FC2U653T/Flores-Ortega et al. - 2023 - Network efficiency of spatial systems with fractal morphology a geometric graphs approach.pdf}
}

@inproceedings{foerster2018,
  title = {Counterfactual Multi-Agent Policy Gradients},
  booktitle = {Proceedings of the {{Thirty-Second AAAI Conference}} on {{Artificial Intelligence}} and {{Thirtieth Innovative Applications}} of {{Artificial Intelligence Conference}} and {{Eighth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}},
  author = {Foerster, Jakob N. and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  year = {2018},
  month = feb,
  series = {{{AAAI}}'18/{{IAAI}}'18/{{EAAI}}'18},
  pages = {2974--2982},
  publisher = {AAAI Press},
  address = {New Orleans, Louisiana, USA},
  urldate = {2024-01-02},
  abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
  isbn = {978-1-57735-800-8},
  file = {/Users/nobr/Zotero/storage/7VAEF6UP/Foerster et al. - 2018 - Counterfactual multi-agent policy gradients.pdf}
}

@book{fogel2006,
  title = {Evolutionary Computation: Toward a New Philosophy of Machine Intelligence},
  shorttitle = {Evolutionary Computation},
  author = {Fogel, David B.},
  year = {2006},
  edition = {3rd ed},
  publisher = {John Wiley \& Sons},
  address = {Hoboken, N.J},
  collaborator = {IEEE Neural Networks Council},
  isbn = {978-0-471-66951-7},
  langid = {english},
  lccn = {QA76.9.C65 F64 2006},
  keywords = {Artificial intelligence,Computer simulation,Evolutionary computation},
  file = {/Users/nobr/Zotero/storage/YBKSSM4Y/Fogel - 2006 - Evolutionary computation toward a new philosophy of machine intelligence.pdf}
}

@misc{forney1965,
  title = {Concatenated {{Codes}}},
  author = {Forney, David},
  year = {1965},
  urldate = {2023-12-11},
  howpublished = {https://apps.dtic.mil/sti/pdfs/AD0628956.pdf},
  file = {/Users/nobr/Zotero/storage/4HJRPS76/AD0628956.pdf}
}

@article{forney1973,
  title = {The Viterbi Algorithm},
  author = {Forney, G.D.},
  year = {1973},
  month = mar,
  journal = {Proceedings of the IEEE},
  volume = {61},
  number = {3},
  pages = {268--278},
  issn = {1558-2256},
  doi = {10.1109/PROC.1973.9030},
  urldate = {2023-12-11},
  abstract = {The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.},
  file = {/Users/nobr/Zotero/storage/DXARCDCV/Forney - 1973 - The viterbi algorithm.pdf;/Users/nobr/Zotero/storage/ERS5P63Q/1450960.html}
}

@book{forsyth2012,
  title = {Computer Vision: A Modern Approach},
  shorttitle = {Computer Vision},
  author = {Forsyth, David and Ponce, Jean},
  year = {2012},
  edition = {2nd ed},
  publisher = {Pearson},
  address = {Boston},
  isbn = {978-0-13-608592-8},
  langid = {english},
  lccn = {TA1634 .F65 2012},
  keywords = {Computer vision,Problems exercises etc},
  file = {/Users/nobr/Zotero/storage/V87XSXLG/Forsyth and Ponce - 2012 - Computer vision a modern approach.pdf}
}

@misc{frankle2019,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019},
  month = mar,
  number = {arXiv:1803.03635},
  eprint = {1803.03635},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.03635},
  urldate = {2024-11-06},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/7N5FD6XU/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Trainable Neural Networks.pdf;/Users/nobr/Zotero/storage/E6TT5L7L/1803.html}
}

@misc{freemanBraxDifferentiablePhysics2021,
  title = {Brax -- {{A Differentiable Physics Engine}} for {{Large Scale Rigid Body Simulation}}},
  author = {Freeman, C. Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  year = {2021},
  month = jun,
  number = {arXiv:2106.13281},
  eprint = {2106.13281},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.13281},
  urldate = {2023-10-13},
  abstract = {We present Brax, an open source library for rigid body simulation with a focus on performance and parallelism on accelerators, written in JAX. We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine. Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators. Finally, we include notebooks that facilitate training of performant policies on common OpenAI Gym MuJoCo-like tasks in minutes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/V49WZ98K/Freeman et al. - 2021 - Brax -- A Differentiable Physics Engine for Large Scale Rigid Body Simulation.pdf;/Users/nobr/Zotero/storage/ZG7K7Y5Y/2106.html}
}

@book{friedman2018,
  title = {The Little Typer},
  author = {Friedman, Daniel P. and Christiansen, David Thrane and Harper, Robert and McBride, Conor and Bibby, Duane},
  year = {2018},
  publisher = {The MIT Press},
  address = {Cambridge, MA London},
  isbn = {978-0-262-53643-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DWSAYAVU/Friedman et al. - 2018 - The little typer.pdf}
}

@misc{friedrich2014history,
  title = {The History of Carbon Dioxide Emissions},
  author = {Friedrich, Johannes and Damassa, Thomas},
  year = {2014},
  month = may,
  howpublished = {https://www.wri.org/blog/2014/05/history-carbon-dioxide-emissions}
}

@article{frostig2018,
  title = {Compiling Machine Learning Programs via High-Level Tracing},
  author = {Frostig, Roy and Johnson, Matthew James and Leary, Chris},
  year = {2018},
  abstract = {We describe JAX, a domain-specific tracing JIT compiler for generating high-performance accelerator code from pure Python and Numpy machine learning programs. JAX uses the XLA compiler infrastructure to generate optimized code for the program subroutines that are most favorable for acceleration, and these optimized subroutines can be called and orchestrated by arbitrary Python. Because the system is fully compatible with Autograd, it allows forward- and reverse-mode automatic differentiation of Python functions to arbitrary order. Because JAX supports structured control flow, it can generate code for sophisticated machine learning algorithms while maintaining high performance. We show that by combining JAX with Autograd and Numpy we get an easily programmable and highly performant ML system that targets CPUs, GPUs, and TPUs, capable of scaling to multi-core Cloud TPUs.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LZQCR9ZJ/Frostig et al. - Compiling machine learning programs via high-level tracing.pdf}
}

@misc{fuD4RLDatasetsDeep2021,
  title = {{{D4RL}}: {{Datasets}} for {{Deep Data-Driven Reinforcement Learning}}},
  shorttitle = {{{D4RL}}},
  author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  year = {2021},
  month = feb,
  number = {arXiv:2004.07219},
  eprint = {2004.07219},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.07219},
  urldate = {2023-12-05},
  abstract = {The offline reinforcement learning (RL) setting (also known as full batch RL), where a policy is learned from a static dataset, is compelling as progress enables RL methods to take advantage of large, previously-collected datasets, much like how the rise of large datasets has fueled results in supervised learning. However, existing online RL benchmarks are not tailored towards the offline setting and existing offline RL benchmarks are restricted to data generated by partially-trained agents, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. With a focus on dataset collection, examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multitask datasets where an agent performs different tasks in the same environment, and datasets collected with mixtures of policies. By moving beyond simple benchmark tasks and data collected by partially-trained RL agents, we reveal important and unappreciated deficiencies of existing algorithms. To facilitate research, we have released our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms, an evaluation protocol, and open-source examples. This serves as a common starting point for the community to identify shortcomings in existing offline RL methods and a collaborative route for progress in this emerging area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/2LJDYZBR/Fu et al. - 2021 - D4RL Datasets for Deep Data-Driven Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/2LQETE28/2004.html}
}

@article{fuller2020,
  title = {Digital {{Twin}}: {{Enabling Technologies}}, {{Challenges}} and {{Open Research}}},
  shorttitle = {Digital {{Twin}}},
  author = {Fuller, Aidan and Fan, Zhong and Day, Charles and Barlow, Chris},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {108952--108971},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2998358},
  urldate = {2023-12-08},
  abstract = {Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TUWURXGR/Fuller et al. - 2020 - Digital Twin Enabling Technologies, Challenges and Open Research.pdf}
}

@article{gal2020,
  title = {Numeracy, Adult Education, and Vulnerable Adults: A Critical View of a Neglected Field},
  shorttitle = {Numeracy, Adult Education, and Vulnerable Adults},
  author = {Gal, Iddo and Grotl{\"u}schen, Anke and Tout, Dave and Kaiser, Gabriele},
  year = {2020},
  journal = {Zdm},
  volume = {52},
  number = {3},
  pages = {377--394},
  issn = {1863-9690},
  doi = {10.1007/s11858-020-01155-9},
  urldate = {2024-12-25},
  abstract = {This survey paper examines selected issues related to the intersection of three broad scholarly areas: numeracy, adult education, and vulnerability. Numeracy encompasses the ways in which people cope with the mathematical, quantitative, and statistical demands of adult life, and is viewed as an important outcome of schooling and as a foundational skill for all adults. The focus on vulnerability stems from the realization that concerns of policy makers and educators alike often center on populations seen as vulnerable. The paper is organized in five sections. After a brief introduction, Section 2 examines adult numeracy, focusing on five numeracy domains (health, financial, digital, civic, and workplace numeracy), literacy--numeracy linkages, functional and critical aspects of numeracy, and the centrality of numeracy practices, and notes sources of vulnerability for each of these. Section~3 sketches formal, non-formal and informal contexts in which adults learn or develop their numeracy, and examines factors that may be potential sources of vulnerability, including systemic factors and dispositional and affect factors. Section~4 reflects more broadly on the concept of vulnerability, introduces selected aspects of the papers published in this issue of ZDM Mathematics Education, and points to findings regarding adult learners who may be deemed vulnerable. The closing section summarizes conclusions and research directions regarding the intersection of the three core domains. Overall, the paper points to emerging research needs and educational challenges that are relevant to scholars, practitioners, and policy makers interested in developing the numeracy of adults as well as in the mathematics education of younger learners.},
  pmcid = {PMC7131969},
  pmid = {32292526},
  file = {/Users/nobr/Zotero/storage/RA7ULUV6/Gal et al. - 2020 - Numeracy, adult education, and vulnerable adults a critical view of a neglected field.pdf}
}

@article{galloway1995,
  title = {Mr. {{Mark H}}. {{Waters Thermosciences Institute}}, {{ELORET Corp}}. {{Palo Alto}}, {{CA}}},
  author = {Galloway, L and Phillips, A and Kennelly, A},
  year = {1995},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6E4GPK3M/Galloway et al. - Mr. Mark H. Waters Thermosciences Institute, ELORET Corp. Palo Alto, CA.pdf}
}

@article{gandy2018,
  title = {Cities in Deep Time: {{Bio-diversity}}, Metabolic Rift, and the Urban Question},
  shorttitle = {Cities in Deep Time},
  author = {Gandy, Matthew},
  year = {2018},
  month = jan,
  journal = {City},
  volume = {22},
  number = {1},
  pages = {96--105},
  issn = {1360-4813, 1470-3629},
  doi = {10.1080/13604813.2018.1434289},
  urldate = {2023-11-09},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HZZGXUT2/Gandy - 2018 - Cities in deep time Bio-diversity, metabolic rift, and the urban question.pdf}
}

@article{ganzfried2013,
  title = {Game {{Theory-Based Opponent Modeling}} in {{Large Imperfect-Information Games}}},
  author = {Ganzfried, Sam and Sandholm, Tuomas},
  year = {2013},
  abstract = {We develop an algorithm for opponent modeling in large extensive-form games of imperfect information. It works by observing the opponent's action frequencies and building an opponent model by combining information from a precomputed equilibrium strategy with the observations. It then computes and plays a best response to this opponent model; the opponent model and best response are both updated continually in real time. The approach combines gametheoretic reasoning and pure opponent modeling, yielding a hybrid that can effectively exploit opponents after only a small number of interactions. Unlike prior opponent modeling approaches, ours is fundamentally game theoretic and takes advantage of recent algorithms for automated abstraction and equilibrium computation rather than relying on domain-specific prior distributions, historical data, or a handcrafted set of features. Experiments show that our algorithm leads to significantly higher win rates (than an approximateequilibrium strategy) against several opponents in limit Texas Hold'em --- the most studied imperfect-information game in computer science --- including competitors from recent AAAI computer poker competitions.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/EJM4XSA5/Ganzfried and Sandholm - Game Theory-Based Opponent Modeling in Large Imperfect-Information Games.pdf}
}

@article{gao,
  title = {Scaling and Evaluating Sparse Autoencoders},
  author = {Gao, Leo and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3TXU4F23/sparse-autoencoders.pdf}
}

@article{garcia-sanchez2019,
  title = {Georgios {{N}}. {{Yannakakis}} and {{Julian Togelius}}: {{Artificial Intelligence}} and {{Games}}: {{Springer}}, 2018, {{Print ISBN}}: 978-3-319-63518-7, {{Online ISBN}}: 978-3-319-63519-4, {{https://doi.org/10.1007/978-3-319-63519-4}}},
  shorttitle = {Georgios {{N}}. {{Yannakakis}} and {{Julian Togelius}}},
  author = {{Garc{\'i}a-S{\'a}nchez}, Pablo},
  year = {2019},
  month = mar,
  journal = {Genetic Programming and Evolvable Machines},
  volume = {20},
  number = {1},
  pages = {143--145},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-018-9337-0},
  urldate = {2023-11-09},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SY9UUQH2/García-Sánchez - 2019 - Georgios N. Yannakakis and Julian Togelius Artificial Intelligence and Games Springer, 2018, Print.pdf}
}

@article{garcia-sanchez2020,
  title = {Optimizing {{Hearthstone}} Agents Using an Evolutionary Algorithm},
  author = {{Garc{\'i}a-S{\'a}nchez}, Pablo and Tonda, Alberto and {Fern{\'a}ndez-Leiva}, Antonio J. and Cotta, Carlos},
  year = {2020},
  month = jan,
  journal = {Knowledge-Based Systems},
  volume = {188},
  pages = {105032},
  issn = {09507051},
  doi = {10.1016/j.knosys.2019.105032},
  urldate = {2023-11-12},
  abstract = {Digital collectible card games are not only a growing part of the video game industry, but also an interesting research area for the field of computational intelligence. This game genre allows researchers to deal with hidden information, uncertainty and planning, among other aspects. This paper proposes the use of evolutionary algorithms (EAs) to develop agents who play a card game, Hearthstone, by optimizing a data-driven decision-making mechanism that takes into account all the elements currently in play. Agents feature self-learning by means of a competitive coevolutionary training approach, whereby no external sparring element defined by the user is required for the optimization process. One of the agents developed through the proposed approach was runner-up (best 6\%) in an international Hearthstone Artificial Intelligence (AI) competition. Our proposal performed remarkably well, even when it faced state-of-the-art techniques that attempted to take into account future game states, such as Monte-Carlo Tree search. This outcome shows how evolutionary computation could represent a considerable advantage in developing AIs for collectible card games such as Hearthstone. {\copyright} 2019 Elsevier B.V. All rights reserved.},
  langid = {english}
}

@misc{gavranovic2024,
  title = {Categorical {{Deep Learning}}: {{An Algebraic Theory}} of {{Architectures}}},
  shorttitle = {Categorical {{Deep Learning}}},
  author = {Gavranovi{\'c}, Bruno and Lessard, Paul and Dudzik, Andrew and {von Glehn}, Tamara and Ara{\'u}jo, Jo{\~a}o G. M. and Veli{\v c}kovi{\'c}, Petar},
  year = {2024},
  month = feb,
  number = {arXiv:2402.15332},
  eprint = {2402.15332},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.15332},
  urldate = {2024-04-01},
  abstract = {We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Category Theory,Mathematics - Rings and Algebras,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/26LZNN3Z/Gavranović et al. - 2024 - Categorical Deep Learning An Algebraic Theory of Architectures.pdf;/Users/nobr/Zotero/storage/S2VAWHK6/2402.html}
}

@misc{gemmateamGemmaOpenModels2024,
  title = {Gemma: {{Open Models Based}} on {{Gemini Research}} and {{Technology}}},
  shorttitle = {Gemma},
  author = {Gemma Team and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and Hussenot, L{\'e}onard and Sessa, Pier Giuseppe and Chowdhery, Aakanksha and Roberts, Adam and Barua, Aditya and Botev, Alex and {Castro-Ros}, Alex and Slone, Ambrose and H{\'e}liou, Am{\'e}lie and Tacchetti, Andrea and Bulanova, Anna and Paterson, Antonia and Tsai, Beth and Shahriari, Bobak and Lan, Charline Le and {Choquette-Choo}, Christopher A. and Crepy, Cl{\'e}ment and Cer, Daniel and Ippolito, Daphne and Reid, David and Buchatskaya, Elena and Ni, Eric and Noland, Eric and Yan, Geng and Tucker, George and Muraru, George-Christian and Rozhdestvenskiy, Grigory and Michalewski, Henryk and Tenney, Ian and Grishchenko, Ivan and Austin, Jacob and Keeling, James and Labanowski, Jane and Lespiau, Jean-Baptiste and Stanway, Jeff and Brennan, Jenny and Chen, Jeremy and Ferret, Johan and Chiu, Justin and {Mao-Jones}, Justin and Lee, Katherine and Yu, Kathy and Millican, Katie and Sjoesund, Lars Lowe and Lee, Lisa and Dixon, Lucas and Reid, Machel and Miku{\l}a, Maciej and Wirth, Mateo and Sharman, Michael and Chinaev, Nikolai and Thain, Nithum and Bachem, Olivier and Chang, Oscar and Wahltinez, Oscar and Bailey, Paige and Michel, Paul and Yotov, Petko and Chaabouni, Rahma and Comanescu, Ramona and Jana, Reena and Anil, Rohan and McIlroy, Ross and Liu, Ruibo and Mullins, Ryan and Smith, Samuel L. and Borgeaud, Sebastian and Girgin, Sertan and Douglas, Sholto and Pandya, Shree and Shakeri, Siamak and De, Soham and Klimenko, Ted and Hennigan, Tom and Feinberg, Vlad and Stokowiec, Wojciech and Chen, Yu-hui and Ahmed, Zafarali and Gong, Zhitao and Warkentin, Tris and Peran, Ludovic and Giang, Minh and Farabet, Cl{\'e}ment and Vinyals, Oriol and Dean, Jeff and Kavukcuoglu, Koray and Hassabis, Demis and Ghahramani, Zoubin and Eck, Douglas and Barral, Joelle and Pereira, Fernando and Collins, Eli and Joulin, Armand and Fiedel, Noah and Senter, Evan and Andreev, Alek and Kenealy, Kathleen},
  year = {2024},
  month = apr,
  number = {arXiv:2403.08295},
  eprint = {2403.08295},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.08295},
  urldate = {2024-05-23},
  abstract = {This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/M28INB8M/Gemma Team et al. - 2024 - Gemma Open Models Based on Gemini Research and Technology.pdf;/Users/nobr/Zotero/storage/LJUNSIHU/2403.html}
}

@book{gersho1992,
  title = {Vector {{Quantization}} and {{Signal Compression}}},
  author = {Gersho, Allen and Gray, Robert M.},
  year = {1992},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4615-3626-0},
  urldate = {2023-08-28},
  isbn = {978-1-4613-6612-6 978-1-4615-3626-0},
  keywords = {analog,coding,complexity,computer,data compression,entropy,information,Jitter,Modulation,Signal,simulation},
  file = {/Users/nobr/Zotero/storage/D9HYA2PI/Gersho and Gray - 1992 - Vector Quantization and Signal Compression.pdf}
}

@misc{geshkovskiMathematicalPerspectiveTransformers2023,
  title = {A Mathematical Perspective on {{Transformers}}},
  author = {Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10794},
  eprint = {2312.10794},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2023-12-21},
  abstract = {Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Analysis of PDEs,Mathematics - Dynamical Systems},
  file = {/Users/nobr/Zotero/storage/XH4AN8LR/Geshkovski et al. - 2023 - A mathematical perspective on Transformers.pdf}
}

@book{ghallab2016,
  title = {Automated {{Planning}} and {{Acting}}},
  author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
  year = {2016},
  month = jul,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9781139583923},
  urldate = {2023-12-21},
  abstract = {Autonomous AI systems need complex computational techniques for planning and performing actions. Planning and acting require significant deliberation because an intelligent system must coordinate and integrate these activities in order to act effectively in the real world. This book presents a comprehensive paradigm of planning and acting using the most recent and advanced automated-planning techniques. It explains the computational deliberation capabilities that allow an actor, whether physical or virtual, to reason about its actions, choose them, organize them purposefully, and act deliberately to achieve an objective. Useful for students, practitioners, and researchers, this book covers state-of-the-art planning techniques, acting techniques, and their integration which will allow readers to design intelligent systems that are able to act effectively in the real world.},
  isbn = {978-1-107-03727-4 978-1-139-58392-3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RY5AW8AL/Ghallab et al. - 2016 - Automated Planning and Acting.pdf}
}

@incollection{giavitto2002,
  title = {Data {{Structure}} as {{Topological Spaces}}},
  booktitle = {Unconventional {{Models}} of {{Computation}}},
  author = {Giavitto, Jean-Louis and Michel, Olivier},
  editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan},
  year = {2002},
  volume = {2509},
  pages = {137--150},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45833-6_12},
  urldate = {2024-02-17},
  abstract = {In this paper, we propose a topological metaphor for computations: computing consists in moving through a path in a data space and making some elementary computations along this path. This idea underlies an experimental declarative programming language called mgs. mgs introduces the notion of topological collection: a set of values organized by a neighborhood relationship. The basic computation step in mgs relies on the notion of path : a path C is substituted for a path B in a topological collection A. This step is called a transformation and several features are proposed to control the transformation applications. By changing the topological structure of the collection, the underlying computational model is changed. Thus, mgs enables a unified view on several computational mechanisms. Some of them are initially inspired by biological or chemical processes (Gamma and the CHAM, Lindenmayer systems, Paun systems and cellular automata).},
  isbn = {978-3-540-44311-7 978-3-540-45833-3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XV5LDBPI/Giavitto and Michel - 2002 - Data Structure as Topological Spaces.pdf}
}

@article{gibbor2021,
  title = {Cognitive Stimulation Therapy ({{CST}}) for Dementia: A Systematic Review of Qualitative Research},
  shorttitle = {Cognitive Stimulation Therapy ({{CST}}) for Dementia},
  author = {Gibbor, Luke and Yates, Lauren and Volkmer, Anna and Spector, Aimee},
  year = {2021},
  month = jun,
  journal = {Aging \& Mental Health},
  volume = {25},
  number = {6},
  pages = {980--990},
  issn = {1360-7863, 1364-6915},
  doi = {10.1080/13607863.2020.1746741},
  urldate = {2023-11-06},
  abstract = {Introduction: Cognitive Stimulation Therapy (CST) is a well-established intervention for people with dementia shown to improve cognition and quality of life. Past research includes development of a longer term `maintenance CST' and an individual CST programme. Previous reviews of CST have focused on quantitative outcomes or excluded certain formats of CST. This review aimed to fill this gap by evaluating how the voices of facilitators, carers and people with dementia in qualitative studies of CST can contribute to our understanding of its implementation and how it is experienced. Methods: The current systematic review explored the experience and perspectives of people with dementia, facilitators and carers. Thematic Analysis was used to analyse this data, alongside guidance on synthesising qualitative findings. Results: A systematic literature search retrieved 10 relevant studies using qualitative methodology. Eighteen themes were generated, which were grouped into three categories: `Acceptability and feasibility', `Features of CST' and `Key outcomes'. Conclusions: To our knowledge, this is the only review to explore solely qualitative studies of CST. Findings provided insight into the shared features, outcomes and factors affecting implementation, and suggested theories for discrepancies between quantitative and qualitative findings in the literature. Some of the common themes were also in keeping with past reviews.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3DWEFGMD/Gibbor et al. - 2021 - Cognitive stimulation therapy (CST) for dementia a systematic review of qualitative research.pdf}
}

@article{gifford2022,
  title = {A Large and Rich {{EEG}} Dataset for Modeling Human Visual Object Recognition},
  author = {Gifford, Alessandro T. and Dwivedi, Kshitij and Roig, Gemma and Cichy, Radoslaw M.},
  year = {2022},
  month = dec,
  journal = {NeuroImage},
  volume = {264},
  publisher = {Academic Press Inc.},
  issn = {10959572},
  doi = {10.1016/J.NEUROIMAGE.2022.119754},
  urldate = {2023-03-13},
  abstract = {The human brain achieves visual object recognition through multiple stages of linear and nonlinear transformations operating at a millisecond scale. To predict and explain these rapid transformations, computational neuroscientists employ machine learning modeling techniques. However, state-of-the-art models require massive amounts of data to properly train, and to the present day there is a lack of vast brain datasets which extensively sample the temporal dynamics of visual object recognition. Here we collected a large and rich dataset of high temporal resolution EEG responses to images of objects on a natural background. This dataset includes 10 participants, each with 82,160 trials spanning 16,740 image conditions. Through computational modeling we established the quality of this dataset in five ways. First, we trained linearizing encoding models that successfully synthesized the EEG responses to arbitrary images. Second, we correctly identified the recorded EEG data image conditions in a zero-shot fashion, using EEG synthesized responses to hundreds of thousands of candidate image conditions. Third, we show that both the high number of conditions as well as the trial repetitions of the EEG dataset contribute to the trained models' prediction accuracy. Fourth, we built encoding models whose predictions well generalize to novel participants. Fifth, we demonstrate full end-to-end training of randomly initialized DNNs that output EEG responses for arbitrary input images. We release this dataset as a tool to foster research in visual neuroscience and computer vision.},
  pmid = {36400378},
  keywords = {Artificial neural networks,Computational neuroscience,Electroencephalography,Neural encoding models,Open-access data resource,Visual object recognition},
  file = {/Users/nobr/Zotero/storage/TKWND77A/Gifford et al. - 2022 - A large and rich EEG dataset for modeling human visual object recognition.pdf}
}

@misc{gifford2023,
  title = {The {{Algonauts Project}} 2023 {{Challenge}}: {{How}} the {{Human Brain Makes Sense}} of {{Natural Scenes}}},
  shorttitle = {The {{Algonauts Project}} 2023 {{Challenge}}},
  author = {Gifford, A. T. and Lahner, B. and {Saba-Sadiya}, S. and Vilas, M. G. and Lascelles, A. and Oliva, A. and Kay, K. and Roig, G. and Cichy, R. M.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.03198},
  eprint = {2301.03198},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {The sciences of biological and artificial intelligence are ever more intertwined. Neural computational principles inspire new intelligent machines, which are in turn used to advance theoretical understanding of the brain. To promote further exchange of ideas and collaboration between biological and artificial intelligence researchers, we introduce the 2023 installment of the Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes (http://algonauts.csail.mit.edu). This installment prompts the fields of artificial and biological intelligence to come together towards building computational models of the visual brain using the largest and richest dataset of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD provides high-quality fMRI responses to {\textasciitilde}73,000 different naturalistic colored scenes, making it the ideal candidate for data-driven model building approaches promoted by the 2023 challenge. The challenge is open to all and makes results directly comparable and transparent through a public leaderboard automatically updated after each submission, thus allowing for rapid model development. We believe that the 2023 installment will spark symbiotic collaborations between biological and artificial intelligence scientists, leading to a deeper understanding of the brain through cutting-edge computational models and to novel ways of engineering artificial intelligent agents through inductive biases from biological systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/DC9DP6XB/Gifford et al. - 2023 - The Algonauts Project 2023 Challenge How the Huma.pdf;/Users/nobr/Zotero/storage/VNW58B9D/2301.html}
}

@article{gigovic2017,
  title = {Application of the {{GIS-DANP-MABAC}} Multi-Criteria Model for Selecting the Location of Wind Farms: {{A}} Case Study of {{Vojvodina}}, {{Serbia}}},
  shorttitle = {Application of the {{GIS-DANP-MABAC}} Multi-Criteria Model for Selecting the Location of Wind Farms},
  author = {Gigovi{\'c}, Ljubomir and Pamu{\v c}ar, Dragan and Bo{\v z}ani{\'c}, Darko and Ljubojevi{\'c}, Sr{\dj}an},
  year = {2017},
  month = apr,
  journal = {Renewable Energy},
  volume = {103},
  pages = {501--521},
  issn = {09601481},
  doi = {10.1016/j.renene.2016.11.057},
  urldate = {2023-07-13},
  abstract = {The main objective of this study is to develop a reliable model for the identification of locations for the installation of wind farms, which will provide significant support to planners in the strategy for the development and management of wind energy. The proposed model is based on the combined application of Geographic Information Systems (GIS) and Multi-Criteria Decision Analysis (MCDA) using the multi-criteria technique of Decision Making Trial and Evaluation Laboratory (DEMATEL), the Analytic Network Process (ANP) and Multi-Attributive Border Approximation area Comparison (MABAC). Application of the model is presented by means of a case study on the province of Vojvodina, Serbia. The model considers 11 constraints and 11 evaluation criteria which are grouped into economic, social and environmental clusters. The DEMATEL-ANP (DANP) method is used to determine the weight coefficients of the evaluation criteria, and the MABAC method is used to rank the selected viable locations. The final map of benefits is presented using raster cells (alternatives) which are evaluated in the range of 1 (least suitable) to 7 (most suitable). The results show that an area of 321 km2 in Vojvodina is very suitable for the installation of wind farms. Ranking of viable locations using the MABAC method shows that a location in the vicinity of the village of Laudonovac (L8) is most suitable for the installation of wind farms in the province of Vojvodina. A sensitivity analysis, carried out by changing the input weights of the clusters, indicates that the model is useful for identifying suitable locations for the development of wind farm projects, as well as for assessing the suitability of already licensed projects for the construction of wind farms. The proposed method and the results of this study can be used for spatial development policy at all levels of public administration related to renewable energy resources. The model could also help to successfully identify suitable locations for the installation of wind farms in other areas with similar geographical conditions.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZRTKXC5W/Gigović et al. - 2017 - Application of the GIS-DANP-MABAC multi-criteria m.pdf}
}

@inproceedings{gilbert2023,
  title = {Semantic {{Compression}} with {{Large Language Models}}},
  booktitle = {2023 {{Tenth International Conference}} on {{Social Networks Analysis}}, {{Management}} and {{Security}} ({{SNAMS}})},
  author = {Gilbert, Henry and Sandborn, Michael and Schmidt, Douglas C. and {Spencer-Smith}, Jesse and White, Jules},
  year = {2023},
  month = nov,
  pages = {1--8},
  publisher = {IEEE},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.1109/SNAMS60348.2023.10375400},
  urldate = {2024-11-20},
  abstract = {The rise of large language models (LLMs) is revolutionizing information retrieval, question answering, summarization, and code generation tasks. However, in addition to confidently presenting factually inaccurate information at times (known as ``hallucinations''), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information. A common approach to reducing the size of data is through lossless or lossy compression. Yet, in some cases it may not be strictly necessary to perfectly recover every detail from the original data, as long as a requisite level of semantic precision or intent is conveyed.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-1890-6},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/FNBPGTDH/Gilbert et al. - 2023 - Semantic Compression with Large Language Models.pdf}
}

@article{gillin1951,
  title = {Homo {{Ludens}}: {{A Study}} of the {{Play-Element}} in {{Culture}}.},
  shorttitle = {Homo {{Ludens}}},
  author = {Gillin, John L. and Huizinga, J.},
  year = {1951},
  month = apr,
  journal = {American Sociological Review},
  volume = {16},
  number = {2},
  eprint = {2087716},
  eprinttype = {jstor},
  pages = {274},
  issn = {00031224},
  doi = {10.2307/2087716},
  urldate = {2023-11-09},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/36LE546B/Gillin and Huizinga - 1951 - Homo Ludens A Study of the Play-Element in Culture..pdf}
}

@misc{global_carbon_atlas,
  title = {Global Carbon Atlas - {{CO2}} Emissions},
  howpublished = {http://www.globalcarbonatlas.org/}
}

@misc{global_coal_tracker,
  title = {Global Coal Plant Tracker}
}

@misc{globalcarbonproject2018,
  title = {Global Carbon Budget Archive, 2018},
  year = {2018},
  howpublished = {http://www.globalcarbonproject.org/}
}

@misc{globo2018petrobras,
  title = {Since the Strike Began, {{Petrobras}} Has Already Lost {{R}}\$126 Billion in Market Value, Says {{Economatica}}},
  year = {2018},
  month = may
}

@article{glorot,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SATXCVUV/Glorot et al. - Deep Sparse Rectiﬁer Neural Networks.pdf}
}

@misc{goecks2021,
  title = {On Games and Simulators as a Platform for Development of Artificial Intelligence for Command and Control},
  author = {Goecks, Vinicius G. and Waytowich, Nicholas and Asher, Derrik E. and Park, Song Jun and Mittrick, Mark and Richardson, John and Vindiola, Manuel and Logie, Anne and Dennison, Mark and Trout, Theron and Narayanan, Priya and Kott, Alexander},
  year = {2021},
  month = oct,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Games and simulators can be a valuable platform to execute complex multi-agent, multiplayer, imperfect information scenarios with significant parallels to military applications: multiple participants manage resources and make decisions that command assets to secure specific areas of a map or neutralize opposing forces. These characteristics have attracted the artificial intelligence (AI) community by supporting development of algorithms with complex benchmarks and the capability to rapidly iterate over new ideas. The success of artificial intelligence algorithms in real-time strategy games such as StarCraft II have also attracted the attention of the military research community aiming to explore similar techniques in military counterpart scenarios. Aiming to bridge the connection between games and military applications, this work discusses past and current efforts on how games and simulators, together with the artificial intelligence algorithms, have been adapted to simulate certain aspects of military missions and how they might impact the future battlefield. This paper also investigates how advances in virtual reality and visual augmentation systems open new possibilities in human interfaces with gaming platforms and their military parallels.},
  langid = {english},
  keywords = {A.1,Computer Science - Machine Learning,Computer Science - Multiagent Systems,I.2.6,I.6.3},
  file = {/Users/nobr/Zotero/storage/699F3ZDX/Goecks et al. - 2021 - On games and simulators as a platform for development of artificial intelligence for command and con.pdf}
}

@misc{goelCanTransformerRepresent2023,
  title = {Can a {{Transformer Represent}} a {{Kalman Filter}}?},
  author = {Goel, Gautam and Bartlett, Peter},
  year = {2023},
  month = dec,
  number = {arXiv:2312.06937},
  eprint = {2312.06937},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2312.06937},
  urldate = {2023-12-13},
  abstract = {Transformers are a class of autoregressive deep learning architectures which have recently achieved state-of-the-art performance in various vision, language, and robotics tasks. We revisit the problem of Kalman Filtering in linear dynamical systems and show that Transformers can approximate the Kalman Filter in a strong sense. Specifically, for any observable LTI system we construct an explicit causally-masked Transformer which implements the Kalman Filter, up to a small additive error which is bounded uniformly in time; we call our construction the Transformer Filter. Our construction is based on a two-step reduction. We first show that a softmax self-attention block can exactly represent a certain Gaussian kernel smoothing estimator. We then show that this estimator closely approximates the Kalman Filter. We also investigate how the Transformer Filter can be used for measurement-feedback control and prove that the resulting nonlinear controllers closely approximate the performance of standard optimal control policies such as the LQG controller.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/K5SXJZG8/Goel and Bartlett - 2023 - Can a Transformer Represent a Kalman Filter.pdf}
}

@article{goh2021,
  title = {Multimodal {{Neurons}} in {{Artificial Neural Networks}}},
  author = {Goh, Gabriel and Cammarata, Nick and Voss, Chelsea and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  year = {2021},
  month = mar,
  journal = {Distill},
  volume = {6},
  number = {3},
  pages = {10.23915/distill.00030},
  issn = {2476-0757},
  doi = {10.23915/distill.00030},
  urldate = {2024-06-16}
}

@misc{goldfarb2008,
  title = {Goldfarb\_{{NotesonMetamath}}.Pdf},
  author = {Goldfarb, Warren},
  year = {2008},
  urldate = {2023-10-21},
  howpublished = {https://people.math.harvard.edu/{\textasciitilde}wboney/spring19/Goldfarb\_NotesonMetamath.pdf},
  file = {/Users/nobr/Zotero/storage/8IYI3LYY/Goldfarb_NotesonMetamath.pdf}
}

@article{goldie2020,
  title = {Placement {{Optimization}} with {{Deep Reinforcement Learning}}},
  author = {Goldie, Anna and Mirhoseini, Azalia},
  year = {2020},
  pages = {3--7},
  doi = {10.1145/3372780.3378174},
  file = {/Users/nobr/Zotero/storage/U5B8T8SX/document.pdf}
}

@inproceedings{goldsztajnfarelo2022,
  title = {An Adaptive Learning Model for Predicting and Analyzing Student Performance on Flight Training Tasks},
  booktitle = {Signal {{Processing}}, {{Sensor}}/{{Information Fusion}}, and {{Target Recognition XXXI}}},
  author = {Goldsztajn Farelo, David and Bracaglia, Leshya and Dailey, Peter and Tamrakar, Shailesh and Palladino, Anthony and Carroll, Meredith and Valenti, Andrew},
  editor = {Grewe, Lynne L. and Blasch, Erik P. and Kadar, Ivan},
  year = {2022},
  month = jun,
  pages = {36},
  publisher = {SPIE},
  address = {Orlando, United States},
  doi = {10.1117/12.2619068},
  urldate = {2024-02-15},
  abstract = {As complexity and diversity of military assets increase, ensuring that military forces are well-trained becomes complex and costly when relying on traditional classroom instruction. Human instructors bear the burden of manually creating training datasets with tools that are often not geared to their missions, tasks, and objectives. Further, analyzing how well students learned their tasks often requires collecting and managing student performance over time, which may not be feasible in time-critical situations, and may consume instructor time and attention that could be spent facilitating learning. In addition, while one-to-one human tutoring has proven to be effective, it is costly and impractical to provide in every task domain. We present Multi-task Adaptive Learning Tutor (MALT), a concept for an intelligent tutoring system (ITS) for the psychomotor domain that flexibly responds to a user's current tasking, information needs, and cognitive ability to interpret information. As the user performs a series of complex psychomotor sub-tasks drawn from flight procedures implemented in a simulator, MALT will learn to predict which features contributed most to their performance. In a proof-of-concept study, we trained MALT using data collected from pilots, ranging from new student pilots to Certified Flight Instructors, while performing different flight procedures. This paper presents the MALT concept, and methods and results associated with the proof-of-concept study focused on MALT's diagnostic capability. We believe MALT to be among the first to expand the ITS beyond traditional cognitive tasks such as problem solving to include complex psychomotor tasks.},
  isbn = {978-1-5106-5120-3 978-1-5106-5121-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/N7CEX6HT/Goldsztajn Farelo et al. - 2022 - An adaptive learning model for predicting and analyzing student performance on flight training tasks.pdf}
}

@article{gonzalez2008,
  title = {Understanding Individual Human Mobility Patterns},
  author = {Gonz{\'a}lez, Marta C. and Hidalgo, C{\'e}sar A. and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2008},
  month = jun,
  journal = {Nature},
  volume = {453},
  number = {7196},
  pages = {779--782},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06958},
  urldate = {2023-07-10},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SSH9JY6K/González et al. - 2008 - Understanding individual human mobility patterns.pdf}
}

@article{gonzalez2017,
  title = {Our {{Struggle Is My Struggle}}: {{Solidarity Feminism}} as an {{Intersectional Reply}} to {{Neoliberal}} and {{Choice Feminism}}},
  author = {Gonzalez, Maria and Jones, Lisa A},
  year = {2017},
  abstract = {The women's movement has long been pluralistic, yet in recent decades has diversified further along lines of individual choice versus collective action. This has been enabled by new opportunities for women that were not universally accessible. As a result, a form of ``choice feminism'' has developed in some feminists, espe cially in contexts in which neoliberalism is dominant, while calls for intersectional allyship, inclusion, and solidarity have grown louder in others. Responding to this tension, many scholars, particularly those within the field of social work, have shown that choice feminism is characterized by a number of problematic themes that can, paradoxically, reinforce oppression for marginalized people. Particularly, it can offer a heuristic of choice that is used to justify feminist decisions that benefit a small set of women at the expense of standing in solidarity with others and remediating oppression. This complex problem may benefit from a detailed interrogation of allyship and its attendant repercussions. Consequently, this paper forwards a framework for solidarityfeminism---that is, an approach to feminism that centers solidarity against oppression---by favoring inclusive values-based allyship over choice feminism as an intersectional means to address one aspect of the enduring universalism problem within the feminist movement.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/8RIJ53VW/Gonzalez and Jones - Our Struggle Is My Struggle Solidarity Feminism as an Intersectional Reply to Neoliberal and Choice.pdf}
}

@misc{goodfellow2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.2661},
  urldate = {2024-11-21},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/5J6FFPYM/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/Users/nobr/Zotero/storage/MYU8U8C2/1406.html}
}

@misc{goodman2020,
  title = {{{AI}} and {{Wargaming}}},
  author = {Goodman, James and Risi, Sebastian and Lucas, Simon},
  year = {2020},
  month = sep,
  number = {arXiv:2009.08922},
  eprint = {2009.08922},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2009.08922},
  urldate = {2023-11-05},
  abstract = {Recent progress in Game AI has demonstrated that given enough data from human gameplay, or experience gained via simulations, machines can rival or surpass the most skilled human players in classic games such as Go, or commercial computer games such as Starcraft. We review the current state-of-the-art through the lens of wargaming, and ask firstly what features of wargames distinguish them from the usual AI testbeds, and secondly which recent AI advances are best suited to address these wargame-specific features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/UMY2XUPG/Goodman et al. - 2020 - AI and Wargaming.pdf}
}

@misc{goodwork2025,
  title = {What Does {{Palantir}} Actually Do?},
  author = {{Good Work}},
  year = {2025},
  month = mar,
  urldate = {2025-05-12}
}

@article{gorshkov2022,
  title = {Scientific {{Applications}} of {{Distributed Acoustic Sensing}}: {{State-of-the-Art Review}} and {{Perspective}}},
  shorttitle = {Scientific {{Applications}} of {{Distributed Acoustic Sensing}}},
  author = {Gorshkov, Boris G. and Y{\"u}ksel, Kivilcim and Fotiadi, Andrei A. and Wuilpart, Marc and Korobko, Dmitry A. and Zhirnov, Andrey A. and Stepanov, Konstantin V. and Turov, Artem T. and Konstantinov, Yuri A. and Lobach, Ivan A.},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {3},
  pages = {1033},
  issn = {1424-8220},
  doi = {10.3390/s22031033},
  urldate = {2023-12-08},
  abstract = {This work presents a detailed review of the development of distributed acoustic sensors (DAS) and their newest scientific applications. It covers most areas of human activities, such as the engineering, material, and humanitarian sciences, geophysics, culture, biology, and applied mechanics. It also provides the theoretical basis for most well-known DAS techniques and unveils the features that characterize each particular group of applications. After providing a summary of research achievements, the paper develops an initial perspective of the future work and determines the most promising DAS technologies that should be improved.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZU5DJ9QS/Gorshkov et al. - 2022 - Scientific Applications of Distributed Acoustic Sensing State-of-the-Art Review and Perspective.pdf}
}

@techreport{graham2012,
  title = {Moral {{Foundations Theory}} 1 {{Running}} Head: {{MORAL FOUNDATIONS THEORY Moral Foundations Theory}}: {{The Pragmatic Validity}} of {{Moral Pluralism}}},
  author = {Graham, Jesse and Haidt, Jonathan and Koleva, Sena and Motyl, Matt and Iyer, Ravi and Wojcik, Sean P and Ditto, Peter H},
  year = {2012},
  abstract = {Where does morality come from? Why are moral judgments often so similar across cultures, yet sometimes so variable? Is morality one thing, or many? Moral Foundations Theory (MFT) was created to answer these questions. In this chapter we describe the origins, assumptions, and current conceptualization of the theory, and detail the empirical findings that MFT has made possible, both within social psychology and beyond. Looking toward the future, we embrace several critiques of the theory, and specify five criteria for determining what should be considered a foundation of human morality. Finally, we suggest a variety of future directions for MFT and for moral psychology.},
  file = {/Users/nobr/Zotero/storage/XFYPDHR8/Graham et al. - 2012 - Moral Foundations Theory 1 Running head MORAL FOUNDATIONS THEORY Moral Foundations Theory The Prag.pdf}
}

@misc{graves2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = dec,
  number = {arXiv:1410.5401},
  eprint = {1410.5401},
  doi = {10.48550/arXiv.1410.5401},
  urldate = {2024-11-20},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/HZ7S74XC/Graves et al. - 2014 - Neural Turing Machines.pdf}
}

@book{graziano2013,
  title = {Consciousness and the Social Brain},
  author = {Graziano, Michael S. A.},
  year = {2013},
  publisher = {Oxford University Press},
  address = {Oxford ; New York},
  isbn = {978-0-19-992864-4},
  langid = {english},
  lccn = {BF311 .G692 2013},
  keywords = {Brain,Consciousness},
  file = {/Users/nobr/Zotero/storage/WQJKTN4D/Graziano - 2013 - Consciousness and the social brain.pdf}
}

@inproceedings{grbic2020,
  title = {Safe {{Reinforcement Learning}} through {{Meta-learned Instincts}}},
  booktitle = {The 2020 {{Conference}} on {{Artificial Life}}},
  author = {Grbic, Djordje and Risi, Sebastian},
  year = {2020},
  pages = {283--291},
  publisher = {MIT Press},
  address = {Online},
  doi = {10.1162/isal_a_00318},
  urldate = {2023-11-09},
  abstract = {An important goal in reinforcement learning is to create agents that can quickly adapt to new goals while avoiding situations that might cause damage to themselves or their environments. One way agents learn is through exploration mechanisms, which are needed to discover new policies. However, in deep reinforcement learning, exploration is normally done by injecting noise in the action space. While performing well in many domains, this setup has the inherent risk that the noisy actions performed by the agent lead to unsafe states in the environment. Here we introduce a novel approach called Meta-Learned Instinctual Networks (MLIN) that allows agents to safely learn during their lifetime while avoiding potentially hazardous states. At the core of the approach is a plastic network trained through reinforcement learning and an evolved ``instinctual'' network, which does not change during the agent's lifetime but can modulate the noisy output of the plastic network. We test our idea on a simple 2D navigation task with no-go zones, in which the agent has to learn to approach new targets during deployment. MLIN outperforms standard meta-trained networks and allows agents, after an evolutionary training phase, to learn to navigate to new targets without colliding with any of the no-go zones. These results suggest that meta-learning augmented with an instinctual network is a promising new approach for RL in safety-critical domains.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/U9WY5THC/Grbic and Risi - 2020 - Safe Reinforcement Learning through Meta-learned Instincts.pdf}
}

@article{green2008,
  title = {The Primes Contain Arbitrarily Long Arithmetic Progressions},
  author = {Green, Benjamin and Tao, Terence},
  year = {2008},
  month = mar,
  journal = {Annals of Mathematics},
  volume = {167},
  number = {2},
  pages = {481--547},
  issn = {0003-486X},
  doi = {10.4007/annals.2008.167.481},
  urldate = {2024-02-27},
  abstract = {We prove that there are arbitrarily long arithmetic progressions of primes. There are three major ingredients. The first is Szemere{\textasciiacute}di's theorem, which asserts that any subset of the integers of positive density contains progressions of arbitrary length. The second, which is the main new ingredient of this paper, is a certain transference principle. This allows us to deduce from Szemere{\textasciiacute}di's theorem that any subset of a sufficiently pseudorandom set (or measure) of positive relative density contains progressions of arbitrary length. The third ingredient is a recent result of Goldston and Y{\i}ld{\i}r{\i}m, which we reproduce here. Using this, one may place (a large fraction of) the primes inside a pseudorandom set of ``almost primes'' (or more precisely, a pseudorandom measure concentrated on almost primes) with positive relative density.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GW2M3TIJ/Green and Tao - 2008 - The primes contain arbitrarily long arithmetic progressions.pdf}
}

@misc{green2024,
  title = {{{GWtuna}}: {{Trawling}} through the Data to Find {{Gravitational Waves}} with {{Optuna}} and {{Jax}}},
  shorttitle = {{{GWtuna}}},
  author = {Green, Susanna and Lundgren, Andrew},
  year = {2024},
  month = nov,
  number = {arXiv:2411.03207},
  eprint = {2411.03207},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.03207},
  urldate = {2024-11-26},
  abstract = {GWtuna is a fast gravitational-wave search prototype built on Optuna (optimisation software library) and JAX (accelerator-orientated array computation library) [1, 2]. Using Optuna, we introduce black box optimisation algorithms and evolutionary strategy algorithms to the gravitational-wave community. Tree-structured Parzen Estimator (TPE) and Covariance Matrix Adaption Evolution Strategy (CMA-ES) have been used to create the first template bank free search and used to identify binary neutron star mergers. TPE can identify a binary neutron star merger in 1 second (median value) and less than 1000 matched-filter evaluations when 512 seconds of data is searched over. A stopping algorithm is used to curtail the TPE search if the signal-to-noise ratio (SNR) threshold has been reached, or the SNR has not improved in 500 evaluations. If the SNR threshold is surpassed, CMA-ES is used to recover the SNR and the template parameters in 9,000 matched filter iterations taking 48 seconds (median value). GWtuna showcases alternatives to the standard template bank search and therefore has the potential to revolutionise the future of gravitational-wave data analysis.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,General Relativity and Quantum Cosmology},
  file = {/Users/nobr/Zotero/storage/9PCIG22D/Green and Lundgren - 2024 - GWtuna Trawling through the data to find Gravitational Waves with Optuna and Jax.pdf;/Users/nobr/Zotero/storage/LB9N4XNU/2411.html}
}

@misc{gu2023,
  title = {Decoding Natural Image Stimuli from {{fMRI}} Data with a Surface-Based Convolutional Network},
  author = {Gu, Zijin and Jamison, Keith and Kuceyeski, Amy and Sabuncu, Mert},
  year = {2023},
  month = mar,
  number = {arXiv:2212.02409},
  eprint = {2212.02409},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.02409},
  urldate = {2023-11-21},
  abstract = {Due to the low signal-to-noise ratio and limited resolution of functional MRI data, and the high complexity of natural images, reconstructing a visual stimulus from human brain fMRI measurements is a challenging task. In this work, we propose a novel approach for this task, which we call Cortex2Image, to decode visual stimuli with high semantic fidelity and rich fine-grained detail. In particular, we train a surface-based convolutional network model that maps from brain response to semantic image features first (Cortex2Semantic). We then combine this model with a high-quality image generator (Instance-Conditioned GAN) to train another mapping from brain response to fine-grained image features using a variational approach (Cortex2Detail). Image reconstructions obtained by our proposed method achieve state-of-the-art semantic fidelity, while yielding good fine-grained similarity with the ground-truth stimulus. Our code is available at: https://github.com/zijin-gu/meshconv-decoding.git.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/nobr/Zotero/storage/QS8NLYBP/Gu et al. - 2023 - Decoding natural image stimuli from fMRI data with a surface-based convolutional network.pdf;/Users/nobr/Zotero/storage/3NWWUXYM/2212.html}
}

@phdthesis{guerramiller2016,
  type = {{{MESTRE EM ENGENHARIA EL{\'E}TRICA}}},
  title = {{{DISTRIBUTED SPARSITY-AWARE SIGNAL PROCESSING ALGORITHMS FOR SENSOR NETWORKS}}},
  author = {Guerra Miller, Tamara},
  year = {2016},
  month = mar,
  address = {Rio de Janeiro, Brazil},
  doi = {10.17771/PUCRio.acad.27190},
  urldate = {2023-11-09},
  langid = {english},
  school = {PONTIF{\'I}CIA UNIVERSIDADE CAT{\'O}LICA DO RIO DE JANEIRO},
  file = {/Users/nobr/Zotero/storage/UH8MLNJ4/Guerra Miller - 2016 - DISTRIBUTED SPARSITY-AWARE SIGNAL PROCESSING ALGORITHMS FOR SENSOR NETWORKS.pdf}
}

@misc{gui2021,
  title = {Learning {{Rates}} for {{Multi-task Regularization Networks}}},
  author = {Gui, Jie and Zhang, Haizhang},
  year = {2021},
  month = sep,
  number = {arXiv:2104.00453},
  eprint = {2104.00453},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.00453},
  urldate = {2024-11-12},
  abstract = {Multi-task learning is an important trend of machine learning in facing the era of artificial intelligence and big data. Despite a large amount of researches on learning rate estimates of various single-task machine learning algorithms, there is little parallel work for multi-task learning. We present mathematical analysis on the learning rate estimate of multi-task learning based on the theory of vector-valued reproducing kernel Hilbert spaces and matrix-valued reproducing kernels. For the typical multi-task regularization networks, an explicit learning rate dependent both on the number of sample data and the number of tasks is obtained. It reveals that the generalization ability of multi-task learning algorithms is indeed affected as the number of tasks increases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Functional Analysis},
  file = {/Users/nobr/Zotero/storage/2JRJFMIC/Gui and Zhang - 2021 - Learning Rates for Multi-task Regularization Networks.pdf;/Users/nobr/Zotero/storage/A9YNXH35/2104.html}
}

@article{guin2004,
  title = {The {{Carrier Bag Theory}} of {{Fiction}}},
  author = {Guin, Ursula K Le},
  year = {2004},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/W3ABJVCQ/Guin - The Carrier Bag Theory of Fiction.pdf}
}

@article{gullberg2009,
  title = {Attention to {{Speech-Accompanying Gestures}}: {{Eye Movements}} and {{Information Uptake}}},
  author = {Gullberg, Marianne and Kita, Sotaro},
  year = {2009},
  journal = {Journal of Nonverbal Behavior},
  volume = {33},
  number = {4},
  pages = {251--277},
  doi = {10.1007/s10919-009-0073-2},
  file = {/Users/nobr/Zotero/storage/8WVCG4CT/Gullberg and Kita - 2009 - Attention to Speech-Accompanying Gestures Eye Movements and Information Uptake.pdf}
}

@misc{guMambaLinearTimeSequence2023,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  year = {2023},
  month = dec,
  number = {arXiv:2312.00752},
  eprint = {2312.00752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.00752},
  urldate = {2024-01-19},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/M8ECE33K/Gu and Dao - 2023 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf;/Users/nobr/Zotero/storage/X9CUYARZ/2312.html}
}

@article{guo2022,
  title = {Attention {{Mechanisms}} in {{Computer Vision}}: {{A Survey}}},
  shorttitle = {Attention {{Mechanisms}} in {{Computer Vision}}},
  author = {Guo, Meng-Hao and Xu, Tian-Xing and Liu, Jiang-Jiang and Liu, Zheng-Ning and Jiang, Peng-Tao and Mu, Tai-Jiang and Zhang, Song-Hai and Martin, Ralph R. and Cheng, Ming-Ming and Hu, Shi-Min},
  year = {2022},
  month = sep,
  journal = {Computational Visual Media},
  volume = {8},
  number = {3},
  eprint = {2111.07624},
  primaryclass = {cs},
  pages = {331--368},
  issn = {2096-0433, 2096-0662},
  doi = {10.1007/s41095-022-0271-y},
  urldate = {2023-11-17},
  abstract = {Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have achieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video understanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial attention, temporal attention and branch attention; a related repository https://github.com/MenghaoGuo/Awesome-Vision-Attentions is dedicated to collecting related work. We also suggest future directions for attention mechanism research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/I86XUTSZ/Guo et al. - 2022 - Attention Mechanisms in Computer Vision A Survey.pdf;/Users/nobr/Zotero/storage/RSEW6F8W/2111.html}
}

@misc{guo2024,
  title = {Large {{Language Model}} Based {{Multi-Agents}}: {{A Survey}} of {{Progress}} and {{Challenges}}},
  shorttitle = {Large {{Language Model}} Based {{Multi-Agents}}},
  author = {Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
  year = {2024},
  month = apr,
  number = {arXiv:2402.01680},
  eprint = {2402.01680},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01680},
  urldate = {2024-12-03},
  abstract = {Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/CLFEKN77/Guo et al. - 2024 - Large Language Model based Multi-Agents A Survey of Progress and Challenges.pdf;/Users/nobr/Zotero/storage/4F5YU54B/2402.html}
}

@article{gupta2021,
  title = {Obtaining Leaner Deep Neural Networks for Decoding Brain Functional Connectome in a Single Shot},
  author = {Gupta, Sukrit and Chan, Yi Hao and Rajapakse, Jagath C.},
  year = {2021},
  month = sep,
  journal = {Neurocomputing},
  volume = {453},
  pages = {326--336},
  issn = {09252312},
  doi = {10.1016/j.neucom.2020.04.152},
  urldate = {2023-06-19},
  abstract = {Neuroscientific knowledge points to the presence of redundancy in the correlations of the brain's functional activity. These redundancies can be removed to mitigate the problem of overfitting when deep neural network (DNN) models are used to classify neuroimaging datasets. We propose an algorithm that removes insignificant nodes of DNNs in a layerwise manner and then adds a subset of correlated features in a single shot. When performing experiments with functional MRI datasets for classifying patients from healthy controls, we were able to obtain simpler and more generalizable DNNs. The obtained DNNs maintained a similar performance as the full network with only around 2\% of the initial trainable parameters. Further, we used the trained network to identify salient brain regions and connections from functional connectome for multiple brain disorders. The identified biomarkers were found to closely correspond to previously known disease biomarkers. The proposed methods have cross-modal applications in obtaining leaner DNNs that seem to fit neuroimaging data better. The corresponding code is available at https:// github.com/SCSE-Biomedical-Computing-Group/LEAN\_CLIP.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/277V589E/Gupta et al. - 2021 - Obtaining leaner deep neural networks for decoding.pdf}
}

@incollection{gurbuz2020,
  title = {Deep {{Graph Normalizer}}: {{A Geometric Deep Learning Approach}} for {{Estimating Connectional Brain Templates}}},
  shorttitle = {Deep {{Graph Normalizer}}},
  author = {Gurbuz, Mustafa Burak and Rekik, Islem},
  year = {2020},
  volume = {12267},
  eprint = {2012.14131},
  primaryclass = {cs},
  pages = {155--165},
  doi = {10.1007/978-3-030-59728-3_16},
  urldate = {2023-05-08},
  abstract = {A connectional brain template (CBT) is a normalized graph-based representation of a population of brain networks also regarded as an average connectome. CBTs are powerful tools for creating representative maps of brain connectivity in typical and atypical populations. Particularly, estimating a well-centered and representative CBT for populations of multi-view brain networks (MVBN) is more challenging since these networks sit on complex manifolds and there is no easy way to fuse different heterogeneous network views. This problem remains unexplored with the exception of a few recent works rooted in the assumption that the relationship between connectomes are mostly linear. However, such an assumption fails to capture complex patterns and non-linear variation across individuals. Besides, existing methods are simply composed of sequential MVBN processing blocks without any feedback mechanism, leading to error accumulation. To address these issues, we propose Deep Graph Normalizer (DGN), the first geometric deep learning (GDL) architecture for normalizing a population of MVBNs by integrating them into a single connectional brain template. Our end-to-end DGN learns how to fuse multi-view brain networks while capturing non-linear patterns across subjects and preserving brain graph topological properties by capitalizing on graph convolutional neural networks. We also introduce a randomized weighted loss function which also acts as a regularizer to minimize the distance between the population of MVBNs and the estimated CBT, thereby enforcing its centeredness. We demonstrate that DGN significantly outperforms existing state-of-the-art methods on estimating CBTs on both small-scale and large-scale connectomic datasets in terms of both representativeness and discriminability (i.e., identifying distinctive connectivities fingerprinting each brain network population).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/XLVKCK23/Gurbuz and Rekik - 2020 - Deep Graph Normalizer A Geometric Deep Learning A.pdf;/Users/nobr/Zotero/storage/JVMTMVIE/2012.html}
}

@misc{guth2024,
  title = {New Large Value Estimates for {{Dirichlet}} Polynomials},
  author = {Guth, Larry and Maynard, James},
  year = {2024},
  month = may,
  number = {arXiv:2405.20552},
  eprint = {2405.20552},
  primaryclass = {math},
  urldate = {2024-06-04},
  abstract = {We prove new bounds for how often Dirichlet polynomials can take large values. This gives improved estimates for a Dirichlet polynomial of length \$N\$ taking values of size close to \$N{\textasciicircum}\{3/4\}\$, which is the critical situation for several estimates in analytic number theory connected to prime numbers and the Riemann zeta function. As a consequence, we deduce a zero density estimate \$N({\textbackslash}sigma,T){\textbackslash}le T{\textasciicircum}\{30(1-{\textbackslash}sigma)/13+o(1)\}\$ and asymptotics for primes in short intervals of length \$x{\textasciicircum}\{17/30+o(1)\}\$.},
  archiveprefix = {arXiv},
  keywords = {11M26 11N05,Mathematics - Number Theory},
  file = {/Users/nobr/Zotero/storage/R454VZ4I/Guth and Maynard - 2024 - New large value estimates for Dirichlet polynomials.pdf}
}

@article{ha2017,
  title = {A {{Neural Representation}} of {{Sketch Drawings}}},
  author = {Ha, David and Eck, Douglas},
  year = {2017},
  month = apr,
  journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
  eprint = {1704.03477},
  publisher = {International Conference on Learning Representations, ICLR},
  urldate = {2023-03-22},
  abstract = {We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/S8G8BZC7/full-text.pdf}
}

@misc{ha2018,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = sep,
  number = {arXiv:1809.01999},
  eprint = {1809.01999},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-08-10},
  abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper at https://worldmodels.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/XDXU8ZUG/Ha and Schmidhuber - 2018 - Recurrent World Models Facilitate Policy Evolution.pdf}
}

@misc{haerpferWorldValuesSurvey2022,
  title = {World {{Values Survey Time-Series}} (1981-2022) {{Cross-National Data-Set}}},
  author = {Haerpfer, Christian and Inglehart, Ronald and Moreno, Alejandro and Welzel, Christian and Kizilova, Kseniya and {Diez-Medrano}, Jaime and Lagos, Marta and Norris, Pippa and Ponarin, Eduard and Puranen, Bi},
  year = {2022},
  publisher = {World Values Survey Association},
  doi = {10.14281/18241.17},
  urldate = {2024-02-27},
  abstract = {The~World Values Survey (WVS)~is an international research program devoted to the scientific and academic study of social, political, economic, religious and cultural values of people in the world. The project's goal is to assess which impact values stability or change over time has on the social, political and economic development of countries and societies.~The project grew out of the European Values Study~and was started in 1981~by its Founder and first President (1981-2013) Professor~Ronald Inglehart~from the University of Michigan (USA) and his team, and since then has been operating in more than 120 world societies. The main research instrument of the project is a representative comparative social survey which is conducted globally every 5 years. Extensive geographical and thematic scope, free availability of survey data and project findings for broad public turned the WVS into one of the most authoritative and widely-used cross-national surveys in the social sciences. At the moment, WVS is the largest non-commercial cross-national empirical time-series investigation of human beliefs and values ever executed.},
  collaborator = {Kizilova, Kseniya and Haerpfer, Christian and Moreno, Alejandro and Welzel, Christian and Inglehart, Ronald and {Diez-Medrano}, Jaime and Lagos, Marta and Norris, Pippa and Ponarin, Eduard and Puranen, Bi},
  copyright = {Other, No commercial usage},
  langid = {english},
  keywords = {10200 Sociology,10500 Political Science,FOS: Political science,FOS: Sociology,materialism,perception,political culture,Political Science,post-materialism,public opinion,Sociology,value change,value system,value-orientation}
}

@book{Halperin-Kaddari2025QuestForJustice,
  title = {A Quest for Justice: {{October}} 7 and Beyond},
  author = {{Halperin-Kaddari}, Ruth and {Ben-Or}, Nava and {Zagagi-Pinhas}, Sharon},
  year = {2025},
  publisher = {The Dinah Project},
  file = {/Users/nobr/Zotero/storage/HM3EISRQ/Halperin-Kaddari et al. - 2023 - A quest for justice October 7 and beyond.pdf}
}

@misc{hamas2024,
  title = {Our {{Narrative}}... {{Operation Al-Aqsa Flood}}},
  shorttitle = {Our {{Narrative}}...},
  author = {Hamas},
  year = {2024},
  file = {/Users/nobr/Zotero/storage/8UP39UGR/PDF.pdf}
}

@article{hameroff2022,
  title = {Consciousness, {{Cognition}} and the {{Neuronal Cytoskeleton}} -- {{A New Paradigm Needed}} in {{Neuroscience}}},
  author = {Hameroff, Stuart},
  year = {2022},
  month = jun,
  journal = {Frontiers in Molecular Neuroscience},
  volume = {15},
  pages = {869935},
  issn = {1662-5099},
  doi = {10.3389/fnmol.2022.869935},
  urldate = {2025-08-02},
  abstract = {Viewing the brain as a complex computer of simple neurons cannot account for consciousness nor essential features of cognition. Single cell organisms with no synapses perform purposeful intelligent functions using their cytoskeletal microtubules. A new paradigm is needed to view the brain as a scale-invariant hierarchy extending both upward from the level of neurons to larger and larger neuronal networks, but also downward, inward, to deeper, faster quantum and classical processes in cytoskeletal microtubules inside neurons. Evidence shows self-similar patterns of conductive resonances repeating in terahertz, gigahertz, megahertz, kilohertz and hertz frequency ranges in microtubules. These conductive resonances apparently originate in terahertz quantum dipole oscillations and optical interactions among pi electron resonance clouds of aromatic amino acid rings of tryptophan, phenylalanine and tyrosine within each tubulin, the component subunit of microtubules, and the brain's most abundant protein. Evidence from cultured neuronal networks also now shows that gigahertz and megahertz oscillations in dendritic-somatic microtubules regulate specific firings of distal axonal branches, causally modulating membrane and synaptic activities. The brain should be viewed as a scale-invariant hierarchy, with quantum and classical processes critical to consciousness and cognition originating in microtubules inside neurons.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4ECCUCNI/Hameroff - 2022 - Consciousness, Cognition and the Neuronal Cytoskeleton – A New Paradigm Needed in Neuroscience.pdf}
}

@misc{hamiltonSeparatingChirpChat2024,
  title = {Separating the "{{Chirp}}" from the "{{Chat}}": {{Self-supervised Visual Grounding}} of {{Sound}} and {{Language}}},
  shorttitle = {Separating the "{{Chirp}}" from the "{{Chat}}"},
  author = {Hamilton, Mark and Zisserman, Andrew and Hershey, John R. and Freeman, William T.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05629},
  eprint = {2406.05629},
  primaryclass = {cs, eess},
  urldate = {2024-06-14},
  abstract = {We present DenseAV, a novel dual encoder grounding architecture that learns high-resolution, semantically meaningful, and audio-visually aligned features solely through watching videos. We show that DenseAV can discover the ``meaning'' of words and the ``location'' of sounds without explicit localization supervision. Furthermore, it automatically discovers and distinguishes between these two types of associations without supervision. We show that DenseAV's localization abilities arise from a new multi-head feature aggregation operator that directly compares dense image and audio representations for contrastive learning. In contrast, many other systems that learn ``global'' audio and video representations cannot localize words and sound. Finally, we contribute two new datasets to improve the evaluation of AV representations through speech and sound prompted semantic segmentation. On these and other datasets we show DenseAV dramatically outperforms the prior art on speech and sound prompted semantic segmentation. DenseAV outperforms the previous state-of-the-art, ImageBind, on cross-modal retrieval using fewer than half of the parameters. Project Page: {\textbackslash}href\{https://aka.ms/denseav\}\{https://aka.ms/denseav\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/nobr/Zotero/storage/5DV9WP9J/Hamilton et al. - 2024 - Separating the Chirp from the Chat Self-supervised Visual Grounding of Sound and Language.pdf}
}

@misc{hamkinsDidTuringProve2024,
  title = {Did {{Turing}} Prove the Undecidability of the Halting Problem?},
  author = {Hamkins, Joel David and Nenu, Theodor},
  year = {2024},
  month = jun,
  number = {arXiv:2407.00680},
  eprint = {2407.00680},
  primaryclass = {cs, math},
  urldate = {2024-07-02},
  abstract = {We discuss the accuracy of the attribution commonly given to Turing's 1936 paper "On computable numbers..." for the computable undecidability of the halting problem, coming eventually to a nuanced conclusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science,Mathematics - Logic},
  file = {/Users/nobr/Zotero/storage/CTRJBG4S/Hamkins and Nenu - 2024 - Did Turing prove the undecidability of the halting problem.pdf}
}

@misc{hammond2009,
  title = {Wavelets on {{Graphs}} via {{Spectral Graph Theory}}},
  author = {Hammond, David K. and Vandergheynst, Pierre and Gribonval, R{\'e}mi},
  year = {2009},
  month = dec,
  number = {arXiv:0912.3848},
  eprint = {0912.3848},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.0912.3848},
  urldate = {2025-01-02},
  abstract = {We propose a novel method for constructing wavelet transforms of functions defined on the vertices of an arbitrary finite weighted graph. Our approach is based on defining scaling using the the graph analogue of the Fourier domain, namely the spectral decomposition of the discrete graph Laplacian \${\textbackslash}L\$. Given a wavelet generating kernel \$g\$ and a scale parameter \$t\$, we define the scaled wavelet operator \$T\_g{\textasciicircum}t = g(t{\textbackslash}L)\$. The spectral graph wavelets are then formed by localizing this operator by applying it to an indicator function. Subject to an admissibility condition on \$g\$, this procedure defines an invertible transform. We explore the localization properties of the wavelets in the limit of fine scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the transform that avoids the need for diagonalizing \${\textbackslash}L\$. We highlight potential applications of the transform through examples of wavelets on graphs corresponding to a variety of different problem domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematics - Functional Analysis,Mathematics - Information Theory},
  file = {/Users/nobr/Zotero/storage/JWRQGSPZ/Hammond et al. - 2009 - Wavelets on Graphs via Spectral Graph Theory.3848}
}

@article{hammond2011,
  title = {Wavelets on Graphs via Spectral Graph Theory},
  author = {Hammond, David K. and Vandergheynst, Pierre and Gribonval, R{\'e}mi},
  year = {2011},
  month = mar,
  journal = {Applied and Computational Harmonic Analysis},
  volume = {30},
  number = {2},
  pages = {129--150},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2010.04.005},
  urldate = {2024-12-26},
  abstract = {We propose a novel method for constructing wavelet transforms of functions defined on the vertices of an arbitrary finite weighted graph. Our approach is based on defining scaling using the graph analogue of the Fourier domain, namely the spectral decomposition of the discrete graph Laplacian L. Given a wavelet generating kernel g and a scale parameter t, we define the scaled wavelet operator Tgt=g(tL). The spectral graph wavelets are then formed by localizing this operator by applying it to an indicator function. Subject to an admissibility condition on g, this procedure defines an invertible transform. We explore the localization properties of the wavelets in the limit of fine scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the transform that avoids the need for diagonalizing L. We highlight potential applications of the transform through examples of wavelets on graphs corresponding to a variety of different problem domains.},
  keywords = {Graph theory,Overcomplete wavelet frames,Spectral graph theory,Wavelets},
  file = {/Users/nobr/Zotero/storage/63W626YK/Hammond et al. - 2011 - Wavelets on graphs via spectral graph theory.pdf;/Users/nobr/Zotero/storage/GXR7BKDW/Hammond et al. - 2011 - Wavelets on graphs via spectral graph theory.pdf}
}

@article{hammond2023,
  title = {Reasoning about Causality in Games},
  author = {Hammond, Lewis and Fox, James and Everitt, Tom and Carey, Ryan and Abate, Alessandro and Wooldridge, Michael},
  year = {2023},
  month = jul,
  journal = {Artificial Intelligence},
  volume = {320},
  pages = {103919},
  issn = {00043702},
  doi = {10.1016/j.artint.2023.103919},
  urldate = {2023-11-12},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3XB64HD7/Hammond et al. - 2023 - Reasoning about causality in games.pdf}
}

@techreport{hamrick2017unlocking,
  title = {Unlocking Potential: {{State}} of the Voluntary Carbon Markets 2017},
  author = {Hamrick, Kelley and Gallant, Melissa},
  year = {2017},
  pages = {42},
  address = {Washington, DC},
  institution = {Ecosystem Marketplace, Forest Trends}
}

@article{hamrick2019,
  title = {Analogues of Mental Simulation and Imagination in Deep Learning},
  author = {Hamrick, Jessica B.},
  year = {2019},
  month = oct,
  journal = {Current Opinion in Behavioral Sciences},
  volume = {29},
  pages = {8--16},
  publisher = {Elsevier Ltd},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2018.12.011},
  abstract = {Mental simulation --- the capacity to imagine what will or what could be --- is a salient feature of human cognition, playing a key role in a wide range of cognitive abilities. In artificial intelligence, the last few years have seen the development of methods which are analogous to mental models and mental simulation. This paper outlines recent methods in deep learning for constructing such models from data and learning to use them via reinforcement learning, and compares such approaches to human mental simulation. Model-based methods in deep learning can serve as powerful tools for building and scaling cognitive models. However, a number of challenges remain in matching the capacity of human mental simulation for efficiency, compositionality, generalization, and creativity.},
  file = {/Users/nobr/Zotero/storage/TVHPJWEM/Hamrick - 2019 - Analogues of mental simulation and imagination in deep learning.pdf}
}

@inproceedings{han2009,
  title = {Physical Layer Security Game: {{How}} to Date a Girl with Her Boyfriend on the Same Table},
  shorttitle = {Physical Layer Security Game},
  booktitle = {2009 {{International Conference}} on {{Game Theory}} for {{Networks}}},
  author = {Han, Zhu and Marina, Ninoslav and Debbah, Merouane and Hjorungnes, Are},
  year = {2009},
  month = may,
  pages = {287--294},
  doi = {10.1109/GAMENETS.2009.5137412},
  abstract = {Physical layer security is an emerging security concept that achieves perfect secrecy data transmission between the intended network nodes, while the eavesdropping malicious nodes obtain zero information. The so-called secrecy capacity can be improved using friendly jammers that introduce extra interference to the eavesdropping malicious nodes while the interference to the intended destination is limited. In this paper, we investigate the interaction between the source that transmits the desired data and friendly jammers who assist the source by "disguising" the eavesdropper. In order to obtain a distributed solution, we introduce a game theoretic approach. The game is defined in such a way that the source pays the friendly jammers to interfere the eavesdropper, and, therefore, increasing its secrecy capacity. Friendly jammers charge the source with a certain price for this "jamming service". There is a tradeoff for the price: If the price is too low, the profit of the jammers is low; and if the price is too high, the source would not buy the "service" (jamming power) or would buy it from other jammers. To analyze the game outcome, we define and investigate a Stackelberg game and construct a distributed algorithm. Our analysis and simulation results show the effectiveness of friendly jamming and the tradeoff for setting the price. The fancy title comes from the fact that it is similar to a scenario where the main fellow character (the source) tries to send a dating message to a lady (the intended destination), whose "poor" boyfriend plays the role of the eavesdropper that may hear the message. Friends of the source, the so-called "friendly jammers," try to distract the boyfriend, so that the dating message can be secretly transmitted. The game is defined in order to derive what is the optimal price that the friends can charge for this "friendly" action.},
  keywords = {Algorithm design and analysis,Analytical models,Data communication,Data security,Distributed algorithms,Game theory,Game Theory,Information security,Interference,Jamming,Physical layer,Physical Layer Security,Secrecy Capacity,Stackelberg game},
  file = {/Users/nobr/Zotero/storage/VSTQ2UIQ/Han et al. - 2009 - Physical layer security game How to date a girl w.pdf;/Users/nobr/Zotero/storage/IYDBB23U/5137412.html}
}

@article{han2017,
  title = {Propagation of {{Collective Temporal Regularity}} in {{Noisy Hierarchical Networks}}},
  author = {Han, Ruixue and Wang, Jiang and Miao, Rui and Deng, Bin and Qin, Yingmei and Yu, Haitao and Wei, Xile},
  year = {2017},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {28},
  number = {1},
  pages = {191--205},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2015.2502993},
  urldate = {2023-11-19},
  abstract = {Neuronal communication between different brain areas is achieved in terms of spikes. Consequently, spike-time regularity is closely related to many cognitive tasks and timing precision of neural information processing. A recent experiment on primate parietal cortex reports that spike-time regularity increases consistently from primary sensory to higher cortical regions. This observation conflicts with the influential view that spikes in the neocortex are fundamentally irregular. To uncover the underlying network mechanism, we construct a multilayered feedforward neural information transmission pathway and investigate how spike-time regularity evolves across subsequent layers. Numerical results reveal that despite the obviously irregular spiking patterns in previous several layers, neurons in downstream layers can generate rather regular spikes, which depends on the network topology. In particular, we find that collective temporal regularity in deeper layers exhibits resonance-like behavior with respect to both synaptic connection probability and synaptic weight, i.e., the optimal topology parameter maximizes the spike-timing regularity. Furthermore, it is demonstrated that synaptic properties, including inhibition, synaptic transient dynamics, and plasticity, have significant impacts on spike-timing regularity propagation. The emergence of the increasingly regular spiking (RS) patterns in higher parietal regions can, thus, be viewed as a natural consequence of spiking activity propagation between different brain areas. Finally, we validate an important function served by increased RS: promoting reliable propagation of spike-rate signals across downstream layers.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/M7URHXXZ/Han et al. - 2017 - Propagation of Collective Temporal Regularity in Noisy Hierarchical Networks.pdf}
}

@misc{han2018,
  title = {Variational {{Autoencoder}}: {{An Unsupervised Model}} for {{Modeling}} and {{Decoding fMRI Activity}} in {{Visual Cortex}}},
  shorttitle = {Variational {{Autoencoder}}},
  author = {Han, Kuan and Wen, Haiguang and Shi, Junxing and Lu, Kun-Han and Zhang, Yizhen and Liu, Zhongming},
  year = {2018},
  month = jan,
  doi = {10.1101/214247},
  urldate = {2023-05-13},
  abstract = {Goal-driven and feedforward-only convolutional neural networks (CNN) have been shown to be able to predict and decode cortical responses to natural images or videos. Here, we explored an alternative deep neural network, variational auto-encoder (VAE), as a computational model of the visual cortex. We trained a VAE with a five-layer encoder and a five-layer decoder to learn visual representations from a diverse set of unlabeled images. Inspired by the ``free-energy'' principle in neuroscience, we modeled the brain's bottom-up and top-down pathways using the VAE's encoder and decoder, respectively. Following such conceptual relationships, we used VAE to predict or decode cortical activity observed with functional magnetic resonance imaging (fMRI) from three human subjects passively watching natural videos. Compared to CNN, VAE resulted in relatively lower accuracies for predicting the fMRI responses to the video stimuli, especially for higher-order ventral visual areas. However, VAE offered a more convenient strategy for decoding the fMRI activity to reconstruct the video input, by first converting the fMRI activity to the VAE's latent variables, and then converting the latent variables to the reconstructed video frames through the VAE's decoder. This strategy was more advantageous than alternative decoding methods, e.g. partial least square regression, by reconstructing both the spatial structure and color of the visual input. Findings from this study support the notion that the brain, at least in part, bears a generative model of the visual world.},
  copyright = {{\copyright} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XHYLVLGX/Han et al. - 2018 - Variational Autoencoder An Unsupervised Model for Modeling and Decoding fMRI Activity in Visual Cortex.pdf}
}

@article{han2019,
  title = {Variational {{Autoencoder}}: {{An Unsupervised Model}} for {{Encoding}} and {{Decoding fMRI Activity}} in {{Visual Cortex}}},
  shorttitle = {Variational {{Autoencoder}}},
  author = {Han, Kuan and Wen, Haiguang and Shi, Junxing and Lu, Kun-Han and Zhang, Yizhen and Fu, Di and Liu, Zhongming},
  year = {2019},
  month = sep,
  journal = {NeuroImage},
  volume = {198},
  pages = {125--136},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2019.05.039},
  urldate = {2023-05-08},
  abstract = {Goal-driven and feedforward-only convolutional neural networks (CNN) have been shown to be able to predict and decode cortical responses to natural images or videos. Here, we explored an alternative deep neural network, variational auto-encoder (VAE), as a computational model of the visual cortex. We trained a VAE with a five-layer encoder and a five-layer decoder to learn visual representations from a diverse set of unlabeled images. Using the trained VAE, we predicted and decoded cortical activity observed with functional magnetic resonance imaging (fMRI) from three human subjects passively watching natural videos. Compared to CNN, VAE could predict the video-evoked cortical responses with comparable accuracy in early visual areas, but relatively lower accuracy in higher-order visual areas. The distinction between CNN and VAE in terms of encoding performance was primarily attributed to their different learning objectives, rather than their different model architecture or number of parameters. Despite lower encoding accuracies, VAE offered a more convenient strategy for decoding the fMRI activity to reconstruct the video input, by first converting the fMRI activity to the VAE's latent variables, and then converting the latent variables to the reconstructed video frames through the VAE's decoder. This strategy was more advantageous than alternative decoding methods, e.g. partial least square regression, for being able to reconstruct both the spatial structure and color of the visual input. Such findings highlight VAE as an unsupervised model for learning visual representation, as well as its potential and limitations for explaining cortical responses and reconstructing naturalistic and diverse visual experiences.},
  pmcid = {PMC6592726},
  pmid = {31103784},
  file = {/Users/nobr/Zotero/storage/EMAPIUB2/Han et al. - 2019 - Variational autoencoder An unsupervised model for.pdf;/Users/nobr/Zotero/storage/ZUX9HNVT/Han et al. - 2019 - Variational Autoencoder An Unsupervised Model for.pdf}
}

@article{han2020,
  title = {{{DBSCAN OPTIMIZATION FOR IMPROVING MARINE TRAJECTORY CLUSTERING AND ANOMALY DETECTION}}},
  author = {Han, X. and Armenakis, C. and Jadidi, M.},
  year = {2020},
  month = aug,
  journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {XLIII-B4-2020},
  pages = {455--461},
  issn = {2194-9034},
  doi = {10.5194/isprs-archives-XLIII-B4-2020-455-2020},
  urldate = {2023-07-04},
  abstract = {Today maritime transportation represents 90\% of international trade volume and there are more than 50,000 vessels sailing the ocean every day. Therefore, reducing maritime transportation security risks by systematically modelling and surveillance should be of high priority in the maritime domain. By statistics, majority of maritime accidents are caused by human error due to fatigue or misjudgment. Auto-vessels equipped with autonomous and semi-autonomous systems can reduce the reliance on human's intervention, thus make maritime navigation safer. This paper presents a clustering method for route planning and trajectory anomalies detection, which are the essential part of auto-vessel system design and development. In this paper, we present the development of an enhanced densitybased spatial clustering (DBSCAN) method that can be applied on historical or real-time Automatic Identification System (AIS) data, so that vessel routes can be modelled, and the trajectories' anomalies can be detected. The proposed methodology is based on developing an optimized trajectory clustering approach in two stages. Firstly, to increase the attribute dimension of the vessel's positioning data, therefore other characteristics such as velocity and direction are considered in the clustering process along with geospatial information. Secondly, the DBSCAN clustering model has been enhanced by introducing the Mahalanobis Distance metric considering the correlations of the position cluster points aiming to make the identification process more accurate as well as reducing the computational cost.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/U3U8FTCE/Han et al. - 2020 - DBSCAN OPTIMIZATION FOR IMPROVING MARINE TRAJECTORY CLUSTERING AND ANOMALY DETECTION.pdf}
}

@article{han2024,
  title = {Towards {{Uncertainty-Aware Language Agent}}},
  author = {Han, Jiuzhou and Buntine, Wray and Shareghi, Ehsan},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.14016},
  urldate = {2024-02-20},
  abstract = {While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrate that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscore the unreliability of verbalised confidence of LLMs as a proxy for uncertainty.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/nobr/Zotero/storage/JSHD7C6L/Han et al. - 2024 - Towards Uncertainty-Aware Language Agent.pdf}
}

@article{han2024a,
  title = {{{LLM Multi-Agent Systems}}: {{Challenges}} and {{Open Problems}}},
  shorttitle = {{{LLM Multi-Agent Systems}}},
  author = {Han, Shanshan and Zhang, Qifan and Yao, Yuhang and Jin, Weizhao and Xu, Zhaozhuo and He, Chaoyang},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.03578},
  urldate = {2024-12-03},
  abstract = {This paper explores existing works of multi-agent systems and identifies challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents within a multi-agent system, these systems can tackle complex tasks through collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore the potential application of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Multiagent Systems (cs.MA)},
  file = {/Users/nobr/Zotero/storage/6W8IKVYV/Han et al. - 2024 - LLM Multi-Agent Systems Challenges and Open Problems.pdf}
}

@misc{hao2023,
  title = {Reasoning with {{Language Model}} Is {{Planning}} with {{World Model}}},
  author = {Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  year = {2023},
  month = oct,
  number = {arXiv:2305.14992},
  eprint = {2305.14992},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-01},
  abstract = {Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal \${\textbackslash}textit\{world model\}\$ to predict the world \${\textbackslash}textit\{state\}\$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, \${\textbackslash}underline\{R\}\$easoning vi\${\textbackslash}underline\{a\}\$ \${\textbackslash}underline\{P\}\$lanning \${\textbackslash}textbf\{(RAP)\}\$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration \${\textbackslash}textit\{vs.\}\$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33\% relative improvement in a plan generation setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/2BRNCH2L/Hao et al. - 2023 - Reasoning with Language Model is Planning with World Model.pdf;/Users/nobr/Zotero/storage/KJ89IFUF/2305.html}
}

@article{hardy,
  title = {The {{Project Gutenberg eBook}} \#38769: {{A Course}} of {{Pure Mathematics}}},
  author = {Hardy, Godfrey Harold},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/Z2TI7BS3/Hardy - The Project Gutenberg eBook #38769 A Course of Pure Mathematics.pdf}
}

@article{harman,
  title = {Commission on the {{National Defense Strategy}}},
  author = {Harman, Congresswoman Jane and Edelman, Ambassador Eric and Keane, General John M and Mahnken, Thomas G and Rudman, Mara and Sixkiller, Mariah and Starzak, Alissa and Zakheim, Roger},
  abstract = {The threats the United States faces are the most serious and most challenging the nation has encountered since 1945 and include the potential for near-term major war. The United States last fought a global conflict during World War II, which ended nearly 80 years ago. The nation was last prepared for such a fight during the Cold War, which ended 35 years ago. It is not prepared today.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/E46AGEJP/Harman et al. - Commission on the National Defense Strategy.pdf}
}

@article{harper,
  title = {{{PFPL Supplement}}: {{Church}}'s {$\lambda$}-{{Calculus}}},
  author = {Harper, Robert},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SBS6R4KC/Harper - PFPL Supplement Church’s λ-Calculus.pdf}
}

@article{hartog2020,
  title = {Prevalence and Characteristics of Older Adults with a Persistent Death Wish without Severe Illness: A Large Cross-Sectional Survey},
  author = {Hartog, Iris D. and Zomers, Margot L. and {van Thiel}, Ghislaine J. M. W. and Leget, Carlo and Sachs, Alfred P. E. and Uiterwaal, Cuno S. P. M. and {van den Berg}, Vera and {van Wijngaarden}, Els},
  year = {2020},
  journal = {BMC Geriatrics},
  volume = {20},
  number = {1},
  doi = {10.1186/s12877-020-01735-0},
  file = {/Users/nobr/Zotero/storage/I47S33YV/document.pdf}
}

@book{hasselblatt2003,
  title = {A {{First Course}} in {{Dynamics}}: With a {{Panorama}} of {{Recent Developments}}},
  shorttitle = {A {{First Course}} in {{Dynamics}}},
  author = {Hasselblatt, Boris and Katok, Anatole},
  year = {2003},
  month = jun,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511998188},
  urldate = {2023-08-14},
  abstract = {The theory of dynamical systems is a major mathematical discipline closely intertwined with all main areas of mathematics. It has greatly stimulated research in many sciences and given rise to the vast new area variously called applied dynamics, nonlinear science, or chaos theory. This introduction for senior undergraduate and beginning graduate students of mathematics, physics, and engineering combines mathematical rigor with copious examples of important applications. It covers the central topological and probabilistic notions in dynamics ranging from Newtonian mechanics to coding theory. Readers need not be familiar with manifolds or measure theory; the only prerequisite is a basic undergraduate analysis course. The authors begin by describing the wide array of scientific and mathematical questions that dynamics can address. They then use a progression of examples to present the concepts and tools for describing asymptotic behavior in dynamical systems, gradually increasing the level of complexity. The final chapters introduce modern developments and applications of dynamics. Subjects include contractions, logistic maps, equidistribution, symbolic dynamics, mechanics, hyperbolic dynamics, strange attractors, twist maps, and KAM-theory.},
  isbn = {978-0-521-58304-6 978-0-521-58750-1 978-0-511-99818-8},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IMLACYCK/Hasselblatt and Katok - 2003 - A First Course in Dynamics with a Panorama of Rec.pdf}
}

@article{haWorldModels2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = mar,
  journal = {World Models},
  volume = {1},
  number = {1},
  pages = {e10},
  doi = {10.5281/zenodo.1207631},
  urldate = {2025-09-16},
  abstract = {Can agents learn inside of their own dreams?},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZSCIZPWN/Ha and Schmidhuber - 2018 - World Models.pdf}
}

@article{haxby2001,
  title = {Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex},
  author = {Haxby, J. V. and Gobbini, M. I. and Furey, M. L. and Ishai, A. and Schouten, J. L. and Pietrini, P.},
  year = {2001},
  month = sep,
  journal = {Science (New York, N.Y.)},
  volume = {293},
  number = {5539},
  pages = {2425--2430},
  issn = {0036-8075},
  doi = {10.1126/science.1063736},
  abstract = {The functional architecture of the object vision pathway in the human brain was investigated using functional magnetic resonance imaging to measure patterns of response in ventral temporal cortex while subjects viewed faces, cats, five categories of man-made objects, and nonsense pictures. A distinct pattern of response was found for each stimulus category. The distinctiveness of the response to a given category was not due simply to the regions that responded maximally to that category, because the category being viewed also could be identified on the basis of the pattern of response when those regions were excluded from the analysis. Patterns of response that discriminated among all categories were found even within cortical regions that responded maximally to only one category. These results indicate that the representations of faces and objects in ventral temporal cortex are widely distributed and overlapping.},
  langid = {english},
  pmid = {11577229},
  keywords = {Brain Mapping,Face,Female,Form Perception,Humans,Magnetic Resonance Imaging,Male,Pattern Recognition Visual,Recognition Psychology,Temporal Lobe,Visual Pathways},
  file = {/Users/nobr/Zotero/storage/CLWPS83F/Haxby et al. - 2001 - Distributed and overlapping representations of fac.pdf}
}

@inproceedings{hayes2015,
  title = {Introduction to Stochastic Computing and Its Challenges},
  booktitle = {Proceedings of the 52nd {{Annual Design Automation Conference}}},
  author = {Hayes, John P.},
  year = {2015},
  month = jun,
  pages = {1--3},
  publisher = {ACM},
  address = {San Francisco California},
  doi = {10.1145/2744769.2747932},
  urldate = {2024-02-09},
  abstract = {We give a short overview of stochastic computing (SC) and its uses. SC computes with randomized bit-streams that loosely resemble the neural spike trains of the brain. Its key feature is the use of lowcost and low-power logic elements to implement complex numerical operations in a highly error-tolerant fashion. These advantages must be weighed against SC's inherently slow computing speed and low precision. Although studied sporadically since its invention in the 1960s, SC has regained interest recently as potentially suited to some emerging nanotechnologies, and to applications such as ECC decoding and biomedical image processing. However, a number of major challenges must be overcome if this potential is to be fully realized.},
  isbn = {978-1-4503-3520-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4U38WZPY/Hayes - 2015 - Introduction to stochastic computing and its challenges.pdf}
}

@book{haykin2014,
  title = {Adaptive Filter Theory},
  author = {Haykin, Simon S.},
  year = {2014},
  series = {Always Learning},
  edition = {Fifth edition, international edition},
  publisher = {Pearson},
  address = {Upper Saddle River Boston Columbus San Francisco New York},
  collaborator = {Prabhakar, Telagarapu},
  isbn = {978-0-273-76408-3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/VVPBV29F/Haykin - 2014 - Adaptive filter theory.pdf}
}

@misc{he2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = feb,
  number = {arXiv:1502.01852},
  eprint = {1502.01852},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.01852},
  urldate = {2024-11-13},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/EYFS2UUN/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf;/Users/nobr/Zotero/storage/7SE43QQ4/1502.html}
}

@inproceedings{he2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2023-05-10},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IZ9EG4MN/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@article{he2023b,
  title = {Deep {{Transformers}} without {{Shortcuts}}: {{Modifying Self-attention}} for {{Faithful Signal Propagation}}},
  shorttitle = {Deep {{Transformers}} without {{Shortcuts}}},
  author = {He, Bobby and Martens, James and Zhang, Guodong and Botev, Aleksandar and Brock, Andrew and Smith, Samuel L and Teh, Yee Whye},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.10322},
  urldate = {2024-05-25},
  abstract = {Skip connections and normalisation layers form two standard architectural components that are ubiquitous for the training of Deep Neural Networks (DNNs), but whose precise roles are poorly understood. Recent approaches such as Deep Kernel Shaping have made progress towards reducing our reliance on them, using insights from wide NN kernel theory to improve signal propagation in vanilla DNNs (which we define as networks without skips or normalisation). However, these approaches are incompatible with the self-attention layers present in transformers, whose kernels are intrinsically more complicated to analyse and control. And so the question remains: is it possible to train deep vanilla transformers? We answer this question in the affirmative by designing several approaches that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers. Our methods address various intricacies specific to signal propagation in transformers, including the interaction with positional encoding and causal masking. In experiments on WikiText-103 and C4, our approaches enable deep transformers without normalisation to train at speeds matching their standard counterparts, and deep vanilla transformers to reach the same performance as standard ones after about 5 times more iterations.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/nobr/Zotero/storage/3Y2NQ4GA/He et al. - 2023 - Deep Transformers without Shortcuts Modifying Self-attention for Faithful Signal Propagation.pdf}
}

@misc{heckel2024,
  title = {Kmheckel/Spyx: V0.1.20},
  shorttitle = {Kmheckel/Spyx},
  author = {Heckel, Kade and Abreu, Steven and Lenz, Gregor and Nowotny, Thomas and {neworderofjamie}},
  year = {2024},
  month = aug,
  doi = {10.5281/zenodo.13329958},
  urldate = {2024-12-16},
  abstract = {Minor dependency modifications.},
  howpublished = {Zenodo}
}

@misc{heckel2024a,
  title = {Spyx: {{A Library}} for {{Just-In-Time Compiled Optimization}} of {{Spiking Neural Networks}}},
  shorttitle = {Spyx},
  author = {Heckel, Kade M. and Nowotny, Thomas},
  year = {2024},
  month = feb,
  journal = {arXiv.org},
  urldate = {2024-09-05},
  abstract = {As the role of artificial intelligence becomes increasingly pivotal in modern society, the efficient training and deployment of deep neural networks have emerged as critical areas of focus. Recent advancements in attention-based large neural architectures have spurred the development of AI accelerators, facilitating the training of extensive, multi-billion parameter models. Despite their effectiveness, these powerful networks often incur high execution costs in production environments. Neuromorphic computing, inspired by biological neural processes, offers a promising alternative. By utilizing temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance energy efficiency through a reduced and low-power hardware footprint. However, the training of SNNs can be challenging due to their recurrent nature which cannot as easily leverage the massive parallelism of modern AI accelerators. To facilitate the investigation of SNN architectures and dynamics researchers have sought to bridge Python-based deep learning frameworks such as PyTorch or TensorFlow with custom-implemented compute kernels. This paper introduces Spyx, a new and lightweight SNN simulation and optimization library designed in JAX. By pre-staging data in the expansive vRAM of contemporary accelerators and employing extensive JIT compilation, Spyx allows for SNN optimization to be executed as a unified, low-level program on NVIDIA GPUs or Google TPUs. This approach achieves optimal hardware utilization, surpassing the performance of many existing SNN training frameworks while maintaining considerable flexibility.},
  howpublished = {https://arxiv.org/abs/2402.18994v1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IT9CQC5Q/Heckel and Nowotny - 2024 - Spyx A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks.pdf}
}

@article{heil2021,
  title = {Reproducibility Standards for Machine Learning in the Life Sciences},
  author = {Heil, Benjamin J. and Hoffman, Michael M. and Markowetz, Florian and Lee, Su-In and Greene, Casey S. and Hicks, Stephanie C.},
  year = {2021},
  month = oct,
  journal = {Nature Methods},
  volume = {18},
  number = {10},
  pages = {1132--1135},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01256-7},
  urldate = {2024-04-06},
  abstract = {To make machine-learning analyses in the life sciences more computationally reproducible, we propose standards based on data, model and code publication, programming best practices and workflow automation. By meeting these standards, the community of researchers applying machine-learning methods in the life sciences can ensure that their analyses are worthy of trust.},
  copyright = {2021 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Data publication and archiving,Machine learning,Standards},
  file = {/Users/nobr/Zotero/storage/KZDTJVDJ/Heil et al. - 2021 - Reproducibility standards for machine learning in the life sciences.pdf}
}

@article{heinrich2015,
  title = {Fictitious {{Self-Play}} in {{Extensive-Form Games}}},
  author = {Heinrich, Johannes and Lanctot, Marc},
  year = {2015},
  abstract = {Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4U4PNR84/Heinrich and Lanctot - Fictitious Self-Play in Extensive-Form Games.pdf}
}

@article{hellenbrand2023,
  title = {Thin Film Design of Amorphous Hafnium Oxide Nanocomposites Enabling Strong Interfacial Resistive Switching Uniformity},
  author = {Hellenbrand, Markus and Bakhit, Babak and Dou, Hongyi and Xiao, Ming and Hill, Megan O. and Sun, Zhuotong and Mehonic, Adnan and Chen, Aiping and Jia, Quanxi and Wang, Haiyan and {MacManus-Driscoll}, Judith L.},
  year = {2023},
  month = jun,
  journal = {Science Advances},
  volume = {9},
  number = {25},
  pages = {eadg1946},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.adg1946},
  urldate = {2023-07-18},
  abstract = {A design concept of phase-separated amorphous nanocomposite thin films is presented that realizes interfacial resistive switching (RS) in hafnium-oxide-based devices. The films are formed by incorporating an average of 7\% Ba into hafnium oxide during pulsed laser deposition at temperatures {$\leq$}400{$^\circ$}C. The added Ba prevents the films from crystallizing and leads to {$\sim$}20-nm-thin films consisting of an amorphous HfOx host matrix interspersed with {$\sim$}2-nm-wide, {$\sim$}5-to-10-nm-pitch Ba-rich amorphous nanocolumns penetrating approximately two-thirds through the films. This restricts the RS to an interfacial Schottky-like energy barrier whose magnitude is tuned by ionic migration under an applied electric field. Resulting devices achieve stable cycle-to-cycle, device-to-device, and sample-to-sample reproducibility with a measured switching endurance of {$\geq$}104 cycles for a memory window {$\geq$}10 at switching voltages of {\textpm}2 V.~Each device can be set to multiple intermediate resistance states, which enables synaptic spike-timing--dependent plasticity. The presented concept unlocks additional design variables for RS devices.},
  file = {/Users/nobr/Zotero/storage/APJG4WAR/Hellenbrand et al. - 2023 - Thin film design of amorphous hafnium oxide nanoco.pdf}
}

@article{hendler1990,
  title = {{{AI Planning}}: {{Systems}} and {{Techniques}}},
  author = {Hendler, James},
  year = {1990},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5KSRVG6Q/Hendler - AI Planning Systems and Techniques.pdf}
}

@article{hershey,
  title = {Not {{Just Lines}} on a {{Map}}: {{A History}} of {{Military Mapping}}},
  author = {Hershey, Andrew},
  year = {2012},
  journal = {Strategy \& Tactics},
  number = {274},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TZS4DNCU/Hershey - A History of Military Mapping.pdf}
}

@misc{heSimplifyingTransformerBlocks2023,
  title = {Simplifying {{Transformer Blocks}}},
  author = {He, Bobby and Hofmann, Thomas},
  year = {2023},
  month = nov,
  number = {arXiv:2311.01906},
  eprint = {2311.01906},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.01906},
  urldate = {2024-02-09},
  abstract = {A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections \& normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable. In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15\% faster training throughput, and using 15\% fewer parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/TDSLNGQD/He and Hofmann - 2023 - Simplifying Transformer Blocks.pdf;/Users/nobr/Zotero/storage/AXB6FQYI/2311.html}
}

@article{heuillet2021,
  title = {Explainability in Deep Reinforcement Learning},
  author = {Heuillet, Alexandre and Couthouis, Fabien and {D{\'i}az-Rodr{\'i}guez}, Natalia},
  year = {2021},
  month = feb,
  journal = {Knowledge-Based Systems},
  volume = {214},
  pages = {106685},
  issn = {09507051},
  doi = {10.1016/j.knosys.2020.106685},
  urldate = {2023-11-12},
  abstract = {A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RZTY6GCR/Heuillet et al. - 2021 - Explainability in deep reinforcement learning.pdf}
}

@misc{hinckLLaVAGemmaAcceleratingMultimodal2024,
  title = {{{LLaVA-Gemma}}: {{Accelerating Multimodal Foundation Models}} with a {{Compact Language Model}}},
  shorttitle = {{{LLaVA-Gemma}}},
  author = {Hinck, Musashi and Olson, Matthew L. and Cobbley, David and Tseng, Shao-Yen and Lal, Vasudev},
  year = {2024},
  month = mar,
  number = {arXiv:2404.01331},
  eprint = {2404.01331},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-23},
  abstract = {We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVAGemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparablysized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/SM6UKL5Q/Hinck et al. - 2024 - LLaVA-Gemma Accelerating Multimodal Foundation Models with a Compact Language Model.pdf}
}

@book{hirszowicz2016,
  title = {The {{Third Reich}} and the {{Arab East}}},
  author = {Hirszowicz, ?ukasz},
  year = {2016},
  series = {Routledge {{Library Editions}}: {{History}} of the {{Middle East}}},
  number = {v.13},
  publisher = {{Taylor and Francis}},
  address = {Florence},
  isbn = {978-1-315-40939-9},
  langid = {english}
}

@article{hitar-garcia2023,
  title = {Machine {{Learning Methods}} for {{Predicting}} {{{\emph{League}}}}{\emph{ of }}{{{\emph{Legends}}}} {{Game Outcome}}},
  author = {{Hitar-Garc{\'i}a}, Juan Agust{\'i}n and {Mor{\'a}n-Fern{\'a}ndez}, Laura and {Bol{\'o}n-Canedo}, Ver{\'o}nica},
  year = {2023},
  month = jun,
  journal = {IEEE Transactions on Games},
  volume = {15},
  number = {2},
  pages = {171--181},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TG.2022.3153086},
  urldate = {2024-01-25},
  abstract = {The video game League of Legends has several professional leagues and tournaments that offer prizes reaching several million dollars, making it one of the most followed games in the Esports scene. This article addresses the prediction of the winning team in professional matches of the game, using only pregame data. We propose to improve the accuracy of the models trained with the features offered by the game application programming interface (API). To this end, new features are built to collect interesting information, such as the skills of a player handling a certain champion, the synergies between players of the same team or the ability of a player to beat another player. Then, we perform feature selection and train different classification algorithms aiming at obtaining the best model. Experimental results show classification accuracy above 0.70, which is comparable to the results of other proposals presented in the literature, but with the added benefit of using few samples and not requiring the use of external sources to collect additional statistics.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/D3SBZ6Z6/Hitar-García et al. - 2023 - Machine Learning Methods for Predicting League of Legends Game Outcome.pdf}
}

@book{hitler2000,
  title = {Hitler's Table Talk, 1941-1944: His Private Conversations},
  shorttitle = {Hitler's Table Talk, 1941-1944},
  author = {Hitler, Adolf and Cameron, Norman and Stevens, R. H. and {Trevor-Roper}, H. R.},
  year = {2000},
  edition = {3rd ed},
  publisher = {Enigma Books},
  address = {New York City},
  isbn = {978-1-929631-05-6},
  langid = {english},
  lccn = {DD247.H5 A685 2000},
  keywords = {1933-1945,Germany,National socialism,Politics and government,World War 1939-1945},
  file = {/Users/nobr/Zotero/storage/2TPHN6I8/Hitler et al. - 2000 - Hitler's table talk, 1941-1944 his private conversations.pdf}
}

@misc{ho2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/RSKS4P3H/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@article{hochreiter1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2024-01-02},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/Users/nobr/Zotero/storage/R24UIPBZ/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf;/Users/nobr/Zotero/storage/VHUR4IM2/6795963.html}
}

@techreport{hofer2017,
  title = {({{No Title}})},
  author = {Hofer, Helmut and Wysocki, Krzysztof and Polyfold, Eduard Zehnder and Theory, Fredholm},
  year = {2017},
  eprint = {1707.08941v1},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/TCQA8AXZ/full-text.pdf}
}

@book{hoffman1998,
  title = {The Man Who Loved Only Numbers: The Story of {{Paul Erd{\"o}s}} and the Search for Mathematical Truth},
  shorttitle = {The Man Who Loved Only Numbers},
  author = {Hoffman, Paul},
  year = {1998},
  edition = {1. ed},
  publisher = {Hyperion},
  address = {New York, NY},
  isbn = {978-0-7868-6362-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ANW5JTI9/Hoffman - 1998 - The man who loved only numbers the story of Paul Erdös and the search for mathematical truth.pdf}
}

@book{hofstadter2000,
  title = {G{\"o}del, {{Escher}}, {{Bach}}: An Eternal Golden Braid},
  shorttitle = {G{\"o}del, {{Escher}}, {{Bach}}},
  author = {Hofstadter, Douglas R.},
  year = {2000},
  edition = {20th-anniversary ed},
  publisher = {Penguin},
  address = {London},
  isbn = {978-0-14-028920-6},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/AH935EXQ/GEB 20th Anniversary Edition.pdf}
}

@book{homer2006,
  title = {The {{Iliad}} of {{Homer}}},
  author = {Homer},
  year = {2006},
  publisher = {The Project Gutenberg},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DTHJMA9P/The Iliad of Homer.pdf}
}

@misc{hookerHardwareLottery2020,
  title = {The {{Hardware Lottery}}},
  author = {Hooker, Sara},
  year = {2020},
  month = sep,
  number = {arXiv:2009.06489},
  eprint = {2009.06489},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.06489},
  urldate = {2024-02-09},
  abstract = {Hardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which make it increasingly costly to stray off of the beaten path of research ideas. This essay posits that the gains from progress in computing are likely to become even more uneven, with certain research directions moving into the fast-lane while progress on others is further obstructed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Hardware Architecture,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/SBKGN5LL/Hooker - 2020 - The Hardware Lottery.pdf;/Users/nobr/Zotero/storage/GQ63PPWL/2009.html}
}

@article{horak2023,
  title = {Solving Zero-Sum One-Sided Partially Observable Stochastic Games},
  author = {Hor{\'a}k, Karel and Bo{\v s}ansk{\'y}, Branislav and Kova{\v r}{\'i}k, Vojt{\v e}ch and Kiekintveld, Christopher},
  year = {2023},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {316},
  pages = {103838},
  issn = {00043702},
  doi = {10.1016/j.artint.2022.103838},
  urldate = {2023-11-12},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2VLD58KK/Horák et al. - 2023 - Solving zero-sum one-sided partially observable stochastic games.pdf}
}

@misc{hortonLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Simulated Economic Agents}}: {{What Can We Learn}} from {{Homo Silicus}}?},
  shorttitle = {Large {{Language Models}} as {{Simulated Economic Agents}}},
  author = {Horton, John J.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.07543},
  eprint = {2301.07543},
  primaryclass = {econ, q-fin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.07543},
  urldate = {2023-12-07},
  abstract = {Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution.},
  archiveprefix = {arXiv},
  keywords = {Economics - General Economics},
  file = {/Users/nobr/Zotero/storage/LAPZD3PM/Horton - 2023 - Large Language Models as Simulated Economic Agents What Can We Learn from Homo Silicus.pdf;/Users/nobr/Zotero/storage/N8ZX5TVQ/2301.html}
}

@misc{hospedales2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year = {2020},
  month = nov,
  number = {arXiv:2004.05439},
  eprint = {2004.05439},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-02},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/X7AR333X/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf}
}

@misc{hosseiniYouNeedPay2024,
  title = {You {{Need}} to {{Pay Better Attention}}},
  author = {Hosseini, Mehran and Hosseini, Peyman},
  year = {2024},
  month = mar,
  number = {arXiv:2403.01643},
  eprint = {2403.01643},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.01643},
  urldate = {2024-05-29},
  abstract = {We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MNIST, CIFAR100, IMDB Movie Reviews, and Amazon Reviews datasets.},
  archiveprefix = {arXiv},
  keywords = {68T07 (Primary) 68T45 68T50 68T10 15A03 15A04 (Secondary),Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.10,I.2.6,I.2.7,I.4.0,I.5.0,I.7.0},
  file = {/Users/nobr/Zotero/storage/8G6X7XJJ/Hosseini and Hosseini - 2024 - You Need to Pay Better Attention.pdf}
}

@misc{hsiehToolDocumentationEnables2023,
  title = {Tool {{Documentation Enables Zero-Shot Tool-Usage}} with {{Large Language Models}}},
  author = {Hsieh, Cheng-Yu and Chen, Si-An and Li, Chun-Liang and Fujii, Yasuhisa and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and Pfister, Tomas},
  year = {2023},
  month = aug,
  number = {arXiv:2308.00675},
  eprint = {2308.00675},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-19},
  abstract = {Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation---descriptions for the individual tool usage---over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation. Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM [23] and Track Anything [70] models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/V6FERY5H/Hsieh et al. - 2023 - Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models.pdf}
}

@inproceedings{hu2014,
  title = {Digital Implementation of a Spiking Neural Network ({{SNN}}) Capable of Spike-Timing-Dependent Plasticity ({{STDP}}) Learning},
  booktitle = {14th {{IEEE International Conference}} on {{Nanotechnology}}},
  author = {Hu, Di and Zhang, Xu and Xu, Ziye and Ferrari, Silvia and Mazumder, Pinaki},
  year = {2014},
  month = aug,
  pages = {873--876},
  issn = {1944-9399},
  doi = {10.1109/NANO.2014.6968000},
  urldate = {2024-02-02},
  abstract = {The neural network model of computation has been proven to be faster and more energy-efficient than Boolean CMOS computations in numerous real-world applications. As a result, neuromorphic circuits have been garnering growing interest as the integration complexity within chips has reached several billion transistors. This article presents a digital implementation of a re-scalable spiking neural network (SNN) to demonstrate how spike timing-dependent plasticity (STDP) learning can be employed to train a virtual insect to navigate through a terrain with obstacles by processing information from the environment.},
  keywords = {CMOS integrated circuits,Insects,MATLAB,Neuromorphics,Robot sensing systems,Training},
  file = {/Users/nobr/Zotero/storage/MMJ5ZV5A/6968000.html}
}

@article{hu2020,
  title = {Racial {{Segregation}}, {{Testing Site Access}}, and {{COVID-19 Incidence Rate}} in {{Massachusetts}}, {{USA}}},
  author = {Hu, Tao and Yue, Han and Wang, Changzhen and She, Bing and Ye, Xinyue and Liu, Regina and Zhu, Xinyan and Guan, Weihe Wendy and Bao, Shuming},
  year = {2020},
  month = dec,
  journal = {International Journal of Environmental Research and Public Health},
  volume = {17},
  number = {24},
  pages = {9528},
  issn = {1660-4601},
  doi = {10.3390/ijerph17249528},
  urldate = {2023-07-04},
  abstract = {The U.S. has merely 4\% of the world population, but contains 25\% of the world's COVID-19 cases. Since the COVID-19 outbreak in the U.S., Massachusetts has been leading other states in the total number of COVID-19 cases. Racial residential segregation is a fundamental cause of racial disparities in health. Moreover, disparities of access to health care have a large impact on COVID-19 cases. Thus, this study estimates racial segregation and disparities in testing site access and employs economic, demographic, and transportation variables at the city/town level in Massachusetts. Spatial regression models are applied to evaluate the relationships between COVID-19 incidence rate and related variables. This is the first study to apply spatial analysis methods across neighborhoods in the U.S. to examine the COVID-19 incidence rate. The findings are: (1) Residential segregations of Hispanic and Non-Hispanic Black/African Americans have a significantly positive association with COVID-19 incidence rate, indicating the higher susceptibility of COVID-19 infections among minority groups. (2) Non-Hispanic Black/African Americans have the shortest drive time to testing sites, followed by Hispanic, Non-Hispanic Asians, and Non-Hispanic Whites. The drive time to testing sites is significantly negatively associated with the COVID-19 incidence rate, implying the importance of the accessibility of testing sites by all populations. (3) Poverty rate and road density are significant explanatory variables. Importantly, overcrowding represented by more than one person per room is a significant variable found to be positively associated with COVID-19 incidence rate, suggesting the effectiveness of social distancing for reducing infection. (4) Different from the findings of previous studies, the elderly population rate is not statistically significantly correlated with the incidence rate because the elderly population in Massachusetts is less distributed in the hotspot regions of COVID-19 infections. The findings in this study provide useful insights for policymakers to propose new strategies to contain the COVID-19 transmissions in Massachusetts.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HN5HCRGZ/Hu et al. - 2020 - Racial Segregation, Testing Site Access, and COVID.pdf}
}

@article{hu2022,
  title = {Subdivision-{{Based Mesh Convolution Networks}}},
  author = {Hu, Shi-Min and Liu, Zheng-Ning and Guo, Meng-Hao and Cai, Jun-Xiong and Huang, Jiahui and Mu, Tai-Jiang and Martin, Ralph R.},
  year = {2022},
  month = jun,
  journal = {ACM Transactions on Graphics},
  volume = {41},
  number = {3},
  eprint = {2106.02285},
  primaryclass = {cs},
  pages = {1--16},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3506694},
  urldate = {2023-11-28},
  abstract = {Convolutional neural networks (CNNs) have made great breakthroughs in 2D computer vision. However, their irregular structure makes it hard to harness the potential of CNNs directly on meshes. A subdivision surface provides a hierarchical multi-resolution structure, in which each face in a closed 2-manifold triangle mesh is exactly adjacent to three faces. Motivated by these two observations, this paper presents SubdivNet, an innovative and versatile CNN framework for 3D triangle meshes with Loop subdivision sequence connectivity. Making an analogy between mesh faces and pixels in a 2D image allows us to present a mesh convolution operator to aggregate local features from nearby faces. By exploiting face neighborhoods, this convolution can support standard 2D convolutional network concepts, e.g. variable kernel size, stride, and dilation. Based on the multi-resolution hierarchy, we make use of pooling layers which uniformly merge four faces into one and an upsampling method which splits one face into four. Thereby, many popular 2D CNN architectures can be easily adapted to process 3D meshes. Meshes with arbitrary connectivity can be remeshed to have Loop subdivision sequence connectivity via self-parameterization, making SubdivNet a general approach. Extensive evaluation and various applications demonstrate SubdivNet's effectiveness and efficiency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,I.3.5},
  file = {/Users/nobr/Zotero/storage/JJNZGY6A/Hu et al. - 2022 - Subdivision-Based Mesh Convolution Networks.pdf}
}

@article{hu2023a,
  title = {Game-Based {{Platforms}} for {{Artificial Intelligence Research}}},
  author = {Hu, Chengpeng and Zhao, Yunlong and Wang, Ziqi and Du, Haocheng and Liu, Jialin},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2304.13269},
  urldate = {2023-12-15},
  abstract = {Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous open-source games or game-based environments have been implemented for studying artificial intelligence. In addition to single- or multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, discusses the research trend induced by the evolution of those platforms, and gives an outlook.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences},
  file = {/Users/nobr/Zotero/storage/Z9P9VDXW/Hu et al. - 2023 - Game-based Platforms for Artificial Intelligence Research.pdf}
}

@article{hu2024,
  title = {Decentralized Graph-Based Multi-Agent Reinforcement Learning Using Reward Machines},
  author = {Hu, Jueming and Xu, Zhe and Wang, Weichang and Qu, Guannan and Pang, Yutian and Liu, Yongming},
  year = {2024},
  month = jan,
  journal = {Neurocomputing},
  volume = {564},
  pages = {126974},
  issn = {09252312},
  doi = {10.1016/j.neucom.2023.126974},
  urldate = {2023-11-12},
  abstract = {In multi-agent reinforcement learning (MARL), it is challenging for a collection of agents to learn complex temporally extended tasks. The difficulties lie in computational complexity and how to learn the high-level ideas behind reward functions. We study the graph-based Markov Decision Process (MDP), where the dynamics of neighboring agents are coupled. To learn complex temporally extended tasks, we use a reward machine (RM) to encode each agent's task and expose reward function internal structures. RM has the capacity to describe high-level knowledge and encode non-Markovian reward functions. We propose a decentralized learning algorithm to tackle computational complexity, called decentralized graph-based reinforcement learning using reward machines (DGRM), that equips each agent with a localized policy, allowing agents to make decisions independently based on the information available to the agents. DGRM uses the actor-critic structure, and we introduce the tabular Q-function for discrete state problems. We show that the dependency of the Q-function on other agents decreases exponentially as the distance between them increases. To further improve efficiency, we also propose the deep DGRM algorithm, using deep neural networks to approximate the Q-function and policy function to solve large-scale or continuous state problems. The effectiveness of the proposed DGRM algorithm is evaluated by three case studies, two wireless communication case studies with independent and dependent reward functions, respectively, and COVID-19 pandemic mitigation. Experimental results show that local information is sufficient for DGRM and agents can accomplish complex tasks with the help of RM. DGRM improves the global accumulated reward by 119\% compared to the baseline in the case of COVID-19 pandemic mitigation.},
  langid = {english}
}

@misc{hu2024b,
  title = {A {{Survey}} on {{Large Language Model-Based Game Agents}}},
  author = {Hu, Sihao and Huang, Tiansheng and Ilhan, Fatih and Tekin, Selim and Liu, Gaowen and Kompella, Ramana and Liu, Ling},
  year = {2024},
  month = apr,
  number = {arXiv:2404.02039},
  eprint = {2404.02039},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.02039},
  urldate = {2024-12-03},
  abstract = {The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting \& exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/LM3P93FS/Hu et al. - 2024 - A Survey on Large Language Model-Based Game Agents.pdf;/Users/nobr/Zotero/storage/JFN9JVHJ/2404.html}
}

@misc{hu2024c,
  title = {Games for {{Artificial Intelligence Research}}: {{A Review}} and {{Perspectives}}},
  shorttitle = {Games for {{Artificial Intelligence Research}}},
  author = {Hu, Chengpeng and Zhao, Yunlong and Wang, Ziqi and Du, Haocheng and Liu, Jialin},
  year = {2024},
  month = jun,
  number = {arXiv:2304.13269},
  eprint = {2304.13269},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.13269},
  urldate = {2025-08-08},
  abstract = {Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous opensource games or game-based environments have been implemented for studying artificial intelligence. In addition to singleor multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, provides guidance on matching particular types of artificial intelligence with suitable games for testing and matching particular needs in games with suitable artificial intelligence techniques, discusses the research trend induced by the evolution of those platforms, and gives an outlook.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/2H9EVG4V/Hu et al. - 2024 - Games for Artificial Intelligence Research A Review and Perspectives.pdf}
}

@incollection{huang2001,
  title = {Multiscale {{Graphical Modeling}} in {{Space}}: {{Applications}} to {{Command}} and {{Control}}},
  shorttitle = {Multiscale {{Graphical Modeling}} in {{Space}}},
  booktitle = {Spatial {{Statistics}}: {{Methodological Aspects}} and {{Applications}}},
  author = {Huang, Hsin-Cheng and Cressie, Noel},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Moore, Marc},
  year = {2001},
  volume = {159},
  pages = {83--113},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4613-0147-9_5},
  urldate = {2023-11-22},
  abstract = {Recently, a class of multiscale tree-structured models was introduced in terms of scale-recursive dynamics defined on trees. The main advantage of these models is their association with a fast, recursive, Kalmanfilter prediction algorithm. In this article, we propose a more general class of multiscale graphical models over acyclic directed graphs, for use in command and control problems. Moreover, we derive the generalized-Kalmanfilter algorithm for graphical Markov models, which can be used to obtain the optimal predictors and prediction variances for multiscale graphical models.},
  isbn = {978-0-387-95240-6 978-1-4613-0147-9},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7C6DLU9U/Huang and Cressie - 2001 - Multiscale Graphical Modeling in Space Applications to Command and Control.pdf}
}

@article{huang2021,
  title = {Memristive {{Artificial Synapses}} for {{Neuromorphic Computing}}},
  author = {Huang, Wen and Xia, Xuwen and Zhu, Chen and Steichen, Parker and Quan, Weidong and Mao, Weiwei and Yang, Jianping and Chu, Liang and Li, Xing'ao},
  year = {2021},
  month = mar,
  journal = {Nano-Micro Letters},
  volume = {13},
  number = {1},
  pages = {85},
  issn = {2150-5551},
  doi = {10.1007/s40820-021-00618-2},
  urldate = {2024-02-02},
  abstract = {Synaptic devices that mimic synaptic functions are discussed by categorizing them into electrically stimulated, optically stimulated, and photoelectric synergetic synaptic devices based on stimulation of electrical and optical signals.The working mechanisms, progress, and application scenarios of synaptic devices based on electrical and optical signals are compared and analyzed.The performances and future development of various synaptic devices that could be significant for building efficient neuromorphic systems are prospected.},
  langid = {english},
  keywords = {Electrical pulses,Neuromorphic computing,Optical pulses,Photoelectric synergetic effects,Synaptic devices},
  file = {/Users/nobr/Zotero/storage/M4AUGLE7/Huang et al. - 2021 - Memristive Artificial Synapses for Neuromorphic Computing.pdf}
}

@misc{huang2022,
  title = {Inner Monologue: {{Embodied}} Reasoning through Planning with Language Models},
  author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
  year = {2022},
  eprint = {2207.05608},
  primaryclass = {cs.RO},
  archiveprefix = {arXiv}
}

@misc{huangCanLargeLanguage2023,
  title = {Can {{Large Language Models Explain Themselves}}? {{A Study}} of {{LLM-Generated Self-Explanations}}},
  shorttitle = {Can {{Large Language Models Explain Themselves}}?},
  author = {Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.11207},
  eprint = {2310.11207},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.11207},
  urldate = {2024-02-02},
  abstract = {Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/ZJG76GSZ/Huang et al. - 2023 - Can Large Language Models Explain Themselves A Study of LLM-Generated Self-Explanations.pdf;/Users/nobr/Zotero/storage/8GFFFHVS/2310.html}
}

@misc{hubingerSleeperAgentsTraining2024,
  title = {Sleeper {{Agents}}: {{Training Deceptive LLMs}} That {{Persist Through Safety Training}}},
  shorttitle = {Sleeper {{Agents}}},
  author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and DasSarma, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.05566},
  eprint = {2401.05566},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-05},
  abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/nobr/Zotero/storage/922AKXTQ/Hubinger et al. - 2024 - Sleeper Agents Training Deceptive LLMs that Persist Through Safety Training.pdf;/Users/nobr/Zotero/storage/7KYJ4VM4/2401.html}
}

@misc{huGAIA1GenerativeWorld2023,
  title = {{{GAIA-1}}: {{A Generative World Model}} for {{Autonomous Driving}}},
  shorttitle = {{{GAIA-1}}},
  author = {Hu, Anthony and Russell, Lloyd and Yeo, Hudson and Murez, Zak and Fedoseev, George and Kendall, Alex and Shotton, Jamie and Corrado, Gianluca},
  year = {2023},
  month = sep,
  number = {arXiv:2309.17080},
  eprint = {2309.17080},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.17080},
  urldate = {2023-11-11},
  abstract = {Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves. To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representation that captures expectations of future events, combined with its ability to generate realistic samples, provides new possibilities for innovation in the field of autonomy, enabling enhanced and accelerated training of autonomous driving technology.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/UFE2KL9R/Hu et al. - 2023 - GAIA-1 A Generative World Model for Autonomous Driving.pdf;/Users/nobr/Zotero/storage/DFHTDSRU/2309.html}
}

@article{hui2007,
  title = {Measuring the Neighboring and Environmental Effects on Residential Property Value: {{Using}} Spatial Weighting Matrix},
  shorttitle = {Measuring the Neighboring and Environmental Effects on Residential Property Value},
  author = {Hui, Eddie C.M. and Chau, C.K. and Pun, Lilian and Law, M.Y.},
  year = {2007},
  month = jun,
  journal = {Building and Environment},
  volume = {42},
  number = {6},
  pages = {2333--2343},
  issn = {03601323},
  doi = {10.1016/j.buildenv.2006.05.004},
  urldate = {2023-07-04},
  abstract = {The relationship between house price and location is very complicated. Traditional theory mainly emphasizes the relationship between the effects of accessibility to central locations on housing prices and ignores location-specific attributes of housing. Previous studies at best examine only one or a few environmental amenities. However, little has been done on a high-rise, densely populated living environment. This paper thus aims to investigate the ``neighboring and environmental characteristics'' of a residential property on its market value in such unique setting. It uses a more advanced approach---i.e. a hedonic model with spatial adjustments---and applies GIS techniques. The findings are mostly consistent with our hypotheses. The traveling time from the apartment to the central business district is negatively correlated with housing price. Households are also willing to pay more for apartments with sea view and better air. However, greenbelt is not a significant variable on housing price and noise level is even found to be positively correlated with the price. This unexpected finding may be explained by the uniquely dense living environment in Hong Kong where households are willing to sacrifice serenity for convenience. The current research is adequately sophisticated and has contributed to a better understanding of the important theoretical question of how much a household is willing to spend on various environmental and neighbourhood attributes of an apartment.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/95BN4C9L/Hui et al. - 2007 - Measuring the neighboring and environmental effect.pdf}
}

@misc{humayunDeepNetworksAlways2024,
  title = {Deep {{Networks Always Grok}} and {{Here}} Is {{Why}}},
  author = {Humayun, Ahmed Imtiaz and Balestriero, Randall and Baraniuk, Richard},
  year = {2024},
  month = jun,
  number = {arXiv:2402.15555},
  eprint = {2402.15555},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.15555},
  urldate = {2024-06-16},
  abstract = {Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on the local complexity of a DNN's input-output mapping. Our local complexity measures the density of so-called linear regions (aka, spline partition regions) that tile the DNN input space and serves as a utile progress measure for training. We provide the first evidence that, for classification problems, the linear regions undergo a phase transition during training whereafter they migrate away from the training samples (making the DNN mapping smoother there) and towards the decision boundary (making the DNN mapping less smooth there). Grokking occurs post phase transition as a robust partition of the input space thanks to the linearization of the DNN mapping around the training points. Website: https://bit.ly/grok-adversarial},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/N8XPVRQJ/Humayun et al. - 2024 - Deep Networks Always Grok and Here is Why.pdf}
}

@article{hunt2019,
  title = {Challenges and {{Opportunities}} in {{Connectome Construction}} and {{Quantification}} in the {{Developing Human Fetal Brain}}},
  author = {Hunt, David and Dighe, Manjiri and Gatenby, Christopher and Studholme, Colin},
  year = {2019},
  month = oct,
  journal = {Topics in magnetic resonance imaging : TMRI},
  volume = {28},
  number = {5},
  pages = {265--273},
  issn = {0899-3459},
  doi = {10.1097/RMR.0000000000000212},
  urldate = {2023-05-13},
  abstract = {Objectives: The white matter structure of the human brain undergoes critical developmental milestones in utero, which we can observe non-invasively using diffusion-weighted magnetic resonance imaging. In order to understand this fascinating developmental process, we must establish the variability inherent in such a challenging imaging environment and how measurable quantities can be transformed into meaningful connectomes. Methods: We review techniques for reconstructing and studying the brain connectome and explore promising opportunities for in utero studies that could lead to more accurate measurement of structural properties and allow for more refined and insightful analyses of the fetal brain. Results: Opportunities for more sophisticated analyses of the properties of the brain and its dynamic changes have emerged in recent years, based on the development of iterative techniques to reconstruct motion-corrupted diffusion-weighted data. While reconstruction quality is greatly improved, the treatment of fundamental quantities like edge strength require careful treatment because of the specific challenges of imaging in utero. Conclusions: There are intriguing challenges to overcome, from those in analysis due to both imaging limitations and the significant changes in structural connectivity, to further image processing to address the specific properties of the target anatomy and quantification into a developmental connectome.},
  pmcid = {PMC6788770},
  pmid = {31592993},
  file = {/Users/nobr/Zotero/storage/4LR7G7DH/Hunt et al. - 2019 - Challenges and Opportunities in Connectome Constru.pdf}
}

@misc{huPokeLLMonHumanParityAgent2024,
  title = {Pok{\textbackslash}'{{eLLMon}}: {{A Human-Parity Agent}} for {{Pok}}{\textbackslash}'emon {{Battles}} with {{Large Language Models}}},
  shorttitle = {Pok{\textbackslash}'{{eLLMon}}},
  author = {Hu, Sihao and Huang, Tiansheng and Liu, Ling},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01118},
  eprint = {2402.01118},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-16},
  abstract = {We introduce {\textbackslash}textsc\{Pok{\textbackslash}'eLLMon\}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok{\textbackslash}'emon battles. The design of {\textbackslash}textsc\{Pok{\textbackslash}'eLLMon\} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the {\textbackslash}textit\{panic switching\} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates {\textbackslash}textsc\{Pok{\textbackslash}'eLLMon\}'s human-like battle strategies and just-in-time decision making, achieving 49{\textbackslash}\% of win rate in the Ladder competitions and 56{\textbackslash}\% of win rate in the invited battles. Our implementation and playable battle logs are available at: {\textbackslash}url\{https://github.com/git-disl/PokeLLMon\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/Q9IUPDSJ/Hu et al. - 2024 - Pok'eLLMon A Human-Parity Agent for Pok'emon Battles with Large Language Models.pdf;/Users/nobr/Zotero/storage/E8VRZVJQ/2402.html}
}

@misc{ibama,
  title = {Instituto Brasileiro Do Meio Ambiente e Dos Recursos Naturais Renov{\'a}veis}
}

@misc{icao_corsia,
  title = {Carbon Offsetting and Reduction Scheme for International Aviation ({{CORSIA}})}
}

@misc{iea_ccus,
  title = {Carbon Capture, Utilization, and Storage}
}

@book{igual2017,
  title = {Introduction to {{Data Science}}},
  author = {Igual, Laura and Segu{\'i}, Santi},
  year = {2017},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-50017-1},
  urldate = {2023-11-09},
  isbn = {978-3-319-50016-4 978-3-319-50017-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JAN5TQFY/Igual and Seguí - 2017 - Introduction to Data Science.pdf}
}

@article{iii2014,
  title = {Resilient {{Command}} and {{Control}}: {{The Need}} for {{Distributed Control}}},
  author = {Iii, Gilmary Michael Hostage and Broadwell, Larry R},
  year = {2014},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4HLU7IDG/Iii and Broadwell - 2014 - Resilient Command and Control The Need for Distributed Control.pdf}
}

@misc{imf_emissions,
  title = {{{IMF}} Fossil Fuel Subsidies Data}
}

@misc{imo_shipping,
  title = {Low Carbon Shipping and Air Pollution Control}
}

@misc{india_renewable,
  title = {Website of the Ministry of New and Renewable Energy}
}

@techreport{indonesia2016ndc,
  title = {First Nationally Determined Contribution Republic of Indonesia},
  author = {{Republic of Indonesia}},
  year = {2016},
  month = nov,
  institution = {Republic of Indonesia / UNFCCC}
}

@article{innes2018,
  title = {On {{Machine Learning}} and {{Programming Languages}}},
  author = {Innes, Mike and Karpinski, Stefan and Shah, Viral and Barber, David and Stenetorp, Pontus and Besard, Tim and Bradbury, James and Churavy, Valentin and Danisch, Simon and Edelman, Alan and Malmaud, Jon and Revels, Jarrett and Yuret, Deniz},
  year = {2018},
  abstract = {The complexity of Machine Learning (ML) models and the frameworks people are using to build them has exploded along with ML itself. State-of-the-art models are increasingly programs, with support for programming constructs like loops and recursion, and this brings out many interesting issues in the tools we use to create them --- that is, programming languages (PL). This paper1, discusses the necessity for a first class language for machine learning, and what such a language might look like.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DSUTZG25/Innes et al. - On Machine Learning and Programming Languages.pdf}
}

@misc{inpe2018forest,
  title = {Forest Fires Dominate Carbon Emissions during Droughts in the {{Amazon}}},
  author = {{INPE}},
  year = {2018},
  month = feb
}

@article{internationalspaceelevatorconsortium2023,
  title = {Building the {{Space Elevator Tether}}},
  author = {{International Space Elevator Consortium} and Wright, Dennis H},
  year = {2023},
  month = oct,
  journal = {Journal of the British Interplanetary Society},
  volume = {76},
  number = {7},
  pages = {225--231},
  issn = {0007084X},
  doi = {10.59332/jbis-076-07-0225},
  urldate = {2025-03-23},
  abstract = {A simple Earth-based space elevator consists of a cable or tether extending from the surface to well beyond geosynchronous orbit. A climber grips the tether, pulling itself upward and delivering payload to space. For a space elevator to be feasible, the material from which the tether is built must be strong enough that the tether can support itself and any climbers that ascend it. Three such materials exist today and their industrial-scale production seems near at hand. These materials are compared and, based on previous studies, required improvements for each are listed. The material properties, along with the load, determine the shape and mass of the tether. Two example tether shapes are shown: one using the constant stress model and another which takes into account atmospheric winds and orbital debris. Options for tether construction are discussed. These are influenced by the material fabrication process, the way in which materials are combined and arranged in the tether and how much of the fabrication takes place on Earth or in space.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RQZ8ZVWR/International Space Elevator Consortium and Wright - 2023 - Building the Space Elevator Tether.pdf}
}

@article{iovino2022,
  title = {A Survey of {{Behavior Trees}} in Robotics and {{AI}}},
  author = {Iovino, Matteo and Scukins, Edvards and Styrud, Jonathan and {\"O}gren, Petter and Smith, Christian},
  year = {2022},
  month = aug,
  journal = {Robotics and Autonomous Systems},
  volume = {154},
  pages = {104096},
  issn = {09218890},
  doi = {10.1016/j.robot.2022.104096},
  urldate = {2023-11-12},
  abstract = {Behavior Trees (BTs) were invented as a tool to enable modular AI in computer games, but have received an increasing amount of attention in the robotics community in the last decade. With rising demands on agent AI complexity, game programmers found that the Finite State Machines (FSM) that they used scaled poorly and were difficult to extend, adapt and reuse.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/A9YMGHUF/Iovino et al. - 2022 - A survey of Behavior Trees in robotics and AI.pdf}
}

@incollection{IPCC_AR5_Chapter12_2013,
  title = {Long-Term Climate Change: {{Projections}}, Commitments and Irreversibility},
  booktitle = {Climate Change 2013: {{The}} Physical Science Basis. {{Contribution}} of Working Group {{I}} to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change},
  author = {Collins, M. and Knutti, R. and Arblaster, J. and Dufresne, J.-L. and Fichefet, T. and Friedlingstein, P. and Gao, X. and Gutowski, W.J. and Johns, T. and Krinner, G. and Shongwe, M. and Tebaldi, C. and Weaver, A.J. and Wehner, M.},
  editor = {Stocker, T.F. and Qin, D. and Plattner, G.-K. and Tignor, M. and Allen, S.K. and Boschung, J. and Nauels, A. and Xia, Y. and Bex, V. and Midgley, P.M.},
  year = {2013},
  pages = {1029--1136},
  publisher = {Cambridge University Press},
  address = {Cambridge, United Kingdom and New York, NY, USA}
}

@misc{ipcc2006guidelines,
  title = {2006 {{IPCC}} Guidelines for National Greenhouse Gas Inventories},
  author = {{IPCC}},
  year = {2006},
  publisher = {Intergovernmental Panel on Climate Change}
}

@book{IPCC2007,
  title = {Climate Change 2007: {{Synthesis}} Report. {{Contribution}} of Working Groups {{I}}, {{II}} and {{III}} to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change},
  author = {{Intergovernmental Panel on Climate Change (IPCC)}},
  year = {2007},
  publisher = {IPCC},
  address = {Geneva, Switzerland}
}

@incollection{IPCC2013_Chapter12,
  title = {Long-Term Climate Change: {{Projections}}, Commitments and Irreversibility},
  booktitle = {Climate Change 2013: {{The}} Physical Science Basis. {{Contribution}} of Working Group {{I}} to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change},
  author = {Collins, Matthew and Knutti, Reto and Arblaster, Julie and Dufresne, Jean-Louis and Fichefet, Thierry and Friedlingstein, Pierre and Gao, Xuejie and Gutowski, William J. and Johns, Tim and Krinner, Gerhard and Shongwe, Mxolisi and Tebaldi, Claudia and Weaver, Andrew J. and Wehner, Michael},
  editor = {Stocker, Thomas F. and Qin, Dahe and Plattner, Gian-Kasper and Tignor, Melinda and Allen, Simon K. and Boschung, Judith and Nauels, Alexander and Xia, Yu and Bex, Vincent and Midgley, Pauline M.},
  year = {2013},
  pages = {1029--1136},
  publisher = {Cambridge University Press},
  address = {Cambridge, United Kingdom and New York, NY, USA}
}

@techreport{ipcc2013climate,
  title = {Climate Change 2013: {{The}} Physical Science Basis. {{Contribution}} of Working Group {{I}} to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change},
  author = {{IPCC}},
  year = {2013},
  pages = {1535},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  institution = {Cambridge University Press / Intergovernmental Panel on Climate Change},
  collaborator = {Stocker, T. F. and Qin, D. and Plattner, G.-K. and Tignor, M. and Allen, S. K. and Boschung, J. and Nauels, A. and Xia, Y. and Bex, V. and Midgley, P. M.}
}

@incollection{IPCC2013SPM,
  title = {Summary for Policymakers},
  booktitle = {Climate Change 2013: {{The}} Physical Science Basis. {{Contribution}} of Working Group {{I}} to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change},
  author = {{Intergovernmental Panel on Climate Change (IPCC)}},
  editor = {Stocker, T.F. and Qin, D. and Plattner, G.-K. and Tignor, M. and Allen, S.K. and Boschung, J. and Nauels, A. and Xia, Y. and Bex, V. and Midgley, P.M.},
  year = {2013},
  pages = {1--30},
  publisher = {Cambridge University Press},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  doi = {10.1017/CBO9781107415324.004}
}

@book{IPCC2014SYR,
  title = {Climate Change 2014: {{Synthesis}} Report. {{Summary}} for Policymakers},
  author = {Team, Core Writing and Pachauri, R.K. and Meyer, L.A.},
  editor = {Pachauri, R.K. and Meyer, L.A.},
  year = {2014},
  publisher = {Intergovernmental Panel on Climate Change},
  address = {Geneva, Switzerland}
}

@techreport{ipcc2018sr15,
  title = {Global {{Warming}} of 1.5{$^\circ$}{{C}}. {{An IPCC Special Report}} on the Impacts of Global Warming of 1.5{$^\circ$}{{C}} above Pre-Industrial Levels and Related Global Greenhouse Gas Emission Pathways, in the Context of Strengthening the Global Response to the Threat of Climate Change, Sustainable Development, and Efforts to Eradicate Poverty},
  author = {{IPCC}},
  year = {2018},
  address = {Geneva, Switzerland},
  institution = {IPCC},
  collaborator = {{Masson-Delmotte}, V. and Zhai, P. and P{\"o}rtner, H.-O. and Roberts, D. and Skea, J. and Shukla, P. R. and Pirani, A. and {Moufouma-Okia}, W. and P{\'e}an, C. and Pidcock, R. and Connors, S. and Matthews, J. B. R. and Chen, Y. and Zhou, X. and Gomis, M. I. and Lonnoy, E. and Maycock, T. and Tignor, M. and Waterfield, T.}
}

@techreport{ipcc2018summary,
  title = {Summary for Policymakers},
  author = {{IPCC}},
  year = {2018},
  journal = {Global Warming of 1.5{$^\circ$}C. An IPCC Special Report on the impacts of global warming of 1.5{$^\circ$}C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable development, and efforts to eradicate poverty}
}

@article{irie2022,
  title = {A {{Modern Self-Referential Weight Matrix That Learns}} to {{Modify Itself}}},
  author = {Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  year = {2022},
  month = feb,
  eprint = {2202.05780},
  abstract = {The weight matrix (WM) of a neural network (NN) is its program. The programs of many traditional NNs are learned through gradient descent in some error function, then remain fixed. The WM of a self-referential NN, however, can keep rapidly modifying all of itself during runtime. In principle, such NNs can meta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, in the sense of recursive self-improvement. While NN architectures potentially capable of implementing such behaviour have been proposed since the '90s, there have been few if any practical studies. Here we revisit such NNs, building upon recent successes of fast weight programmers and closely related linear Transformers. We propose a scalable self-referential WM (SRWM) that learns to use outer products and the delta update rule to modify itself. We evaluate our SRWM in supervised few-shot learning and in multi-task reinforcement learning with procedurally generated game environments. Our experiments demonstrate both practical applicability and competitive performance of the proposed SRWM. Our code is public.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/K57BTPDY/Irie et al. - 2022 - A Modern Self-Referential Weight Matrix That Learns to Modify Itself.pdf}
}

@article{isella2011,
  title = {What's in a Crowd? {{Analysis}} of Face-to-Face Behavioral Networks},
  shorttitle = {What's in a Crowd?},
  author = {Isella, Lorenzo and Stehl{\'e}, Juliette and Barrat, Alain and Cattuto, Ciro and Pinton, Jean-Fran{\c c}ois and den Broeck, Wouter Van},
  year = {2011},
  month = feb,
  journal = {Journal of Theoretical Biology},
  volume = {271},
  number = {1},
  eprint = {1006.1260},
  primaryclass = {physics, q-bio},
  pages = {166--180},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2010.11.033},
  urldate = {2023-06-14},
  abstract = {The availability of new data sources on human mobility is opening new avenues for investigating the interplay of social networks, human mobility and dynamical processes such as epidemic spreading. Here we analyze data on the time-resolved face-to-face proximity of individuals in large-scale real-world scenarios. We compare two settings with very different properties, a scientific conference and a long-running museum exhibition. We track the behavioral networks of face-to-face proximity, and characterize them from both a static and a dynamic point of view, exposing important differences as well as striking similarities. We use our data to investigate the dynamics of a susceptible-infected model for epidemic spreading that unfolds on the dynamical networks of human proximity. The spreading patterns are markedly different for the conference and the museum case, and they are strongly impacted by the causal structure of the network data. A deeper study of the spreading paths shows that the mere knowledge of static aggregated networks would lead to erroneous conclusions about the transmission paths on the dynamical networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Physics and Society,Quantitative Biology - Other Quantitative Biology},
  file = {/Users/nobr/Zotero/storage/ESAJDVB8/Isella et al. - 2011 - What's in a crowd Analysis of face-to-face behavi.pdf;/Users/nobr/Zotero/storage/E3CLAV3I/1006.html}
}

@article{ishibashi2024,
  title = {Self-{{Organized Agents}}: {{A LLM Multi-Agent Framework}} toward {{Ultra Large-Scale Code Generation}} and {{Optimization}}},
  shorttitle = {Self-{{Organized Agents}}},
  author = {Ishibashi, Yoichi and Nishimura, Yoshimasa},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2404.02183},
  urldate = {2024-12-03},
  abstract = {Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5\% in terms of Pass@1 accuracy.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),Multiagent Systems (cs.MA),Software Engineering (cs.SE)},
  file = {/Users/nobr/Zotero/storage/4QAESS2E/Ishibashi and Nishimura - 2024 - Self-Organized Agents A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Opti.pdf}
}

@misc{isw,
  title = {Institute for the {{Study}} of {{War}}},
  journal = {Institute for the Study of War},
  urldate = {2024-06-04},
  howpublished = {https://www.understandingwar.org/},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/KFFCDMEN/www.understandingwar.org.html}
}

@article{jackson2024,
  title = {The Price of a Cigarette: 20 Minutes of Life?},
  shorttitle = {The Price of a Cigarette},
  author = {Jackson, Sarah E. and Jarvis, Martin J. and West, Robert},
  year = {2024},
  month = dec,
  journal = {Addiction},
  pages = {add.16757},
  issn = {0965-2140, 1360-0443},
  doi = {10.1111/add.16757},
  urldate = {2025-02-28},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HF4D9JUL/Jackson et al. - 2024 - The price of a cigarette 20 minutes of life.pdf}
}

@book{jacobsen2024,
  title = {Greenland in {{Arctic Security}}: ({{De}})Securitization {{Dynamics}} under {{Climatic Thaw}} and {{Geopolitical Freeze}}},
  shorttitle = {Greenland in {{Arctic Security}}},
  editor = {Jacobsen, Marc and Gad, Ulrik Pram and W{\ae}ver, Ole},
  year = {2024},
  edition = {12676130},
  publisher = {University of Michigan Press},
  address = {Ann Arbor, MI},
  doi = {10.3998/mpub.12676130},
  urldate = {2025-08-02},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
  isbn = {978-0-472-07670-3 978-0-472-90439-6},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/W5BN7S9B/Jacobsen et al. - 2024 - Greenland in Arctic Security (De)securitization Dynamics under Climatic Thaw and Geopolitical Freez.pdf}
}

@article{jaderberg,
  title = {Open-{{Ended}} Learning Leads to {{Generally Capable Agents}}},
  author = {Jaderberg, Max and Mathieu, Michael and McAleese, Nat and {Bradley-Schmieg}, Nathalie and Wong, Nathaniel and Porcel, Nicolas and {Hughes-Fitt}, Steph and Dalibard, Valentin and Czarnecki, Wojciech Marian},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/FQ6GQH4M/Jaderberg et al. - Open-Ended learning leads to Generally Capable Agents.pdf}
}

@article{jaderberg2019,
  title = {Human-Level Performance in {{3D}} Multiplayer Games with Population-Based Reinforcement Learning},
  author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Casta{\~n}eda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
  year = {2019},
  journal = {Science},
  volume = {364},
  number = {6443},
  pages = {859--865},
  doi = {10.1126/science.aau6249},
  file = {/Users/nobr/Zotero/storage/K3CZFSK9/Jaderberg et al. - 2019 - Human-level performance in 3D multiplayer games with population-based reinforcement learning.pdf;/Users/nobr/Zotero/storage/ZJXPNR2P/Jaderberg et al. - 2019 - Human-level performance in 3D multiplayer games with population-based reinforcement learning.pdf}
}

@article{jain2022,
  title = {How to {{Do}} a {{Vocab Swap}}? {{A Study}} of {{Embedding Replacement}} for {{Pre-trained Transformers}}},
  shorttitle = {How to {{Do}} a {{Vocab Swap}}?},
  author = {Jain, Neel and Kirchenbauer, John and Geiping, Jonas and Goldstein, Tom},
  year = {2022},
  month = sep,
  urldate = {2023-12-21},
  abstract = {There are a wide range of different tokenizers and vocabularies that have been used to train language models, and training a language model on just one of these can be prohibitively expensive. The ability to swap the vocabulary of a model after it has been trained enables models to be adapted to different tokenizers, and even different languages, without the computational or data cost of from-scratch training. In this paper, we ask when such swaps are possible, and how to perform them effectively? The major challenge of performing a vocab swap is re-learning the parameters of the embedding layer for the vocabulary. We observe that it is possible to re-learn the embedding for a vocabulary using a naive initialization, and we investigate strong initialization strategies that enable learning of new embeddings for swapped vocabularies, even when those vocabularies come from a different source language than the original language model.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/8RU82T9J/Jain et al. - 2022 - How to Do a Vocab Swap A Study of Embedding Replacement for Pre-trained Transformers.pdf}
}

@misc{jairbolsonaro2014,
  title = {{{SIRKIS DIZ QUE EM}} 10 {{ANOS A}} "{{DEMOCRACIA}}" {{CUBANA SER{\'A} CAPITALISTA}}},
  author = {{Jair Bolsonaro}},
  year = {2014},
  month = nov,
  urldate = {2025-05-12}
}

@book{james2013,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4614-7138-7},
  urldate = {2023-11-09},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/VF73SG9H/James et al. - 2013 - An Introduction to Statistical Learning.pdf}
}

@misc{jannerOfflineReinforcementLearning2021,
  title = {Offline {{Reinforcement Learning}} as {{One Big Sequence Modeling Problem}}},
  author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  year = {2021},
  month = nov,
  number = {arXiv:2106.02039},
  eprint = {2106.02039},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.02039},
  urldate = {2023-12-21},
  abstract = {Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/EV794QST/Janner et al. - 2021 - Offline Reinforcement Learning as One Big Sequence Modeling Problem.pdf;/Users/nobr/Zotero/storage/R8C53WKQ/2106.html}
}

@article{janowicz2020,
  title = {{{GeoAI}}: Spatially Explicit Artificial Intelligence Techniques for Geographic Knowledge Discovery and Beyond},
  shorttitle = {{{GeoAI}}},
  author = {Janowicz, Krzysztof and Gao, Song and McKenzie, Grant and Hu, Yingjie and Bhaduri, Budhendra},
  year = {2020},
  month = apr,
  journal = {International Journal of Geographical Information Science},
  volume = {34},
  number = {4},
  pages = {625--636},
  publisher = {Taylor \& Francis},
  issn = {1365-8816},
  doi = {10.1080/13658816.2019.1684500},
  urldate = {2023-07-10},
  file = {/Users/nobr/Zotero/storage/D59IXTPS/Janowicz et al. - 2020 - GeoAI spatially explicit artificial intelligence .pdf}
}

@article{janowicz2022,
  title = {{{GeoAI}}, Counter-{{AI}}, and Human Geography: {{A}} Conversation},
  shorttitle = {{{GeoAI}}, Counter-{{AI}}, and Human Geography},
  author = {Janowicz, Krzysztof and Sieber, Ren{\'e}e and Crampton, Jeremy},
  year = {2022},
  month = nov,
  journal = {Dialogues in Human Geography},
  volume = {12},
  number = {3},
  pages = {446--458},
  publisher = {SAGE Publications},
  issn = {2043-8206},
  doi = {10.1177/20438206221132510},
  urldate = {2023-07-10},
  abstract = {This conversation inaugurates a new venture for Dialogues in Human Geography in which we host a discussion on topics of concern to our readers. Inspired by the underlying ethos of the journal as a place for dialogue, this is neither an interview nor an article, but rather an opportunity to bring together people with a range of views. In this discussion, we begin by tackling the issue of artificial intelligence and machine learning in geography, sometimes called GeoAI (geographic artificial intelligence). What is at stake with this development? We discuss how the legacy of the critical GIS movement, and specifically what Ren{\'e}e Sieber calls `counter-AI', may yet have a role to play. For Krzysztof Janowicz, geographers are just getting started with GeoAI and many exciting developments lie ahead. Yet both sound a note of caution about data representation, bias, and blackboxing algorithms, as well as the need for accountability and how, ultimately, critique should be situated. The conversation took place in July 2022, and has been edited for clarity.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/A7UJ3SNV/Janowicz et al. - 2022 - GeoAI, counter-AI, and human geography A conversa.pdf}
}

@misc{jax2018github,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = {2018}
}

@book{jaynes2003,
  title = {The Origin of Consciousness in the Breakdown of the Bicameral Mind},
  author = {Jaynes, Julian},
  year = {2003},
  publisher = {Houghton Mifflin},
  address = {Boston},
  isbn = {978-0-618-05707-8},
  langid = {english},
  annotation = {OCLC: 718136228},
  file = {/Users/nobr/Zotero/storage/VBZGMXSV/Jaynes - 2003 - The origin of consciousness in the breakdown of the bicameral mind.pdf}
}

@misc{jentzen2023,
  title = {Mathematical {{Introduction}} to {{Deep Learning}}: {{Methods}}, {{Implementations}}, and {{Theory}}},
  shorttitle = {Mathematical {{Introduction}} to {{Deep Learning}}},
  author = {Jentzen, Arnulf and Kuckuck, Benno and {von Wurstemberger}, Philippe},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  urldate = {2024-01-01},
  abstract = {This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-\{{\textbackslash}L\}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.},
  howpublished = {https://arxiv.org/abs/2310.20360v1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/K3T74Q2A/Jentzen et al. - 2023 - Mathematical Introduction to Deep Learning Methods, Implementations, and Theory.pdf}
}

@article{jeon2022,
  title = {An {{Information-Theoretic Framework}} for {{Deep Learning}}},
  author = {Jeon, Hong Jun and Van Roy, Benjamin},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {3279--3291},
  urldate = {2024-11-21},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/MKXXM6XQ/Jeon and Van Roy - 2022 - An Information-Theoretic Framework for Deep Learning.pdf}
}

@misc{jiang2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  howpublished = {https://arxiv.org/abs/2310.06825v1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/65RA2C5B/Jiang et al. - 2023 - Mistral 7B.pdf}
}

@inproceedings{jiang2023b,
  title = {``{{Low-Resource}}'' {{Text Classification}}: {{A Parameter-Free Classification Method}} with {{Compressors}}},
  shorttitle = {``{{Low-Resource}}'' {{Text Classification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {Jiang, Zhiying and Yang, Matthew and Tsirlin, Mikhail and Tang, Raphael and Dai, Yiqin and Lin, Jimmy},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {6810--6828},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.findings-acl.426},
  urldate = {2024-11-23},
  abstract = {Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that's easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip with a k-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.},
  file = {/Users/nobr/Zotero/storage/7XTCZSNB/Jiang et al. - 2023 - “Low-Resource” Text Classification A Parameter-Free Classification Method with Compressors.pdf}
}

@misc{jiangMixtralExperts2024,
  title = {Mixtral of {{Experts}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04088},
  eprint = {2401.04088},
  primaryclass = {cs},
  urldate = {2024-01-09},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/2M332WTY/Jiang et al. - 2024 - Mixtral of Experts.pdf}
}

@misc{jiangStructGPTGeneralFramework2023,
  title = {{{StructGPT}}: {{A General Framework}} for {{Large Language Model}} to {{Reason}} over {{Structured Data}}},
  shorttitle = {{{StructGPT}}},
  author = {Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2023},
  month = oct,
  number = {arXiv:2305.09645},
  eprint = {2305.09645},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.09645},
  urldate = {2024-02-12},
  abstract = {In this paper, we study how to improve the zero-shot reasoning ability of large language models{\textasciitilde}(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an {\textbackslash}emph\{Iterative Reading-then-Reasoning{\textasciitilde}(IRR)\} approach for solving question answering tasks based on structured data, called {\textbackslash}textbf\{StructGPT\}. In our approach, we construct the specialized function to collect relevant evidence from structured data ({\textbackslash}ie {\textbackslash}emph\{reading\}), and let LLMs concentrate the reasoning task based on the collected information ({\textbackslash}ie {\textbackslash}emph\{reasoning\}). Specially, we propose an {\textbackslash}emph\{invoking-linearization-generation\} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/RUCAIBox/StructGPT\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/95FCJYR2/Jiang et al. - 2023 - StructGPT A General Framework for Large Language Model to Reason over Structured Data.pdf;/Users/nobr/Zotero/storage/ZFYNMF37/2305.html}
}

@misc{jinGeneGPTAugmentingLarge2023,
  title = {{{GeneGPT}}: {{Augmenting Large Language Models}} with {{Domain Tools}} for {{Improved Access}} to {{Biomedical Information}}},
  shorttitle = {{{GeneGPT}}},
  author = {Jin, Qiao and Yang, Yifan and Chen, Qingyu and Lu, Zhiyong},
  year = {2023},
  month = may,
  number = {arXiv:2304.09667},
  eprint = {2304.09667},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.09667},
  urldate = {2024-02-12},
  abstract = {While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations. Augmenting LLMs with domain-specific tools such as database utilities can facilitate easier and more precise access to specialized knowledge. In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions. Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs by in-context learning and an augmented decoding algorithm that can detect and execute API calls. Experimental results show that GeneGPT achieves state-of-the-art performance on eight tasks in the GeneTuring benchmark with an average score of 0.83, largely surpassing retrieval-augmented LLMs such as the new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1) API demonstrations have good cross-task generalizability and are more useful than documentations for in-context learning; (2) GeneGPT can generalize to longer chains of API calls and answer multi-hop questions in GeneHop, a novel dataset introduced in this work; (3) Different types of errors are enriched in different tasks, providing valuable insights for future improvements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Quantitative Biology - Genomics},
  file = {/Users/nobr/Zotero/storage/BVFL2PF8/Jin et al. - 2023 - GeneGPT Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Inform.pdf;/Users/nobr/Zotero/storage/NQK5J6ID/2304.html}
}

@article{johnson2023,
  title = {Finding {{AI Faces}} in the {{Moon}} and {{Armies}} in the {{Clouds}}: {{Anthropomorphising Artificial Intelligence}} in {{Military Human-Machine Interactions}}},
  shorttitle = {Finding {{AI Faces}} in the {{Moon}} and {{Armies}} in the {{Clouds}}},
  author = {Johnson, James},
  year = {2023},
  month = apr,
  journal = {Global Society},
  pages = {1--16},
  issn = {1360-0826, 1469-798X},
  doi = {10.1080/13600826.2023.2205444},
  urldate = {2023-11-19},
  abstract = {Why are we likely to see anthropomorphisms in military artificial intelligence (AI) human-machine interactions (HMIs)? And what are the potential consequences of this phenomena? Since its inception, AI has been conceptualised in anthropomorphic terms, employing biomimicry to digitally map the human brain as analogies to human reasoning. Hybrid teams of human soldiers and autonomous agents controlled by AI are expected to play an increasingly more significant role in future military operations. The article argues that anthropomorphism will play a critical role in future human-machine interactions in tactical operations. The article identifies some potential epistemological, normative, and ethical consequences of humanising algorithms for the conduct of war. It also considers the possible impact of the AI-anthropomorphism phenomenon on the inversion of AI anthropomorphism and the dehumanisation of war.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZZS436JG/Johnson - 2023 - Finding AI Faces in the Moon and Armies in the Clouds Anthropomorphising Artificial Intelligence in.pdf}
}

@article{jonas2017,
  title = {Could a {{Neuroscientist Understand}} a {{Microprocessor}}?},
  author = {Jonas, Eric and Kording, Konrad Paul},
  year = {2017},
  journal = {PLOS Computational Biology},
  abstract = {There is a popular belief in neuroscience that we are primarily data limited, and that producing large, multimodal, and complex datasets will, with the help of advanced data analysis algorithms, lead to fundamental insights into the way the brain processes information. These datasets do not yet exist, and if they did we would have no way of evaluating whether or not the algorithmically-generated insights were sufficient or even correct. To address this, here we take a classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the microprocessor. This suggests current analytic approaches in neuroscience may fall short of producing meaningful understanding of neural systems, regardless of the amount of data. Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DQ28VE9I/Jonas and Kording - 2017 - Could a Neuroscientist Understand a Microprocessor.pdf}
}

@article{jones1993,
  title = {Intelligent {{Automated Agents}} for {{Flight Training Simulators}}},
  author = {Jones, Randolph M and Tambe, Milind},
  year = {1993},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3YLMHM3B/Jones and Tambe - Intelligent Automated Agents for Flight Training Simulators.pdf}
}

@misc{jraph2020,
  title = {Jraph: {{A}} Library for Graph Neural Networks in Jax.},
  author = {{jraph}},
  year = {2020},
  urldate = {2023-12-15},
  howpublished = {https://github.com/google-deepmind/jraph/tree/master},
  file = {/Users/nobr/Zotero/storage/E76AJI5I/master.html}
}

@article{jumper2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2024-11-23},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50~years9. Despite recent progress10--14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology},
  file = {/Users/nobr/Zotero/storage/5UETH6KB/Jumper et al. - 2021 - Highly accurate protein structure prediction with AlphaFold.pdf}
}

@article{justesen2019,
  title = {Algorithms for {{Adaptive Game-playing Agents}}},
  author = {Justesen, Niels Orsleff},
  year = {2019},
  langid = {english}
}

@inproceedings{justesen2019a,
  title = {{{MAP-Elites}} for Noisy Domains by Adaptive Sampling},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Justesen, Niels and Risi, Sebastian and Mouret, Jean-Baptiste},
  year = {2019},
  month = jul,
  pages = {121--122},
  publisher = {ACM},
  address = {Prague Czech Republic},
  doi = {10.1145/3319619.3321904},
  urldate = {2025-01-14},
  isbn = {978-1-4503-6748-6},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3AVPPYZP/Justesen et al. - 2019 - MAP-Elites for noisy domains by adaptive sampling.pdf}
}

@article{juveOneMotherTwo2025,
  title = {One Mother for Two Species via Obligate Cross-Species Cloning in Ants},
  author = {Juv{\'e}, Y. and Lutrat, C. and Ha, A. and Weyna, A. and Lauroua, E. and Afonso Silva, A. C. and Roux, C. and Schifani, E. and Galkowski, C. and Lebas, C. and Allio, R. and Stoyanov, I. and Galtier, N. and {Schlick-Steiner}, B. C. and Steiner, F. M. and Baas, D. and Kaufmann, B. and Romiguier, J.},
  year = {2025},
  month = sep,
  journal = {Nature},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-025-09425-w},
  urldate = {2025-09-19},
  abstract = {Abstract                            Living organisms are assumed to produce same-species offspring               1,2               . Here, we report a shift from this norm in               Messor ibericus               , an ant that lays individuals from two distinct species. In this life cycle, females must clone males of another species because they require their sperm to produce the worker caste. As a result, males from the same mother exhibit distinct genomes and morphologies, as they belong to species that diverged over 5\,million years ago. The evolutionary history of this system appears as sexual parasitism               3               that evolved into a natural case of cross-species cloning               4,5               , resulting in the maintenance of a male-only lineage cloned through distinct species' ova. We term females exhibiting this reproductive mode as xenoparous, meaning they give birth to other species as part of their life cycle.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/NYBLYSQS/Juvé et al. - 2025 - One mother for two species via obligate cross-species cloning in ants.pdf}
}

@article{kaczynski1995,
  title = {Industrial {{Society}} and {{Its Future}}},
  author = {Kaczynski, Theodore John},
  year = {1995},
  journal = {The Washington Post},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JB9QGLSC/Kaczynski - 1995 - Industrial Society and Its Future.pdf}
}

@book{kahneman2011,
  title = {Thinking, Fast and Slow},
  author = {Kahneman, Daniel},
  year = {2011},
  edition = {1st ed},
  publisher = {{Farrar, Straus and Giroux}},
  address = {New York},
  isbn = {978-0-374-27563-1 978-0-374-53355-7 978-0-606-27564-4},
  lccn = {BF441 .K238 2011},
  keywords = {Decision making,Intuition,Reasoning,Thought and thinking}
}

@misc{kaikalii_evonet_github,
  title = {Evonet: {{Basic}} Evolutionary Neural Network in {{Uiua}}},
  author = {{Kai Schmidt}},
  year = {2025}
}

@article{kalimeri2017,
  title = {Predicting {{Demographics}}, {{Moral Foundations}}, and {{Human Values}} from {{Digital Behaviors}}},
  author = {Kalimeri, Kyriaki and Beiro, Mariano G. and Delfino, Matteo and Raleigh, Robert and Cattuto, Ciro},
  year = {2017},
  month = dec,
  eprint = {1712.01930},
  doi = {10.1016/j.chb.2018.11.024},
  abstract = {Personal electronic devices including smartphones give access to behavioural signals that can be used to learn about the characteristics and preferences of individuals. In this study, we explore the connection between demographic and psychological attributes and the digital behavioural records, for a cohort of 7,633 people, closely representative of the US population with respect to gender, age, geographical distribution, education, and income. Along with the demographic data, we collected self-reported assessments on validated psychometric questionnaires for moral traits and basic human values and combined this information with passively collected multi-modal digital data from web browsing behaviour and smartphone usage. A machine learning framework was then designed to infer both the demographic and psychological attributes from the behavioural data. In a cross-validated setting, our models predicted demographic attributes with good accuracy as measured by the weighted AUROC score (Area Under the Receiver Operating Characteristic), but were less performant for the moral traits and human values. These results call for further investigation since they are still far from unveiling individuals' psychological fabric. This connection, along with the most predictive features that we provide for each attribute, might prove useful for designing personalised services, communication strategies, and interventions, and can be used to sketch a portrait of people with a similar worldview.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Information Retrieval,Computer Science - Social and Information Networks},
  file = {/Users/nobr/Zotero/storage/33SLR9DT/Kalimeri et al. - 2019 - Predicting Demographics, Moral Foundations, and Human Values from Digital Behaviors.pdf;/Users/nobr/Zotero/storage/RJKABRFP/Kalimeri et al. - 2017 - Predicting Demographics, Moral Foundations, and Human Values from Digital Behaviors.pdf}
}

@misc{kang2024,
  title = {Unfamiliar {{Finetuning Examples Control How Language Models Hallucinate}}},
  author = {Kang, Katie and Wallace, Eric and Tomlin, Claire and Kumar, Aviral and Levine, Sergey},
  year = {2024},
  month = may,
  number = {arXiv:2403.05612},
  eprint = {2403.05612},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-20},
  abstract = {Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how finetuned LLMs hallucinate. Our investigation reveals an interesting pattern: as inputs become more unfamiliar, LLM outputs tend to default towards a ``hedged'' prediction, whose form is determined by how the unfamiliar examples in the finetuning data are supervised. Thus, by strategically modifying these examples' supervision, we can control LLM predictions for unfamiliar inputs (e.g., teach them to say ``I don't know''). Based on these principles, we develop an RL approach that more reliably mitigates hallucinations for long-form generation tasks, by tackling the challenges presented by reward model hallucinations. We validate our findings with a series of controlled experiments in multiple-choice QA on MMLU, as well as long-form biography and book/movie plot generation tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/IN3SN7RX/Kang et al. - 2024 - Unfamiliar Finetuning Examples Control How Language Models Hallucinate.pdf}
}

@misc{kannan2023,
  title = {{{SMART-LLM}}: {{Smart Multi-Agent Robot Task Planning}} Using {{Large Language Models}}},
  shorttitle = {{{SMART-LLM}}},
  author = {Kannan, Shyam Sundar and Venkatesh, Vishnunandan L. N. and Min, Byung-Cheol},
  year = {2023},
  month = sep,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and realworld scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/84H6PCY7/Kannan et al. - 2023 - SMART-LLM Smart Multi-Agent Robot Task Planning using Large Language Models.pdf}
}

@article{kaplan2017champagne,
  title = {Like `Champagne Bottles Being Opened': {{Scientists}} Document an Ancient {{Arctic}} Methane Explosion},
  author = {Kaplan, Sarah},
  year = {2017},
  month = jun,
  journal = {The Washington Post}
}

@incollection{karaboga2007,
  title = {Artificial {{Bee Colony}} ({{ABC}}) {{Optimization Algorithm}} for {{Solving Constrained Optimization Problems}}},
  booktitle = {Foundations of {{Fuzzy Logic}} and {{Soft Computing}}},
  author = {Karaboga, Dervis and Basturk, Bahriye},
  editor = {Melin, Patricia and Castillo, Oscar and Aguilar, Luis T. and Kacprzyk, Janusz and Pedrycz, Witold},
  year = {2007},
  volume = {4529},
  pages = {789--798},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-540-72950-1_77},
  urldate = {2023-11-28},
  isbn = {978-3-540-72917-4 978-3-540-72950-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PHR2C694/Karaboga and Basturk - 2007 - Artificial Bee Colony (ABC) Optimization Algorithm for Solving Constrained Optimization Problems.pdf}
}

@misc{kataokaPretrainingNaturalImages2021,
  title = {Pre-Training without {{Natural Images}}},
  author = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},
  year = {2021},
  month = jan,
  number = {arXiv:2101.08515},
  eprint = {2101.08515},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning. We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law existing in the background knowledge of the real world. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinite scale dataset of labeled images. Although the models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, does not necessarily outperform models pre-trained with human annotated datasets at all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/QJVX87FW/Kataoka et al. - 2021 - Pre-training without Natural Images.pdf}
}

@misc{katharopoulos2020,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = aug,
  number = {arXiv:2006.16236},
  eprint = {2006.16236},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2006.16236},
  urldate = {2023-07-03},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/4S5NHVL2/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transformers with Linear Attention.pdf}
}

@article{kaufmann2023,
  title = {Champion-Level Drone Racing Using Deep Reinforcement Learning},
  author = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and M{\"u}ller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
  year = {2023},
  month = aug,
  journal = {Nature},
  volume = {620},
  number = {7976},
  pages = {982--987},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06419-4},
  urldate = {2023-12-05},
  abstract = {First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Aerospace engineering,Computer science,Electrical and electronic engineering,Mechanical engineering},
  file = {/Users/nobr/Zotero/storage/A3XYXHKL/Kaufmann et al. - 2023 - Champion-level drone racing using deep reinforcement learning.pdf}
}

@inproceedings{kennedy1995,
  title = {Particle Swarm Optimization},
  booktitle = {Proceedings of {{ICNN}}'95 - {{International Conference}} on {{Neural Networks}}},
  author = {Kennedy, J. and Eberhart, R.},
  year = {1995},
  volume = {4},
  pages = {1942--1948},
  publisher = {IEEE},
  address = {Perth, WA, Australia},
  doi = {10.1109/ICNN.1995.488968},
  urldate = {2023-11-26},
  isbn = {978-0-7803-2768-9},
  file = {/Users/nobr/Zotero/storage/64Q9YCKI/Kennedy and Eberhart - 1995 - Particle swarm optimization.pdf}
}

@book{kennedy2016,
  title = {Micro-, {{Meso-}} and {{Macro-Connectomics}} of the {{Brain}}},
  editor = {Kennedy, Henry and Van Essen, David C. and Christen, Yves},
  year = {2016},
  publisher = {Springer},
  address = {Cham (CH)},
  urldate = {2023-12-15},
  abstract = {This book has brought together leading investigators who work in the new arena of brain connectomics. This includes `macro-connectome' efforts to comprehensively chart long-distance pathways and functional networks; `micro-connectome' efforts to identify every neuron, axon, dendrite, synapse, and glial process within restricted brain regions; and `meso-connectome' efforts to systematically map both local and long-distance connections using anatomical tracers. This book highlights cutting-edge methods that can accelerate progress in elucidating static `hard-wired' circuits of the brain as well as dynamic interactions that are vital for brain function. The power of connectomic approaches in characterizing abnormal circuits in the many brain disorders that afflict humankind is considered. Experts in computational neuroscience and network theory provide perspectives needed for synthesizing across different scales in space and time. Altogether, this book provides an integrated view of the challenges and opportunities in deciphering brain circuits in health and disease.},
  copyright = {Copyright 2016, The Editor(s) (if applicable) and the Author(s) This book is published open access.},
  isbn = {978-3-319-27776-9 978-3-319-27777-6},
  langid = {english},
  lccn = {NBK435763},
  pmid = {28590611}
}

@article{keren2023,
  title = {A Computational Framework for Physics-Informed Symbolic Regression with Straightforward Integration of Domain Knowledge},
  author = {Keren, Liron Simon and Liberzon, Alex and Lazebnik, Teddy},
  year = {2023},
  month = jan,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {1249},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-28328-2},
  urldate = {2024-01-22},
  abstract = {Discovering a meaningful symbolic expression that explains experimental data is a fundamental challenge in many scientific fields. We present a novel, open-source computational framework called Scientist-Machine Equation Detector (SciMED), which integrates scientific discipline wisdom in a scientist-in-the-loop approach, with state-of-the-art symbolic regression (SR) methods. SciMED combines a wrapper selection method, that is based on a genetic algorithm, with automatic machine learning and two levels of SR methods. We test SciMED on five configurations of a settling sphere, with and without aerodynamic non-linear drag force, and with excessive noise in the measurements. We show that SciMED is sufficiently robust to discover the correct physically meaningful symbolic expressions from the data, and demonstrate how the integration of domain knowledge enhances its performance. Our results indicate better performance on these tasks than the state-of-the-art SR software packages , even in cases where no knowledge is integrated. Moreover, we demonstrate how SciMED can alert the user about possible missing features, unlike the majority of current SR systems.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Applied physics,Computational science,Information theory and computation,Scientific data,Software},
  file = {/Users/nobr/Zotero/storage/L5Z8VWJ5/Keren et al. - 2023 - A computational framework for physics-informed sym.pdf}
}

@article{kernberg2017,
  title = {The Concept of the Death Drive: {{A}} Clinical Perspective},
  author = {Kernberg, Otto},
  year = {2017},
  journal = {The International Journal of Psychoanalysis},
  volume = {90},
  number = {5},
  pages = {1009--1023},
  doi = {10.1111/j.1745-8315.2009.00187.x},
  file = {/Users/nobr/Zotero/storage/AFC932VU/document.pdf}
}

@misc{khalifa2020,
  title = {{{PCGRL}}: {{Procedural Content Generation}} via {{Reinforcement Learning}}},
  shorttitle = {{{PCGRL}}},
  author = {Khalifa, Ahmed and Bontrager, Philip and Earle, Sam and Togelius, Julian},
  year = {2020},
  month = aug,
  number = {arXiv:2001.09212},
  eprint = {2001.09212},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.09212},
  urldate = {2025-08-09},
  abstract = {We investigate how reinforcement learning can be used to train level-designing agents. This represents a new approach to procedural content generation in games, where level design is framed as a game, and the content generator itself is learned. By seeing the design problem as a sequential task, we can use reinforcement learning to learn how to take the next action so that the expected final level quality is maximized. This approach can be used when few or no examples exist to train from, and the trained generator is very fast. We investigate three different ways of transforming two-dimensional level design problems into Markov decision processes and apply these to three game environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/5SG6YE2Q/Khalifa et al. - 2020 - PCGRL Procedural Content Generation via Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/QICUW9HJ/2001.html}
}

@misc{khandelwal2023,
  title = {Automated Deep Learning Segmentation of High-Resolution 7 {{T}} Postmortem {{MRI}} for Quantitative Analysis of Structure-Pathology Correlations in Neurodegenerative Diseases},
  author = {Khandelwal, Pulkit and Duong, Michael Tran and Sadaghiani, Shokufeh and Lim, Sydney and Denning, Amanda and Chung, Eunice and Ravikumar, Sadhana and Arezoumandan, Sanaz and Peterson, Claire and Bedard, Madigan and Capp, Noah and Ittyerah, Ranjit and Migdal, Elyse and Choi, Grace and Kopp, Emily and Loja, Bridget and Hasan, Eusha and Li, Jiacheng and Bahena, Alejandra and Prabhakaran, Karthik and Mizsei, Gabor and Gabrielyan, Marianna and Schuck, Theresa and Trotman, Winifred and Robinson, John and Ohm, Daniel and Lee, Edward B. and Trojanowski, John Q. and McMillan, Corey and Grossman, Murray and Irwin, David J. and Detre, John and Tisdall, M. Dylan and Das, Sandhitsu R. and Wisse, Laura E. M. and Wolk, David A. and Yushkevich, Paul A.},
  year = {2023},
  month = oct,
  number = {arXiv:2303.12237},
  eprint = {2303.12237},
  doi = {10.48550/arXiv.2303.12237},
  urldate = {2024-10-28},
  abstract = {Postmortem MRI allows brain anatomy to be examined at high resolution and to link pathology measures with morphometric measurements. However, automated segmentation methods for brain mapping in postmortem MRI are not well developed, primarily due to limited availability of labeled datasets, and heterogeneity in scanner hardware and acquisition protocols. In this work, we present a high resolution of 135 postmortem human brain tissue specimens imaged at 0.3 mm\${\textasciicircum}\{3\}\$ isotropic using a T2w sequence on a 7T whole-body MRI scanner. We developed a deep learning pipeline to segment the cortical mantle by benchmarking the performance of nine deep neural architectures, followed by post-hoc topological correction. We then segment four subcortical structures (caudate, putamen, globus pallidus, and thalamus), white matter hyperintensities, and the normal appearing white matter. We show generalizing capabilities across whole brain hemispheres in different specimens, and also on unseen images acquired at 0.28 mm{\textasciicircum}3 and 0.16 mm{\textasciicircum}3 isotropic T2*w FLASH sequence at 7T. We then compute localized cortical thickness and volumetric measurements across key regions, and link them with semi-quantitative neuropathological ratings. Our code, Jupyter notebooks, and the containerized executables are publicly available at: https://pulkit-khandelwal.github.io/exvivo-brain-upenn},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/JE3WRLS6/Khandelwal et al. - 2023 - Automated deep learning segmentation of high-resolution 7 T postmortem MRI for quantitative analysis of structure-pathology correlations in neurodegenerative diseases.pdf}
}

@article{khovratovich,
  title = {How to {{Prove False Statements}}: {{Practical Attacks}} on {{Fiat-Shamir}}},
  author = {Khovratovich, Dmitry and Rothblum, Ron D and Soukhanov, Lev},
  abstract = {The Fiat-Shamir (FS) transform is a prolific and powerful technique for compiling publiccoin interactive protocols into non-interactive ones. Roughly speaking, the idea is to replace the random coins of the verifier with the evaluations of a complex hash function.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/KBHXUML3/Khovratovich et al. - How to Prove False Statements Practical Attacks on Fiat-Shamir.pdf}
}

@article{kim2023,
  title = {Evidence for the Utility of Quantum Computing before Fault Tolerance},
  author = {Kim, Youngseok and Eddins, Andrew and Anand, Sajant and Wei, Ken Xuan and Van Den Berg, Ewout and Rosenblatt, Sami and Nayfeh, Hasan and Wu, Yantao and Zaletel, Michael and Temme, Kristan and Kandala, Abhinav},
  year = {2023},
  month = jun,
  journal = {Nature},
  volume = {618},
  number = {7965},
  pages = {500--505},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-023-06096-3},
  urldate = {2024-02-09},
  abstract = {Abstract                            Quantum computing promises to offer substantial speed-ups over its classical counterpart for certain problems. However, the greatest impediment to realizing its full potential is noise that is inherent to these systems. The widely accepted solution to this challenge is the implementation of fault-tolerant quantum circuits, which is out of reach for current processors. Here we report experiments on a noisy 127-qubit processor and demonstrate the measurement of accurate expectation values for circuit volumes at a scale beyond brute-force classical computation. We argue that this represents evidence for the utility of quantum computing in a pre-fault-tolerant era. These experimental results are enabled by advances in the coherence and calibration of a superconducting processor at this scale and the ability to characterize               1               and controllably manipulate noise across such a large device. We establish the accuracy of the measured expectation values by comparing them with the output of exactly verifiable circuits. In the regime of strong entanglement, the quantum computer provides correct results for which leading classical approximations such as pure-state-based 1D (matrix product states,~MPS) and 2D (isometric tensor network states, isoTNS) tensor network methods               2,3               break down. These experiments demonstrate a foundational tool for the realization of near-term quantum applications               4,5               .},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HYF4SGT9/Kim et al. - 2023 - Evidence for the utility of quantum computing before fault tolerance.pdf}
}

@book{kincaid2009,
  title = {Numerical Analysis: Mathematics of Scientific Computing},
  shorttitle = {Numerical Analysis},
  author = {Kincaid, David and Cheney, E. W.},
  year = {2009},
  series = {The {{Sally}} Series},
  edition = {3rd ed},
  number = {2},
  publisher = {American Mathematical Society},
  address = {Providence, R.I},
  isbn = {978-0-8218-4788-6},
  langid = {english},
  lccn = {QA297 .K563 2009},
  keywords = {Numerical analysis},
  annotation = {OCLC: ocn267055046},
  file = {/Users/nobr/Zotero/storage/MQEJFZ27/Kincaid and Cheney - 2009 - Numerical analysis mathematics of scientific computing.pdf}
}

@article{kingma2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  month = dec,
  journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
  eprint = {1312.6114},
  publisher = {International Conference on Learning Representations, ICLR},
  urldate = {2023-03-22},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/SBJ5G8IR/full-text.pdf}
}

@misc{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/N5GGYPC5/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf}
}

@article{kingma2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  primaryclass = {cs, stat},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  urldate = {2023-05-10},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/LXP9TK5L/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf}
}

@article{kingma2021,
  title = {Variational {{Diffusion Models}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  year = {2021},
  month = jul,
  journal = {Advances in Neural Information Processing Systems},
  volume = {26},
  eprint = {2107.00630},
  pages = {21696--21707},
  publisher = {Neural information processing systems foundation},
  issn = {10495258},
  urldate = {2023-04-11},
  abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at https://github.com/google-research/vdm .},
  archiveprefix = {arXiv},
  isbn = {9781713845393},
  file = {/Users/nobr/Zotero/storage/XSK8APBC/full-text.pdf}
}

@misc{kingma2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2022},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6114},
  urldate = {2024-11-21},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/CBMMLP6Z/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/Users/nobr/Zotero/storage/VJKFR5TE/1312.html}
}

@misc{kipf2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.02907},
  urldate = {2023-04-25},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/VAXDR4G4/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/Users/nobr/Zotero/storage/F2T83FQ4/1609.html}
}

@misc{kipf2017a,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.02907},
  urldate = {2024-12-26},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/DMLUEWVL/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolutional Networks.pdf}
}

@article{kitano1997,
  title = {{{RoboCup}}},
  author = {Kitano, Hiroaki and Asada, Minoru and Kuniyoshi, Yasuo and Noda, Itsuki and Osawa, Eiichi},
  year = {1997},
  pages = {340--347},
  publisher = {Association for Computing Machinery (ACM)},
  doi = {10.1145/267658.267738},
  urldate = {2023-03-22},
  abstract = {Location: Web Keywords: Robot soccer senior projects Comments: A web link to the RoboCup initiative.},
  file = {/Users/nobr/Zotero/storage/ALRU96MJ/full-text.pdf}
}

@article{klapper2021,
  title = {Mechanical {{Turk Jurisprudence}}},
  author = {Klapper, Shlomo},
  year = {2021},
  keywords = {/unread},
  file = {/Users/nobr/Zotero/storage/YEQFZZ3J/Klapper - Mechanical Turk Jurisprudence.pdf}
}

@book{klein2004,
  title = {Elementary Mathematics from an Advanced Standpoint. {{Geometry}}},
  author = {Klein, Felix},
  year = {2004},
  edition = {Dover edition},
  publisher = {Dover Publications},
  address = {Mineola, N.Y},
  isbn = {978-0-486-43481-0},
  langid = {english},
  lccn = {QA461 .K4513 2004},
  keywords = {Geometry,Study and teaching}
}

@book{kleinbaum2012,
  title = {Survival Analysis: A Self-Learning Text},
  shorttitle = {Survival Analysis},
  author = {Kleinbaum, David G. and Klein, Mitchel},
  year = {2012},
  series = {Statistics for Biology and Health},
  edition = {3rd ed},
  publisher = {Springer},
  address = {New York},
  isbn = {978-1-4419-6645-2 978-1-4419-6646-9},
  langid = {english},
  lccn = {R853.S7 K543 2012},
  keywords = {Programmed Instruction,Survival Analysis,Survival analysis (Biometry)},
  annotation = {OCLC: ocn760292284},
  file = {/Users/nobr/Zotero/storage/SFLNS7JR/Kleinbaum and Klein - 2012 - Survival analysis a self-learning text.pdf}
}

@misc{knuth1992,
  title = {Computer {{Modern}}},
  author = {Knuth, Donald},
  year = {1992},
  urldate = {2024-12-05}
}

@article{kocsis2006,
  title = {Improved {{Monte-Carlo Search}}},
  author = {Kocsis, Levente and Szepesvari, Csaba and Willemson, Jan},
  year = {2006},
  abstract = {Monte-Carlo search has been successful in many non-deterministic games, and recently in deterministic games with high branching factor. One of the drawbacks of the current approaches is that even if the iterative process would last for a very long time, the selected move does not necessarily converge to a game-theoretic optimal one. In this paper we introduce a new algorithm, UCT, which extends a bandit algorithm for Monte-Carlo search. It is proven that the probability that the algorithm selects the correct move converges to 1. Moreover it is shown empirically that the algorithm converges rather fast even in comparison with alpha-beta search. Experiments in Amazons and Clobber indicate that the UCT algorithm outperforms considerably a plain Monte-Carlo version, and it is competitive against alpha-beta based game programs.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/NMTD42M4/Kocsis et al. - Improved Monte-Carlo Search.pdf}
}

@article{kolchinsky2014,
  title = {Multi-Scale Integration and Predictability in Resting State Brain Activity},
  author = {Kolchinsky, Artemy and {van den Heuvel}, Martijn P. and Griffa, Alessandra and Hagmann, Patric and Rocha, Luis M. and Sporns, Olaf and Go{\~n}i, Joaqu{\'i}n},
  year = {2014},
  journal = {Frontiers in Neuroinformatics},
  volume = {8},
  issn = {1662-5196},
  urldate = {2023-05-13},
  abstract = {The human brain displays heterogeneous organization in both structure and function. Here we develop a method to characterize brain regions and networks in terms of information-theoretic measures. We look at how these measures scale when larger spatial regions as well as larger connectome sub-networks are considered. This framework is applied to human brain fMRI recordings of resting-state activity and DSI-inferred structural connectivity. We find that strong functional coupling across large spatial distances distinguishes functional hubs from unimodal low-level areas, and that this long-range functional coupling correlates with structural long-range efficiency on the connectome. We also find a set of connectome regions that are both internally integrated and coupled to the rest of the brain, and which resemble previously reported resting-state networks. Finally, we argue that information-theoretic measures are useful for characterizing the functional organization of the brain at multiple scales.},
  file = {/Users/nobr/Zotero/storage/DAQVS954/Kolchinsky et al. - 2014 - Multi-scale integration and predictability in rest.pdf}
}

@article{kondrashev1991,
  title = {A History of {{APL}} in the {{USSR}}},
  author = {Kondrashev, Andrei and Luksha, Oleg},
  year = {1991},
  month = dec,
  journal = {SIGAPL APL Quote Quad},
  volume = {22},
  number = {2},
  pages = {8--12},
  issn = {0163-6006},
  doi = {10.1145/130647.130656},
  urldate = {2025-01-31},
  file = {/Users/nobr/Zotero/storage/MCNMW9D7/Kondrashev and Luksha - 1991 - A history of APL in the USSR.pdf}
}

@article{kong2023,
  title = {Reinforcement {{Learning}} for {{Multiaircraft Autonomous Air Combat}} in {{Multisensor UCAV Platform}}},
  author = {Kong, Weiren and Zhou, Deyun and Du, Yongjie and Zhou, Ying and Zhao, Yiyang},
  year = {2023},
  month = sep,
  journal = {IEEE Sensors Journal},
  volume = {23},
  number = {18},
  pages = {20596--20606},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2022.3220324},
  urldate = {2024-01-15},
  abstract = {Autonomous air combat has received significant attention from researchers working on artificial intelligence (AI) applications. Most previous research on autonomous air combat has focused on one-on-one air combat scenarios in which air combat situational information is considered to be precisely observable. However, most modern air combats are conducted in formations, where air combat situational information is obtained from multiple sensors. Therefore, we introduce a novel automated maneuver decision architecture for close-range multiaircraft air combat scenarios under the multisensor unmanned combat aerial vehicle (UCAV) platform that can handle air combat scenarios with variablesized formations. Then, a multiagent reinforcement learning (MARL) algorithm is proposed to obtain the strategy. The training performance of the training algorithm is evaluated, the obtained strategy is analyzed in different air combat scenarios, and it is found that these formations exhibit effective cooperative behavior in symmetric and asymmetric situations. Finally, we give ideas for the engineering implementation of a maneuver control architecture. This study provides a solution for future multiaircraft autonomous air combat.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4EVXUT7M/Kong et al. - 2023 - Reinforcement Learning for Multiaircraft Autonomous Air Combat in Multisensor UCAV Platform.pdf}
}

@article{konstantinos2019,
  title = {A {{Decision Support System}} Methodology for Selecting Wind Farm Installation Locations Using {{AHP}} and {{TOPSIS}}: {{Case}} Study in {{Eastern Macedonia}} and {{Thrace}} Region, {{Greece}}},
  shorttitle = {A {{Decision Support System}} Methodology for Selecting Wind Farm Installation Locations Using {{AHP}} and {{TOPSIS}}},
  author = {Konstantinos, Ioannou and Georgios, Tsantopoulos and Garyfalos, Arabatzis},
  year = {2019},
  month = sep,
  journal = {Energy Policy},
  volume = {132},
  pages = {232--246},
  issn = {03014215},
  doi = {10.1016/j.enpol.2019.05.020},
  urldate = {2023-07-13},
  abstract = {The optimization of spatial planning in order to identify the most suitable places for the installation of wind farms is one of the most difficult problems mainly due to the need of identification and calculation of a variety of qualitative and quantitative parameters as well as their effect on the final solution. Multi Criteria Decision Making Methods (MCDM) are commonly used in order to solve this problem and are combined with Geographic Information Systems (GIS) to spatially represent the results from the application of the MCDM methodology. This paper presents a methodology which is based on the combination of a MCDM methodology called Analytical Hierarch Process (AHP) and GIS in order to determine the most suitable locations for wind farms installation. The calculated locations are then ranked using the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) in order to rank the locations based on installation suitability. The application of this methodology can help decision makers to easily overcome conflicting parameters and propose optimal solutions which are acceptable from citizens and stake holders while at the same time are economical and environmental friendly.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6XW65N52/Konstantinos et al. - 2019 - A Decision Support System methodology for selectin.pdf}
}

@misc{kosmyna2025,
  title = {Your {{Brain}} on {{ChatGPT}}: {{Accumulation}} of {{Cognitive Debt}} When {{Using}} an {{AI Assistant}} for {{Essay Writing Task}}},
  shorttitle = {Your {{Brain}} on {{ChatGPT}}},
  author = {Kosmyna, Nataliya and Hauptmann, Eugene and Yuan, Ye Tong and Situ, Jessica and Liao, Xian-Hao and Beresnitzky, Ashly Vivian and Braunstein, Iris and Maes, Pattie},
  year = {2025},
  month = jun,
  number = {arXiv:2506.08872},
  eprint = {2506.08872},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2506.08872},
  urldate = {2025-06-16},
  abstract = {This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/GXIJQWUM/Kosmyna et al. - 2025 - Your Brain on ChatGPT Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing T.pdf}
}

@inproceedings{koyamada2023pgx,
  title = {Pgx: {{Hardware-accelerated}} Parallel Game Simulators for Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Koyamada, Sotetsu and Okano, Shinri and Nishimori, Soichiro and Murata, Yu and Habara, Keigo and Kita, Haruka and Ishii, Shin},
  year = {2023},
  volume = {36},
  pages = {45716--45743}
}

@misc{kozik2021,
  title = {Mimicking {{Playstyle}} by {{Adapting Parameterized Behavior Trees}} in {{RTS Games}}},
  author = {Kozik, Andrzej and Machalewski, Tomasz and Marek, Mariusz and Ochmann, Adrian},
  year = {2021},
  month = nov,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {The discovery of Behavior Trees (BTs) impacted the field of Artificial Intelligence (AI) in games, by providing flexible and natural representation of non-player characters (NPCs) logic, manageable by game-designers. Nevertheless, increased pressure on ever better NPCs AI-agents forced complexity of handcrafted BTs to became barely-tractable and error-prone. On the other hand, while many just-launched on-line games suffer from player-shortage, the existence of AI with a broad-range of capabilities could increase players retention. Therefore, to handle above challenges, recent trends in the field focused on automatic creation of AI-agents: from deep- and reinforcementlearning techniques to combinatorial (constrained) optimization and evolution of BTs. In this paper, we present a novel approach to semi-automatic construction of AI-agents, that mimic and generalize given human gameplays by adapting and tuning of expert-created BT under a developed similarity metric between source and BT gameplays. To this end, we formulated mixed discrete-continuous optimization problem, in which topological and functional changes of the BT are reflected in numerical variables, and constructed a dedicated hybrid-metaheuristic. The performance of presented approach was verified experimentally in a prototype real-time strategy game. Carried out experiments confirmed efficiency and perspectives of presented approach, which is going to be applied in a commercial game.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/HWQTE35Y/Kozik et al. - 2021 - Mimicking Playstyle by Adapting Parameterized Behavior Trees in RTS Games.pdf}
}

@article{kramar2022,
  title = {Negotiation and Honesty in Artificial Intelligence Methods for the Board Game of {{Diplomacy}}},
  author = {Kram{\'a}r, J{\'a}nos and Eccles, Tom and Gemp, Ian and Tacchetti, Andrea and McKee, Kevin R. and Malinowski, Mateusz and Graepel, Thore and Bachrach, Yoram},
  year = {2022},
  month = dec,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {7214},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-34473-5},
  urldate = {2024-09-09},
  abstract = {The success of human civilization is rooted in our ability to cooperate by communicating and making joint plans. We study how artificial agents may use communication to better cooperate in Diplomacy, a long-standing AI challenge. We propose negotiation algorithms allowing agents to agree on contracts regarding joint plans, and show they outperform agents lacking this ability. For humans, misleading others about our intentions forms a barrier to cooperation. Diplomacy requires reasoning about our opponents' future plans, enabling us to study broken commitments between agents and the conditions for honest cooperation. We find that artificial agents face a similar problem as humans: communities of communicating agents are susceptible to peers who deviate from agreements. To defend against this, we show that the inclination to sanction peers who break contracts dramatically reduces the advantage of such deviators. Hence, sanctioning helps foster mostly truthful communication, despite conditions that initially favor deviations from agreements.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science},
  file = {/Users/nobr/Zotero/storage/V494K32W/Kramár et al. - 2022 - Negotiation and honesty in artificial intelligence methods for the board game of Diplomacy.pdf}
}

@misc{krause2025,
  title = {Probabilistic {{Artificial Intelligence}}},
  author = {Krause, Andreas and H{\"u}botter, Jonas},
  year = {2025},
  month = feb,
  number = {arXiv:2502.05244},
  eprint = {2502.05244},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05244},
  urldate = {2025-08-02},
  abstract = {Artificial intelligence commonly refers to the science and engineering of artificial systems that can carry out tasks generally associated with requiring aspects of human intelligence, such as playing games, translating languages, and driving cars. In recent years, there have been exciting advances in learning-based, data-driven approaches towards AI, and machine learning and deep learning have enabled computer systems to perceive the world in unprecedented ways. Reinforcement learning has enabled breakthroughs in complex games such as Go and challenging robotics tasks such as quadrupedal locomotion. A key aspect of intelligence is to not only make predictions, but reason about the uncertainty in these predictions, and to consider this uncertainty when making decisions. This is what this manuscript on "Probabilistic Artificial Intelligence" is about. The first part covers probabilistic approaches to machine learning. We discuss the differentiation between "epistemic" uncertainty due to lack of data and "aleatoric" uncertainty, which is irreducible and stems, e.g., from noisy observations and outcomes. We discuss concrete approaches towards probabilistic inference and modern approaches to efficient approximate inference. The second part of the manuscript is about taking uncertainty into account in sequential decision tasks. We consider active learning and Bayesian optimization -- approaches that collect data by proposing experiments that are informative for reducing the epistemic uncertainty. We then consider reinforcement learning and modern deep RL approaches that use neural network function approximation. We close by discussing modern approaches in model-based RL, which harness epistemic and aleatoric uncertainty to guide exploration, while also reasoning about safety.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/AIAGSP8T/Krause and Hübotter - 2025 - Probabilistic Artificial Intelligence.pdf}
}

@inproceedings{krizhevsky2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-11-16},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/Users/nobr/Zotero/storage/KKQARZF8/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@article{krizhevsky2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  urldate = {2023-05-10},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/G7TPIV6P/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@inproceedings{krogh1991,
  title = {A {{Simple Weight Decay Can Improve Generalization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krogh, Anders and Hertz, John},
  year = {1991},
  volume = {4},
  publisher = {Morgan-Kaufmann},
  urldate = {2024-11-25},
  abstract = {It has  been observed  in  numerical simulations that a weight decay  can  im(cid:173) prove generalization in a feed-forward  neural network.  This paper explains  why.  It is  proven  that  a  weight  decay  has  two effects  in  a  linear  network.  First,  it  suppresses  any  irrelevant  components  of  the  weight  vector  by  choosing  the smallest  vector  that solves  the learning  problem.  Second,  if  the size  is chosen  right,  a weight  decay  can suppress some of the effects  of  static  noise  on  the  targets,  which  improves  generalization  quite  a  lot.  It  is  then  shown  how  to extend  these  results  to networks  with hidden  layers  and  non-linear  units.  Finally  the  theory  is  confirmed  by  some numerical  simulations using  the  data from  NetTalk.},
  file = {/Users/nobr/Zotero/storage/LEB6XFSJ/Krogh and Hertz - 1991 - A Simple Weight Decay Can Improve Generalization.pdf}
}

@article{kumar2020,
  title = {{{BrainIAK}} Tutorials: {{User-friendly}} Learning Materials for Advanced {{fMRI}} Analysis},
  shorttitle = {{{BrainIAK}} Tutorials},
  author = {Kumar, Manoj and Ellis, Cameron T. and Lu, Qihong and Zhang, Hejia and Capot{\u a}, Mihai and Willke, Theodore L. and Ramadge, Peter J. and {Turk-Browne}, Nicholas B. and Norman, Kenneth A.},
  year = {2020},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {1},
  pages = {e1007549},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007549},
  urldate = {2023-05-12},
  abstract = {Advanced brain imaging analysis methods, including multivariate pattern analysis (MVPA), functional connectivity, and functional alignment, have become powerful tools in cognitive neuroscience over the past decade. These tools are implemented in custom code and separate packages, often requiring different software and language proficiencies. Although usable by expert researchers, novice users face a steep learning curve. These difficulties stem from the use of new programming languages (e.g., Python), learning how to apply machine-learning methods to high-dimensional fMRI data, and minimal documentation and training materials. Furthermore, most standard fMRI analysis packages (e.g., AFNI, FSL, SPM) focus on preprocessing and univariate analyses, leaving a gap in how to integrate with advanced tools. To address these needs, we developed BrainIAK (brainiak.org), an open-source Python software package that seamlessly integrates several cutting-edge, computationally efficient techniques with other Python packages (e.g., Nilearn, Scikit-learn) for file handling, visualization, and machine learning. To disseminate these powerful tools, we developed user-friendly tutorials (in Jupyter format; https://brainiak.org/tutorials/) for learning BrainIAK and advanced fMRI analysis in Python more generally. These materials cover techniques including: MVPA (pattern classification and representational similarity analysis); parallelized searchlight analysis; background connectivity; full correlation matrix analysis; inter-subject correlation; inter-subject functional connectivity; shared response modeling; event segmentation using hidden Markov models; and real-time fMRI. For long-running jobs or large memory needs we provide detailed guidance on high-performance computing clusters. These notebooks were successfully tested at multiple sites, including as problem sets for courses at Yale and Princeton universities and at various workshops and hackathons. These materials are freely shared, with the hope that they become part of a pool of open-source software and educational materials for large-scale, reproducible fMRI analysis and accelerated discovery.},
  langid = {english},
  keywords = {Cognitive neuroscience,Computer software,Functional magnetic resonance imaging,Human learning,Machine learning,Machine learning algorithms,Open source software,Programming languages},
  file = {/Users/nobr/Zotero/storage/5NUMHXGZ/Kumar et al. - 2020 - BrainIAK tutorials User-friendly learning materia.pdf}
}

@misc{kumar2023,
  title = {{{MyCrunchGPT}}: {{A chatGPT}} Assisted Framework for Scientific Machine Learning},
  shorttitle = {{{MyCrunchGPT}}},
  author = {Kumar, Varun and Gleyzer, Leonard and Kahana, Adar and Shukla, Khemraj and Karniadakis, George Em},
  year = {2023},
  month = jul,
  number = {arXiv:2306.15551},
  eprint = {2306.15551},
  primaryclass = {physics},
  publisher = {arXiv},
  urldate = {2023-09-16},
  abstract = {Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still timeconsuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of CrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the CrunchGPT, and create an infrastructure that can facilitate a broader vision, we built a webapp based guided user interface, that includes options for a comprehensive summary report. The overall objective is to extend CrunchGPT to handle diverse problems in computational mechanics, design, optimization and controls, and general scientific computing tasks involved in SciML, hence using it as a research assistant tool but also as an educational tool. While here the examples focus in fluid mechanics, future versions will target solid mechanics and materials science, geophysics, systems biology and bioinformatics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Physics - Physics and Society},
  file = {/Users/nobr/Zotero/storage/SZSS5EG8/Kumar et al. - 2023 - MyCrunchGPT A chatGPT assisted framework for scie.pdf}
}

@misc{kumar2024,
  title = {Automating the {{Search}} for {{Artificial Life}} with {{Foundation Models}}},
  author = {Kumar, Akarsh and Lu, Chris and Kirsch, Louis and Tang, Yujin and Stanley, Kenneth O. and Isola, Phillip and Ha, David},
  year = {2024},
  month = dec,
  number = {arXiv:2412.17799},
  eprint = {2412.17799},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.17799},
  urldate = {2025-01-09},
  abstract = {With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/NSHHFFYA/Kumar et al. - 2024 - Automating the Search for Artificial Life with Foundation Models.pdf}
}

@misc{laban2025,
  title = {{{LLMs Get Lost In Multi-Turn Conversation}}},
  author = {Laban, Philippe and Hayashi, Hiroaki and Zhou, Yingbo and Neville, Jennifer},
  year = {2025},
  month = may,
  number = {arXiv:2505.06120},
  eprint = {2505.06120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.06120},
  urldate = {2025-05-15},
  abstract = {Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39\% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/nobr/Zotero/storage/49UQJC2J/Laban et al. - 2025 - LLMs Get Lost In Multi-Turn Conversation.pdf}
}

@book{lahera2021,
  title = {Persuasive {{Gaming}} in {{Context}}},
  author = {La Hera, Teresa and Jansz, Jeroen and Raessens, Joost and Schouten, Ben},
  year = {2021},
  publisher = {Amsterdam University Press},
  doi = {10.5117/9789463728805},
  urldate = {2023-11-09},
  abstract = {The rapid developments in new communication technologies have facilitated the popularization of digital games, which has translated into an exponential growth of the game industry in recent decades. The ubiquitous presence of digital games has resulted in an expansion of the applications of these games from mere entertainment purposes to a great variety of serious purposes. In this edited volume, we narrow the scope of attention by focusing on what game theorist Ian Bogost has called 'persuasive games', that is, gaming practices that combine the dissemination of information with attempts to engage players in particular attitudes and behaviors.             This volume offers a multifaceted reflection on persuasive gaming, that is, on the process of these particular games being played by players. The purpose is to better understand when and how digital games can be used for persuasion by further exploring persuasive games and some other kinds of persuasive playful interaction as well. The book critically integrates what has been accomplished in separate research traditions to offer a multidisciplinary approach to understanding persuasive gaming that is closely linked to developments in the industry by including the exploration of relevant case studies.},
  isbn = {978-90-485-4393-9 978-94-6372-880-5},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BLAZTXB6/La Hera et al. - 2021 - Persuasive Gaming in Context.pdf}
}

@article{lai1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T.L and Robbins, Herbert},
  year = {1985},
  month = mar,
  journal = {Advances in Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {4--22},
  issn = {01968858},
  doi = {10.1016/0196-8858(85)90002-8},
  urldate = {2024-09-11},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/KMT5ZYZ5/Lai and Robbins - 1985 - Asymptotically efficient adaptive allocation rules.pdf}
}

@misc{langeEvosaxJAXbasedEvolution2022,
  title = {Evosax: {{JAX-based Evolution Strategies}}},
  shorttitle = {Evosax},
  author = {Lange, Robert Tjarko},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04180},
  eprint = {2212.04180},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.04180},
  urldate = {2023-10-13},
  abstract = {The deep learning revolution has greatly been accelerated by the 'hardware lottery': Recent advances in modern hardware accelerators and compilers paved the way for large-scale batch gradient optimization. Evolutionary optimization, on the other hand, has mainly relied on CPU-parallelism, e.g. using Dask scheduling and distributed multi-host infrastructure. Here we argue that also modern evolutionary computation can significantly benefit from the massive computational throughput provided by GPUs and TPUs. In order to better harness these resources and to enable the next generation of black-box optimization algorithms, we release evosax: A JAX-based library of evolution strategies which allows researchers to leverage powerful function transformations such as just-in-time compilation, automatic vectorization and hardware parallelization. evosax implements 30 evolutionary optimization algorithms including finite-difference-based, estimation-of-distribution evolution strategies and various genetic algorithms. Every single algorithm can directly be executed on hardware accelerators and automatically vectorized or parallelized across devices using a single line of code. It is designed in a modular fashion and allows for flexible usage via a simple ask-evaluate-tell API. We thereby hope to facilitate a new wave of scalable evolutionary optimization algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/QI7G4I9R/Lange - 2022 - evosax JAX-based Evolution Strategies.pdf;/Users/nobr/Zotero/storage/TBV5GB9B/2212.html}
}

@misc{lauriereScalableDeepReinforcement2022,
  title = {Scalable {{Deep Reinforcement Learning Algorithms}} for {{Mean Field Games}}},
  author = {Lauri{\`e}re, Mathieu and Perrin, Sarah and Girgin, Sertan and Muller, Paul and Jain, Ayush and Cabannes, Theophile and Piliouras, Georgios and P{\'e}rolat, Julien and {\'E}lie, Romuald and Pietquin, Olivier and Geist, Matthieu},
  year = {2022},
  month = jun,
  number = {arXiv:2203.11973},
  eprint = {2203.11973},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2023-12-05},
  abstract = {Mean Field Games (MFGs) have been introduced to efficiently approximate games with very large populations of strategic agents. Recently, the question of learning equilibria in MFGs has gained momentum, particularly using model-free reinforcement learning (RL) methods. One limiting factor to further scale up using RL is that existing algorithms to solve MFGs require the mixing of approximated quantities such as strategies or q-values. This is far from being trivial in the case of non-linear function approximation that enjoy good generalization properties, e.g. neural networks. We propose two methods to address this shortcoming. The first one learns a mixed strategy from distillation of historical data into a neural network and is applied to the Fictitious Play algorithm. The second one is an online mixing method based on regularization that does not require memorizing historical data or previous estimates. It is used to extend Online Mirror Descent. We demonstrate numerically that these methods efficiently enable the use of Deep RL algorithms to solve various MFGs. In addition, we show that these methods outperform SotA baselines from the literature.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/F7A4QRNV/Laurière et al. - 2022 - Scalable Deep Reinforcement Learning Algorithms for Mean Field Games.pdf}
}

@article{lawrencec2024,
  title = {Causal {{Scrubbing}}: A Method for Rigorously Testing Interpretability Hypotheses [{{Redwood Research}}]},
  shorttitle = {Causal {{Scrubbing}}},
  author = {LawrenceC and {Garriga-alonso}, Adri{\`a} and {Goldowsky-Dill}, Nicholas and {ryan\_greenblatt} and {jenny} and Radhakrishnan, Ansh and Buck and Thomas, Nate},
  year = {2024},
  month = nov,
  urldate = {2024-11-06},
  abstract = {Causal scrubbing is a new tool for evaluating mechanistic interpretability hypotheses. The algorithm tries to replace all model activations that shou{\dots}},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GTEAA3IN/causal-scrubbing-a-method-for-rigorously-testing.html}
}

@book{lebon2002,
  title = {The Crowd: A Study of the Popular Mind},
  shorttitle = {The Crowd},
  author = {Le Bon, Gustave},
  year = {2002},
  publisher = {Dover Publications},
  address = {Mineola, NY},
  isbn = {978-0-486-41956-5},
  langid = {english},
  annotation = {OCLC: 1052627908},
  file = {/Users/nobr/Zotero/storage/KD8BTJEU/Le Bon - 2002 - The crowd a study of the popular mind.pdf}
}

@misc{leCodeRLMasteringCode2022,
  title = {{{CodeRL}}: {{Mastering Code Generation}} through {{Pretrained Models}} and {{Deep Reinforcement Learning}}},
  shorttitle = {{{CodeRL}}},
  author = {Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven C. H.},
  year = {2022},
  month = nov,
  number = {arXiv:2207.01780},
  eprint = {2207.01780},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-05},
  abstract = {Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose ``CodeRL'', a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/nobr/Zotero/storage/HDMN7J8K/Le et al. - 2022 - CodeRL Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning.pdf}
}

@inproceedings{lecun1989,
  title = {Handwritten {{Digit Recognition}} with a {{Back-Propagation Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  year = {1989},
  volume = {2},
  publisher = {Morgan-Kaufmann},
  urldate = {2023-11-16},
  abstract = {We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 \% error rate and about a 9\% reject rate on zipcode digits provided  by the U.S. Postal Service.},
  file = {/Users/nobr/Zotero/storage/2L5BYXUH/LeCun et al. - 1989 - Handwritten Digit Recognition with a Back-Propagation Network.pdf}
}

@article{lee2020,
  title = {{{RecipeGPT}}: {{Generative Pre-training Based Cooking Recipe Generation}} and {{Evaluation System}}},
  author = {Lee, Helena H. and Shu, Ke and Achananuparp, Palakorn and Prasetyo, Philips Kokoh and Liu, Yue and Lim, Ee-Peng and Varshney, Lav R.},
  year = {2020},
  month = mar,
  journal = {The Web Conference 2020 - Companion of the World Wide Web Conference, WWW 2020},
  volume = {17},
  eprint = {2003.02498v1},
  pages = {181--184},
  publisher = {Association for Computing Machinery},
  doi = {10.1145/3366424.3383536},
  urldate = {2023-03-22},
  abstract = {Interests in the automatic generation of cooking recipes have been growing steadily over the past few years thanks to a large amount of online cooking recipes. We present RecipeGPT, a novel online recipe generation and evaluation system. The system provides two modes of text generations: (1) instruction generation from given recipe title and ingredients; and (2) ingredient generation from recipe title and cooking instructions. Its back-end text generation module comprises a generative pre-trained language model GPT-2 fine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation module allows the users to conveniently inspect the quality of the generated recipe contents and store the results for future reference. RecipeGPT can be accessed online at https://recipegpt.org/.},
  archiveprefix = {arXiv},
  keywords = {natural language generation,recipe generation,web application},
  file = {/Users/nobr/Zotero/storage/GYXF7RP2/Lee et al. - 2020 - RecipeGPT Generative Pre-training Based Cooking R.pdf}
}

@article{lee2020a,
  title = {Enabling {{Spike-Based Backpropagation}} for {{Training Deep Neural Network Architectures}}},
  author = {Lee, Chankyu and Sarwar, Syed Shakib and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  year = {2020},
  month = feb,
  journal = {Frontiers in Neuroscience},
  volume = {14},
  pages = {119},
  issn = {1662-453X},
  doi = {10.3389/fnins.2020.00119},
  urldate = {2024-03-20},
  abstract = {Spiking Neural Networks (SNNs) have recently emerged as a prominent neural computing paradigm. However, the typical shallow SNN architectures have limited capacity for expressing complex representations while training deep SNNs using input spikes has not been successful so far. Diverse methods have been proposed to get around this issue such as converting off-the-shelf trained deep Artificial Neural Networks (ANNs) to SNNs. However, the ANN-SNN conversion scheme fails to capture the temporal dynamics of a spiking system. On the other hand, it is still a difficult problem to directly train deep SNNs using input spike events due to the discontinuous, non-differentiable nature of the spike generation function. To overcome this problem, we propose an approximate derivative method that accounts for the leaky behavior of LIF neurons. This method enables training deep convolutional SNNs directly (with input spike events) using spike-based backpropagation. Our experiments show the effectiveness of the proposed spike-based learning on deep networks (VGG and Residual architectures) by achieving the best classification accuracies in MNIST, SVHN, and CIFAR-10 datasets compared to other SNNs trained with a spike-based learning. Moreover, we analyze sparse event-based computations to demonstrate the efficacy of the proposed SNN training method for inference operation in the spiking domain.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/68BY3MTD/Lee et al. - 2020 - Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures.pdf}
}

@misc{lee2021,
  title = {A Surrogate Loss Function for Optimization of \${{F}}\_{\textbackslash}beta\$ Score in Binary Classification with Imbalanced Data},
  author = {Lee, Namgil and Yang, Heejung and Yoo, Hojin},
  year = {2021},
  month = apr,
  number = {arXiv:2104.01459},
  eprint = {2104.01459},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-16},
  abstract = {The \$F\_{\textbackslash}beta\$ score is a commonly used measure of classification performance, which plays crucial roles in classification tasks with imbalanced data sets. However, the \$F\_{\textbackslash}beta\$ score cannot be used as a loss function by gradient-based learning algorithms for optimizing neural network parameters due to its non-differentiability. On the other hand, commonly used loss functions such as the binary cross-entropy (BCE) loss are not directly related to performance measures such as the \$F\_{\textbackslash}beta\$ score, so that neural networks optimized by using the loss functions may not yield optimal performance measures. In this study, we investigate a relationship between classification performance measures and loss functions in terms of the gradients with respect to the model parameters. Then, we propose a differentiable surrogate loss function for the optimization of the \$F\_{\textbackslash}beta\$ score. We show that the gradient paths of the proposed surrogate \$F\_{\textbackslash}beta\$ loss function approximate the gradient paths of the large sample limit of the \$F\_{\textbackslash}beta\$ score. Through numerical experiments using ResNets and benchmark image data sets, it is demonstrated that the proposed surrogate \$F\_{\textbackslash}beta\$ loss function is effective for optimizing \$F\_{\textbackslash}beta\$ scores under class imbalances in binary classification tasks compared with other loss functions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.5.2,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/C4PZT2E3/Lee et al. - 2021 - A surrogate loss function for optimization of $F_.pdf;/Users/nobr/Zotero/storage/65TYCHFT/2104.html}
}

@article{lee2021a,
  title = {Nano-Biosupercapacitors Enable Autarkic Sensor Operation in Blood},
  author = {Lee, Yeji and Bandari, Vineeth Kumar and Li, Zhe and {Medina-S{\'a}nchez}, Mariana and Maitz, Manfred F. and Karnaushenko, Daniil and Tsurkan, Mikhail V. and Karnaushenko, Dmitriy D. and Schmidt, Oliver G.},
  year = {2021},
  month = aug,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {4967},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-24863-6},
  urldate = {2023-12-12},
  abstract = {Abstract                            Today's smallest energy storage devices for in-vivo applications are larger than 3\,mm               3               and lack the ability to continuously drive the complex functions of smart dust electronic and microrobotic systems. Here, we create a tubular biosupercapacitor occupying a mere volume of 1/1000\,mm               3               (=1 nanoliter), yet delivering up to 1.6\,V in blood. The tubular geometry of this nano-biosupercapacitor provides efficient self-protection against external forces from pulsating blood or muscle contraction. Redox enzymes and living cells, naturally present in blood boost the performance of the device by 40\% and help to solve the self-discharging problem persistently encountered by miniaturized supercapacitors. At full capacity, the nano-biosupercapacitors drive a complex integrated sensor system to measure the pH-value in blood. This demonstration opens up opportunities for next generation intravascular implants and microrobotic systems operating in hard-to-reach small spaces deep inside the human body.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BYMWM2MV/Lee et al. - 2021 - Nano-biosupercapacitors enable autarkic sensor operation in blood.pdf}
}

@misc{leeExploringPrimeNumber2024,
  title = {Exploring {{Prime Number Classification}}: {{Achieving High Recall Rate}} and {{Rapid Convergence}} with {{Sparse Encoding}}},
  shorttitle = {Exploring {{Prime Number Classification}}},
  author = {Lee, Serin and Kim, S.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.03363},
  eprint = {2402.03363},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-02-13},
  abstract = {This paper presents a novel approach at the intersection of machine learning and number theory, focusing on the classification of prime and non-prime numbers. At the core of our research is the development of a highly sparse encoding method, integrated with conventional neural network architectures. This combination has shown promising results, achieving a recall of over 99{\textbackslash}\% in identifying prime numbers and 79{\textbackslash}\% for non-prime numbers from an inherently imbalanced sequential series of integers, while exhibiting rapid model convergence before the completion of a single training epoch. We performed training using \$10{\textasciicircum}6\$ integers starting from a specified integer and tested on a different range of \$2 {\textbackslash}times 10{\textasciicircum}6\$ integers extending from \$10{\textasciicircum}6\$ to \$3 {\textbackslash}times 10{\textasciicircum}6\$, offset by the same starting integer. While constrained by the memory capacity of our resources, which limited our analysis to a span of \$3{\textbackslash}times10{\textasciicircum}6\$, we believe that our study contribute to the application of machine learning in prime number analysis. This work aims to demonstrate the potential of such applications and hopes to inspire further exploration and possibilities in diverse fields.},
  archiveprefix = {arXiv},
  keywords = {11 68,Computer Science - Machine Learning,G.0,G.1.0,G.1.10,G.1.m,I.0,I.1.1,I.2.0,I.2.6,I.2.m,I.m,J.2,Mathematics - Number Theory},
  file = {/Users/nobr/Zotero/storage/HDKSELVI/Lee and Kim - 2024 - Exploring Prime Number Classification Achieving High Recall Rate and Rapid Convergence with Sparse .pdf;/Users/nobr/Zotero/storage/6DFHPLY7/2402.html}
}

@misc{leeGrokfastAcceleratedGrokking2024,
  title = {Grokfast: {{Accelerated Grokking}} by {{Amplifying Slow Gradients}}},
  shorttitle = {Grokfast},
  author = {Lee, Jaerin and Kang, Bong Gyun and Kim, Kihoon and Lee, Kyoung Mu},
  year = {2024},
  month = jun,
  number = {arXiv:2405.20233},
  eprint = {2405.20233},
  primaryclass = {cs},
  urldate = {2024-06-09},
  abstract = {One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data. Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than \${\textbackslash}times 50\$ with only a few lines of code that amplifies the slow-varying components of gradients. The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden generalization. Our code is available at https://github.com/ironjr/grokfast.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/H73G3VMH/Lee et al. - 2024 - Grokfast Accelerated Grokking by Amplifying Slow Gradients.pdf}
}

@article{lehman2011,
  title = {Abandoning {{Objectives}}: {{Evolution Through}} the {{Search}} for {{Novelty Alone}}},
  shorttitle = {Abandoning {{Objectives}}},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  year = {2011},
  month = jun,
  journal = {Evolutionary Computation},
  volume = {19},
  number = {2},
  pages = {189--223},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/EVCO_a_00025},
  urldate = {2023-12-08},
  abstract = {In evolutionary computation, the fitness function normally measures progress towards an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search towards dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution: Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artificial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search significantly outperforms objective-based search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/C3FEVU6A/Lehman and Stanley - 2011 - Abandoning Objectives Evolution Through the Search for Novelty Alone.pdf}
}

@misc{lehman2025,
  title = {Evolution and {{The Knightian Blindspot}} of {{Machine Learning}}},
  author = {Lehman, Joel and Meyerson, Elliot and {El-Gaaly}, Tarek and Stanley, Kenneth O. and Ziyaee, Tarin},
  year = {2025},
  month = jan,
  number = {arXiv:2501.13075},
  eprint = {2501.13075},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2501.13075},
  urldate = {2025-04-09},
  abstract = {This paper claims that machine learning (ML) largely overlooks an important facet of general intelligence: robustness to a qualitatively unknown future in an open world. Such robustness relates to Knightian uncertainty (KU) in economics, i.e. uncertainty that cannot be quantified, which is excluded from consideration in ML's key formalisms. This paper aims to identify this blind spot, argue its importance, and catalyze research into addressing it, which we believe is necessary to create truly robust open-world AI. To help illuminate the blind spot, we contrast one area of ML, reinforcement learning (RL), with the process of biological evolution. Despite staggering ongoing progress, RL still struggles in open-world situations, often failing under unforeseen situations. For example, the idea of zero-shot transferring a self-driving car policy trained only in the US to the UK currently seems exceedingly ambitious. In dramatic contrast, biological evolution routinely produces agents that thrive within an open world, sometimes even to situations that are remarkably out-of-distribution (e.g. invasive species; or humans, who do undertake such zero-shot international driving). Interestingly, evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients. We explore the assumptions underlying RL's typical formalisms, showing how they limit RL's engagement with the unknown unknowns characteristic of an ever-changing complex world. Further, we identify mechanisms through which evolutionary processes foster robustness to novel and unpredictable challenges, and discuss potential pathways to algorithmically embody them. The conclusion is that the intriguing remaining fragility of ML may result from blind spots in its formalisms, and that significant gains may result from direct confrontation with the challenge of KU.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/ZII8G8CF/Lehman et al. - 2025 - Evolution and The Knightian Blindspot of Machine Learning.pdf}
}

@misc{lehmanEvolutionLargeModels2022,
  title = {Evolution through {{Large Models}}},
  author = {Lehman, Joel and Gordon, Jonathan and Jain, Shawn and Ndousse, Kamal and Yeh, Cathy and Stanley, Kenneth O.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08896},
  eprint = {2206.08896},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-11},
  abstract = {This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/AQ9XSTR5/Lehman et al. - 2022 - Evolution through Large Models.pdf;/Users/nobr/Zotero/storage/IA5V8FBC/2206.html}
}

@misc{lei_rio2011,
  title = {Law No. 5,248 of January 27, 2011: {{Municipal}} Policy on Climate Change and Sustainable Development},
  author = {{City of Rio de Janeiro}},
  year = {2011},
  month = jan
}

@article{leibo2017,
  title = {Multi-Agent {{Reinforcement Learning}} in {{Sequential Social Dilemmas}}},
  author = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  year = {2017},
  month = feb,
  journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
  volume = {1},
  eprint = {1702.03037},
  pages = {464--473},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)}},
  issn = {15582914},
  urldate = {2023-03-22},
  abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
  archiveprefix = {arXiv},
  isbn = {9781510855076},
  keywords = {Agent-based social simulation,Cooperation,Markov games,Non-cooperative games,Social dilemmas},
  file = {/Users/nobr/Zotero/storage/3346Y88P/full-text.pdf}
}

@article{lemoult2019,
  title = {Depression: {{A}} Cognitive Perspective},
  author = {LeMoult, Joelle and Gotlib, Ian H.},
  year = {2019},
  journal = {Clinical Psychology Review},
  volume = {69},
  pages = {51--66},
  doi = {10.1016/j.cpr.2018.06.008},
  file = {/Users/nobr/Zotero/storage/XHUQCC7E/document.pdf}
}

@book{leon-garcia2008,
  title = {Probability, Statistics, and Random Processes for Electrical Engineering},
  author = {{Leon-Garcia}, Alberto and {Leon-Garcia}, Alberto},
  year = {2008},
  edition = {3rd ed},
  publisher = {Pearson/Prentice Hall},
  address = {Upper Saddle River, NJ},
  isbn = {978-0-13-147122-1},
  langid = {english},
  lccn = {TK153 .L425 2008},
  keywords = {Electrical engineering,Mathematics,Probabilities,Stochastic processes},
  annotation = {OCLC: ocn181079252},
  file = {/Users/nobr/Zotero/storage/5QHNVA5X/Leon-Garcia and Leon-Garcia - 2008 - Probability, statistics, and random processes for electrical engineering.pdf}
}

@article{lequere2018global,
  title = {Global Carbon Budget 2018},
  author = {Le Qu{\'e}r{\'e}, Corinne and others},
  year = {2018},
  journal = {Earth System Science Data},
  volume = {10},
  pages = {2141--2194},
  doi = {10.5194/essd-10-2141-2018}
}

@inproceedings{lerer2019,
  title = {Learning {{Existing Social Conventions}} via {{Observationally Augmented Self-Play}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Lerer, Adam and Peysakhovich, Alexander},
  year = {2019},
  month = jan,
  pages = {107--114},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3306618.3314268},
  urldate = {2024-01-14},
  isbn = {978-1-4503-6324-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3BCJARTH/Lerer and Peysakhovich - 2019 - Learning Existing Social Conventions via Observationally Augmented Self-Play.pdf}
}

@article{levin2019,
  title = {The {{Computational Boundary}} of a ``{{Self}}'': {{Developmental Bioelectricity Drives Multicellularity}} and {{Scale-Free Cognition}}},
  shorttitle = {The {{Computational Boundary}} of a ``{{Self}}''},
  author = {Levin, Michael},
  year = {2019},
  month = dec,
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {2688},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02688},
  urldate = {2025-03-02},
  abstract = {All epistemic agents physically consist of parts that must somehow comprise an integrated cognitive self. Biological individuals consist of subunits (organs, cells, and molecular networks) that are themselves complex and competent in their own native contexts. How do coherent biological Individuals result from the activity of smaller sub-agents? To understand the evolution and function of metazoan creatures' bodies and minds, it is essential to conceptually explore the origin of multicellularity and the scaling of the basal cognition of individual cells into a coherent larger organism. In this article, I synthesize ideas in cognitive science, evolutionary biology, and developmental physiology toward a hypothesis about the origin of Individuality: ``Scale-Free Cognition.'' I propose a fundamental definition of an Individual based on the ability to pursue goals at an appropriate level of scale and organization and suggest a formalism for defining and comparing the cognitive capacities of highly diverse types of agents. Any Self is demarcated by a computational surface -- the spatio-temporal boundary of events that it can measure, model, and try to affect. This surface sets a functional boundary a cognitive ``light cone'' which defines the scale and limits of its cognition. I hypothesize that higher level goal-directed activity and agency, resulting in larger cognitive boundaries, evolve from the primal homeostatic drive of living things to reduce stress -- the difference between current conditions and life-optimal conditions. The mechanisms of developmental bioelectricity - the ability of all cells to form electrical networks that process information suggest a plausible set of gradual evolutionary steps that naturally lead from physiological homeostasis in single cells to memory, prediction, and ultimately complex cognitive agents, via scale-up of the basic drive of infotaxis. Recent data on the molecular mechanisms of pre-neural bioelectricity suggest a model of how increasingly sophisticated cognitive functions emerge smoothly from cell-cell communication used to guide embryogenesis and regeneration. This set of hypotheses provides a novel perspective on numerous phenomena, such as cancer, and makes several unique, testable predictions for interdisciplinary research that have implications not only for evolutionary developmental biology but also for biomedicine and perhaps artificial intelligence and exobiology.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RGLQV32S/Levin - 2019 - The Computational Boundary of a “Self” Developmental Bioelectricity Drives Multicellularity and Sca.pdf}
}

@book{levy1988,
  title = {Computer {{Chess Compendium}}},
  editor = {Levy, David},
  year = {1988},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-1968-0},
  urldate = {2023-08-10},
  isbn = {978-1-4757-1970-3 978-1-4757-1968-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/QASV84KG/Levy - 1988 - Computer Chess Compendium.pdf}
}

@article{li2007,
  title = {What Do We Perceive in a Glance of a Real-World Scene?},
  author = {Li, Fei Fei and Iyer, Asha and Koch, Christof and Perona, Pietro},
  year = {2007},
  month = jan,
  journal = {Journal of Vision},
  volume = {7},
  number = {1},
  issn = {15347362},
  doi = {10.1167/7.1.10},
  abstract = {What do we see when we glance at a natural scene and how does it change as the glance becomes longer? We asked naive subjects to report in a free-form format what they saw when looking at briefly presented real-life photographs. Our subjects received no specific information as to the content of each stimulus. Thus, our paradigm differs from previous studies where subjects were cued before a picture was presented and/or were probed with multiple-choice questions. In the first stage, 90 novel grayscale photographs were foveally shown to a group of 22 native-English-speaking subjects. The presentation time was chosen at random from a set of seven possible times (from 27 to 500 ms). A perceptual mask followed each photograph immediately. After each presentation, subjects reported what they had just seen as completely and truthfully as possible. In the second stage, another group of naive individuals was instructed to score each of the descriptions produced by the subjects in the first stage. Individual scores were assigned to more than a hundred different attributes. We show that within a single glance, much object- and scene-level information is perceived by human subjects. The richness of our perception, though, seems asymmetrical. Subjects tend to have a propensity toward perceiving natural scenes as being outdoor rather than indoor. The reporting of sensory- or feature-level information of a scene (such as shading and shape) consistently precedes the reporting of the semantic-level information. But once subjects recognize more semantic-level components of a scene, there is little evidence suggesting any bias toward either scene-level or object-level recognition. {\copyright} ARVO.},
  pmid = {17461678},
  keywords = {Entry level,Event recognition,Free recall,Indoor,Natural scene,Object categorization,Object recognition,Outdoor,Perception,Real-world scene,Scene categorization,Segmentation,Sensory-level perception,Subordinate,Superordinate},
  file = {/Users/nobr/Zotero/storage/FQKUX25P/Li et al. - 2007 - What do we perceive in a glance of a real-world scene.pdf}
}

@misc{li2017,
  title = {Demystifying {{Neural Style Transfer}}},
  author = {Li, Yanghao and Wang, Naiyan and Liu, Jiaying and Hou, Xiaodi},
  year = {2017},
  month = jul,
  number = {arXiv:1701.01036},
  eprint = {1701.01036},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-11},
  abstract = {Neural Style Transfer has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/58FJ36JB/Li et al. - 2017 - Demystifying Neural Style Transfer.pdf;/Users/nobr/Zotero/storage/3WJ7H5UG/1701.html}
}

@inproceedings{li2019,
  title = {{{NATTACK}}: {{Learning}} the {{Distributions}} of {{Adversarial Examples}} for an {{Improved Black-Box Attack}} on {{Deep Neural Networks}}},
  shorttitle = {{{NATTACK}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Yandong and Li, Lijun and Wang, Liqiang and Zhang, Tong and Gong, Boqing},
  year = {2019},
  month = may,
  pages = {3866--3876},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-12-04},
  abstract = {Powerful adversarial attack methods are vital for understanding how to construct robust deep neural networks (DNNs) and for thoroughly testing defense techniques. In this paper, we propose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques developed recently. Instead of searching for an "optimal" adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this distribution is likely an adversarial example, without the need of accessing the DNN's internal layers or weights. Our approach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BER92V2G/Li et al. - 2019 - NATTACK Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep.pdf;/Users/nobr/Zotero/storage/SBR73UT5/Li et al. - 2019 - NATTACK Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep.pdf}
}

@article{li2021,
  title = {{{BrainGNN}}: {{Interpretable Brain Graph Neural Network}} for {{fMRI Analysis}}},
  shorttitle = {{{BrainGNN}}},
  author = {Li, Xiaoxiao and Zhou, Yuan and Dvornek, Nicha and Zhang, Muhan and Gao, Siyuan and Zhuang, Juntang and Scheinost, Dustin and Staib, Lawrence H. and Ventola, Pamela and Duncan, James S.},
  year = {2021},
  month = dec,
  journal = {Medical Image Analysis},
  volume = {74},
  pages = {102233},
  issn = {1361-8415},
  doi = {10.1016/j.media.2021.102233},
  urldate = {2023-05-16},
  abstract = {Understanding which brain regions are related to a specific neurological disorder or cognitive stimuli has been an important area of neuroimaging research. We propose BrainGNN, a graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI) and discover neurological biomarkers. Considering the special property of brain graphs, we design novel ROI-aware graph convolutional (Ra-GConv) layers that leverage the topological and functional information of fMRI. Motivated by the need for transparency in medical image analysis, our BrainGNN contains ROI-selection pooling layers (R-pool) that highlight salient ROIs (nodes in the graph), so that we can infer which ROIs are important for prediction. Furthermore, we propose regularization terms---unit loss, topK pooling (TPK) loss and group-level consistency (GLC) loss---on pooling results to encourage reasonable ROI-selection and provide flexibility to encourage either fully individual- or patterns that agree with group-level data. We apply the BrainGNN framework on two independent fMRI datasets: an Autism Spectrum Disorder (ASD) fMRI dataset and data from the Human Connectome Project (HCP) 900 Subject Release. We investigate different choices of the hyper-parameters and show that BrainGNN outperforms the alternative fMRI image analysis methods in terms of four different evaluation metrics. The obtained community clustering and salient ROI detection results show a high correspondence with the previous neuroimaging-derived evidence of biomarkers for ASD and specific task states decoded for HCP. Our code is available at https://github.com/xxlya/BrainGNN\_Pytorch},
  langid = {english},
  keywords = {ASD,Biomarker,fMRI,GNN},
  file = {/Users/nobr/Zotero/storage/KQ2D3E6F/Li et al. - 2021 - BrainGNN Interpretable Brain Graph Neural Network.pdf;/Users/nobr/Zotero/storage/KG8B95US/S1361841521002784.html}
}

@article{li2021a,
  title = {{{AI Choreographer}}: {{Music Conditioned 3D Dance Generation}} with {{AIST}}++},
  author = {Li, Ruilong and Yang, Shan and Ross, David A. and Kanazawa, Angjoo},
  year = {2021},
  month = jan,
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  eprint = {2101.08779},
  pages = {13381--13392},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15505499},
  doi = {10.1109/ICCV48922.2021.01315},
  urldate = {2023-03-22},
  abstract = {We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses -- the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict \$N\$ future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively.},
  archiveprefix = {arXiv},
  isbn = {9781665428125},
  file = {/Users/nobr/Zotero/storage/92N49KHD/full-text.pdf}
}

@misc{li2023,
  title = {Self-{{Confirming Transformer}} for {{Locally Consistent Online Adaptation}} in {{Multi-Agent Reinforcement Learning}}},
  author = {Li, Tao and Guevara, Juan and Xie, Xinghong and Zhu, Quanyan},
  year = {2023},
  month = oct,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Offline reinforcement learning (RL) leverages previously collected data to extract policies that return satisfying performance in online environments. However, offline RL suffers from the distribution shift between the offline dataset and the online environment. In the multi-agent RL (MARL) setting, this distribution shift may arise from the nonstationary opponents (exogenous agents beyond control) in the online testing who display distinct behaviors from those recorded in the offline dataset. Hence, the key to the broader deployment of offline MARL is the online adaptation to nonstationary opponents. Recent advances in large language models have demonstrated the surprising generalization ability of the transformer architecture in sequence modeling, which prompts one to wonder whether the offline-trained transformer policy adapts to nonstationary opponents during online testing. This work proposes the self-confirming loss (SCL) in offline transformer training to address the online nonstationarity, which is motivated by the self-confirming equilibrium (SCE) in game theory. The gist is that the transformer learns to predict the opponents' future moves based on which it acts accordingly. As a weaker variant of Nash equilibrium (NE), SCE (equivalently, SCL) only requires local consistency: the agent's local observations do not deviate from its conjectures, leading to a more adaptable policy than the one dictated by NE focusing on global optimality. We evaluate the online adaptability of the self-confirming transformer (SCT) by playing against nonstationary opponents employing a variety of policies, from the random one to the benchmark MARL policies. Experimental results demonstrate that SCT can adapt to nonstationary opponents online, achieving higher returns than vanilla transformers and offline MARL baselines.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/FPIWNU77/Li et al. - 2023 - Self-Confirming Transformer for Locally Consistent Online Adaptation in Multi-Agent Reinforcement Le.pdf}
}

@article{li2024,
  title = {Discovering Neural Policies to Drive Behaviour by Integrating Deep Reinforcement Learning Agents with Biological Neural Networks},
  author = {Li, Chenguang and Kreiman, Gabriel and Ramanathan, Sharad},
  year = {2024},
  month = jun,
  journal = {Nature Machine Intelligence},
  volume = {6},
  number = {6},
  pages = {726--738},
  issn = {2522-5839},
  doi = {10.1038/s42256-024-00854-2},
  urldate = {2024-09-05},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/QPDNTP7N/gk8172.pdf}
}

@misc{lim2024,
  title = {Large {{Language Models}} as {{In-context AI Generators}} for {{Quality-Diversity}}},
  author = {Lim, Bryan and Flageat, Manon and Cully, Antoine},
  year = {2024},
  month = jun,
  number = {arXiv:2404.15794},
  eprint = {2404.15794},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2404.15794},
  urldate = {2025-01-01},
  abstract = {Quality-Diversity (QD) approaches are a promising direction to develop open-ended processes as they can discover archives of high-quality solutions across diverse niches. While already successful in many applications, QD approaches usually rely on combining only one or two solutions to generate new candidate solutions. As observed in open-ended processes such as technological evolution, wisely combining large diversity of these solutions could lead to more innovative solutions and potentially boost the productivity of QD search. In this work, we propose to exploit the pattern-matching capabilities of generative models to enable such efficient solution combinations. We introduce In-context QD, a framework of techniques that aim to elicit the in-context capabilities of pre-trained Large Language Models (LLMs) to generate interesting solutions using few-shot and many-shot prompting with quality-diverse examples from the QD archive as context. Applied to a series of common QD domains, In-context QD displays promising results compared to both QD baselines and similar strategies developed for single-objective optimization. Additionally, this result holds across multiple values of parameter sizes and archive population sizes, as well as across domains with distinct characteristics from BBO functions to policy search. Finally, we perform an extensive ablation that highlights the key prompt design considerations that encourage the generation of promising solutions for QD.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/RYSX42WR/Lim et al. - 2024 - Large Language Models as In-context AI Generators for Quality-Diversity.pdf}
}

@misc{limAcceleratedQualityDiversityMassive2022,
  title = {Accelerated {{Quality-Diversity}} through {{Massive Parallelism}}},
  author = {Lim, Bryan and Allard, Maxime and Grillotti, Luca and Cully, Antoine},
  year = {2022},
  month = oct,
  number = {arXiv:2202.01258},
  eprint = {2202.01258},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {Quality-Diversity (QD) optimization algorithms are a well-known approach to generate large collections of diverse and high-quality solutions. However, derived from evolutionary computation, QD algorithms are population-based methods which are known to be data-inefficient and require large amounts of computational resources. This makes QD algorithms slow when used in applications where solution evaluations are computationally costly. A common approach to speed up QD algorithms is to evaluate solutions in parallel, for instance by using physical simulators in robotics. Yet, this approach is limited to several dozen of parallel evaluations as most physics simulators can only be parallelized more with a greater number of CPUs. With recent advances in simulators that run on accelerators, thousands of evaluations can now be performed in parallel on single GPU/TPU. In this paper, we present QDax, an accelerated implementation of MAP-Elites which leverages massive parallelism on accelerators to make QD algorithms more accessible. We show that QD algorithms are ideal candidates to take advantage of progress in hardware acceleration. We demonstrate that QD algorithms can scale with massive parallelism to be run at interactive timescales without any significant effect on the performance. Results across standard optimization functions and four neuroevolution benchmark environments show that experiment runtimes are reduced by two factors of magnitudes, turning days of computation into minutes. More surprising, we observe that reducing the number of generations by two orders of magnitude, and thus having significantly shorter lineage does not impact the performance of QD algorithms. These results show that QD can now benefit from hardware acceleration, which contributed significantly to the bloom of deep learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/GVC3EZM9/Lim et al. - 2022 - Accelerated Quality-Diversity through Massive Parallelism.pdf}
}

@article{lin-greenberg2022,
  title = {Wargaming for {{International Relations}} Research},
  author = {{Lin-Greenberg}, Erik and Pauly, Reid B.C. and Schneider, Jacquelyn G.},
  year = {2022},
  month = mar,
  journal = {European Journal of International Relations},
  volume = {28},
  number = {1},
  pages = {83--109},
  publisher = {SAGE Publications Ltd},
  issn = {1354-0661},
  doi = {10.1177/13540661211064090},
  urldate = {2024-09-09},
  abstract = {Political scientists are increasingly integrating wargames into their research. Either by fielding original games or by leveraging archival wargame materials, researchers can study rare events or topics where evidence is difficult to observe. However, scholars have little guidance on how to apply this novel methodological approach to political science research. This article evaluates how political scientists can use wargames as a method of scholarly inquiry and sets out to establish a research agenda for wargaming in International Relations. We first differentiate wargames from other methodological approaches and highlight their ecological validity. We then chart out how researchers can build and run their own games or draw from archival wargames for theory development and testing. In doing so, we explain how researchers can navigate issues of recruitment, bias, validity, and generalizability when using wargames for research, and identify ways to evaluate the potential benefits and pitfalls of wargames as a tool of inquiry. We argue that wargames offer unique opportunities for political scientists to study decision-making processes both in and beyond the International Relations subfield.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/63V5HYQ3/Lin-Greenberg et al. - 2022 - Wargaming for International Relations research.pdf}
}

@misc{lin2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
  year = {2015},
  month = feb,
  number = {arXiv:1405.0312},
  eprint = {1405.0312},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-08},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/99JHN779/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/Users/nobr/Zotero/storage/LFKJXTSD/1405.html}
}

@misc{lin2022,
  title = {Mind {{Reader}}: {{Reconstructing}} Complex Images from Brain Activities},
  shorttitle = {Mind {{Reader}}},
  author = {Lin, Sikun and Sprague, Thomas and Singh, Ambuj K.},
  year = {2022},
  month = sep,
  number = {arXiv:2210.01769},
  eprint = {2210.01769},
  primaryclass = {cs, eess, q-bio},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {Understanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from fMRI (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of fMRI datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level fMRI signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode fMRI signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/6HRB4LCE/Lin et al. - 2022 - Mind Reader Reconstructing complex images from br.pdf;/Users/nobr/Zotero/storage/JFU745YQ/2210.html}
}

@misc{lin2023,
  title = {Generating {{Structured Output}} from {{LLMs}}},
  author = {Lin, Timothy},
  year = {2023},
  month = nov,
  journal = {Quasilinear Musings},
  urldate = {2024-02-08},
  abstract = {A survey on the different methodologies used to generate structured output from LLMs, from model fine-tuning, to domain specific language, and schema engineering.},
  howpublished = {https://www.timlrx.com/blog/generating-structured-output-from-llms},
  langid = {american},
  file = {/Users/nobr/Zotero/storage/EUBKKPAL/generating-structured-output-from-llms.html}
}

@article{lindsay2017,
  title = {The Conceptual Penis as a Social Construct},
  author = {Lindsay, Jamie and Boyle, Peter},
  year = {2017},
  month = jan,
  journal = {Cogent Social Sciences},
  volume = {3},
  number = {1},
  pages = {1330439},
  issn = {2331-1886},
  doi = {10.1080/23311886.2017.1330439},
  urldate = {2024-01-20},
  abstract = {Anatomical penises may exist, but as pre-operative transgendered women also have anatomical penises, the penis vis-{\`a}-vis maleness is an incoherent construct. We argue that the conceptual penis is better understood not as an anatomical organ but as a social construct isomorphic to performative toxic masculinity. Through detailed poststructuralist discursive criticism and the example of climate change, this paper will challenge the prevailing and damaging social trope that penises are best understood as the male sexual organ and reassign it a more fitting role as a type of masculine performance.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DRZ739HF/Lindsay and Boyle - 2017 - The conceptual penis as a social construct.pdf}
}

@misc{lindsayFeaturebasedAttentionConvolutional2015,
  title = {Feature-Based {{Attention}} in {{Convolutional Neural Networks}}},
  author = {Lindsay, Grace W.},
  year = {2015},
  month = dec,
  number = {arXiv:1511.06408},
  eprint = {1511.06408},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06408},
  urldate = {2023-11-16},
  abstract = {Convolutional neural networks (CNNs) have proven effective for image processing tasks, such as object recognition and classification. Recently, CNNs have been enhanced with concepts of attention, similar to those found in biology. Much of this work on attention has focused on effective serial spatial processing. In this paper, I introduce a simple procedure for applying feature-based attention (FBA) to CNNs and compare multiple implementation options. FBA is a top-down signal applied globally to an input image which aides in detecting chosen objects in cluttered or noisy settings. The concept of FBA and the implementation details tested here were derived from what is known (and debated) about biological object- and feature-based attention. The implementations of FBA described here increase performance on challenging object detection tasks using a procedure that is simple, fast, and does not require additional iterative training. Furthermore, the comparisons performed here suggest that a proposed model of biological FBA (the "feature similarity gain model") is effective in increasing performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/TXCWE4Y4/Lindsay - 2015 - Feature-based Attention in Convolutional Neural Networks.pdf;/Users/nobr/Zotero/storage/ZFEUAF5N/1511.html}
}

@misc{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2018},
  month = feb,
  number = {arXiv:1708.02002},
  eprint = {1708.02002},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-03},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/GNSREZH5/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf}
}

@misc{linVideoLLaVALearningUnited2023,
  title = {Video-{{LLaVA}}: {{Learning United Visual Representation}} by {{Alignment Before Projection}}},
  shorttitle = {Video-{{LLaVA}}},
  author = {Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li},
  year = {2023},
  month = nov,
  number = {arXiv:2311.10122},
  eprint = {2311.10122},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-23},
  abstract = {The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8\%, 9.9\%, 18.6\%, and 10.1\% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/U9IBMM3U/Lin et al. - 2023 - Video-LLaVA Learning United Visual Representation by Alignment Before Projection.pdf}
}

@article{lipton2018,
  title = {The {{Mythos}} of {{Model Interpretability}}: {{In}} Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  shorttitle = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  year = {2018},
  month = jun,
  journal = {Queue},
  volume = {16},
  number = {3},
  pages = {31--57},
  issn = {1542-7730},
  doi = {10.1145/3236386.3241340},
  urldate = {2024-05-27},
  abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
  file = {/Users/nobr/Zotero/storage/8IMS5893/Lipton - 2018 - The Mythos of Model Interpretability In machine learning, the concept of interpretability is both important and slippery..pdf}
}

@book{lispector2012,
  title = {{\'A}gua Viva},
  author = {Lispector, Clarice and Tobler, Stefan and Moser, Benjamin and Lispector, Clarice},
  year = {2012},
  publisher = {New Directions},
  address = {New York},
  isbn = {978-0-8112-1990-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6VG4DHUE/Lispector et al. - 2012 - Água viva.pdf}
}

@misc{liSurveyTransformersReinforcement2023,
  title = {A {{Survey}} on {{Transformers}} in {{Reinforcement Learning}}},
  author = {Li, Wenzhe and Luo, Hao and Lin, Zichuan and Zhang, Chongjie and Lu, Zongqing and Ye, Deheng},
  year = {2023},
  month = sep,
  number = {arXiv:2301.03044},
  eprint = {2301.03044},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-21},
  abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/X7ABCJ88/Li et al. - 2023 - A Survey on Transformers in Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/QEHWT3IV/2301.html}
}

@article{liu2019,
  title = {Toward {{AI}} Fashion Design: {{An Attribute-GAN}} Model for Clothing Match},
  author = {Liu, Linlin and Zhang, Haijun and Ji, Yuzhu and Jonathan Wu, Q. M.},
  year = {2019},
  month = may,
  journal = {Neurocomputing},
  volume = {341},
  pages = {156--167},
  publisher = {Elsevier B.V.},
  issn = {18728286},
  doi = {10.1016/j.neucom.2019.03.011},
  abstract = {Dressing in clothes based on the matching rules of color, texture, shape, etc., can have a major impact on perception, including making people appear taller or thinner, as well as exhibiting personal style. Unlike the extant fashion mining literature, in which style is usually classified according to similarity, this paper investigates clothing match rules based on semantic attributes according to the generative adversarial network (GAN) model. Specifically, we propose an Attribute-GAN to generate clothing-match pairs automatically. The core of Attribute-GAN constitutes training a generator, supervised by an adversarial trained collocation discriminator and attribute discriminator. To implement the Attributed-GAN, we built a large-scale outfit dataset by ourselves and annotated clothing attributes manually. Extensive experimental results confirm the effectiveness of our proposed method in comparison to several state-of-the-art methods.},
  keywords = {Attribute,Clothing match,Fashion data,Generative adversarial network},
  file = {/Users/nobr/Zotero/storage/KA9FIWHG/1-s2.0-S0925231219303133-main.pdf}
}

@article{liu2021,
  title = {Self-Play Reinforcement Learning with Comprehensive Critic in Computer Games},
  author = {Liu, Shanqi and Cao, Junjie and Wang, Yujie and Chen, Wenzhou and Liu, Yong},
  year = {2021},
  month = aug,
  journal = {Neurocomputing},
  volume = {449},
  pages = {207--213},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.04.006},
  urldate = {2024-01-08},
  abstract = {Self-play reinforcement learning, where agents learn by playing with themselves, has been successfully applied in many game scenarios. However, the training procedure for self-play reinforcement learning is unstable and more sample-inefficient than (general) reinforcement learning, especially in imperfect information games. To improve the self-play training process, we incorporate a comprehensive critic into the policy gradient method to form a self-play actor-critic (SPAC) method for training agents to play computer games. We evaluate our method in four different environments in both competitive and cooperative tasks. The results show that the agent trained with our SPAC method outperforms those trained with deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO) algorithms in many different evaluation approaches, which vindicate the effect of our comprehensive critic in the self-play training procedure.},
  keywords = {Computer game,Reinforcement learning,Self-play},
  file = {/Users/nobr/Zotero/storage/5FLHQT6C/Liu et al. - 2021 - Self-play reinforcement learning with comprehensive critic in computer games.pdf;/Users/nobr/Zotero/storage/IGQLEZ7D/S0925231221005245.html}
}

@article{liu2022,
  title = {Context-{{Aware Taxi Dispatching}} at {{City-Scale Using Deep Reinforcement Learning}}},
  author = {Liu, Zhidan and Li, Jiangzhou and Wu, Kaishun},
  year = {2022},
  month = mar,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {3},
  pages = {1996--2009},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2020.3030252},
  urldate = {2023-12-05},
  abstract = {Proactive taxi dispatching is of great importance to balance taxi demand-supply gaps among different locations in a city. Recent advances primarily rely on deep reinforcement learning (DRL) to directly learn the optimal dispatching policy. These works, however, are still not sufficiently efficient because they overlook several pieces of valuable context information. As a result, they may generate quite a few improper actions and introduce unnecessary coordination costs. To improve existing works, we present COX -- a context-aware taxi dispatching approach that incorporates rich contexts into DRL modeling for more efficient taxi reallocations. Specifically, rather than simply dividing the service area into grids, COX proposes a road connectivity aware clustering algorithm to divide the road network graph into zones for practical taxi dispatching. In addition, COX comprehensively analyzes zone-level taxi demands and supplies through accurate taxi demand prediction and timely updates of taxi statuses. COX improves the DRL modeling by integrating these derived contexts, e.g., state representation with complete demand/supply data and sequential action generation with full coordination among idle taxis. In particular, we implement an environment simulator to train and evaluate COX using a large real-world taxi dataset. Extensive experiments show that COX outperforms state-of-the-art approaches on various performance metrics, e.g., on average improving the total order values by 6.74\%, while reducing the number of unserved taxi orders and passengers' waiting time by 4.92\% and 44.84\%, respectively.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IMBEJ5GE/Liu et al. - 2022 - Context-Aware Taxi Dispatching at City-Scale Using Deep Reinforcement Learning.pdf}
}

@misc{liu2022a,
  title = {Towards {{Understanding Grokking}}: {{An Effective Theory}} of {{Representation Learning}}},
  shorttitle = {Towards {{Understanding Grokking}}},
  author = {Liu, Ziming and Kitouni, Ouail and Nolte, Niklas and Michaud, Eric J. and Tegmark, Max and Williams, Mike},
  year = {2022},
  month = oct,
  number = {arXiv:2205.10343},
  eprint = {2205.10343},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.10343},
  urldate = {2024-11-06},
  abstract = {We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. We find on transformers the grokking phase stays closer to the memorization phase (compared to the comprehension phase), leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Physics - Classical Physics},
  file = {/Users/nobr/Zotero/storage/GD9QTYN6/Liu et al. - 2022 - Towards Understanding Grokking An Effective Theory of Representation Learning.pdf;/Users/nobr/Zotero/storage/4LZDXIUT/2205.html}
}

@misc{liu2023,
  title = {Seeing Is {{Believing}}: {{Brain-Inspired Modular Training}} for {{Mechanistic Interpretability}}},
  shorttitle = {Seeing Is {{Believing}}},
  author = {Liu, Ziming and Gan, Eric and Tegmark, Max},
  year = {2023},
  month = may,
  number = {arXiv:2305.08746},
  eprint = {2305.08746},
  primaryclass = {cond-mat, q-bio},
  doi = {10.48550/arXiv.2305.08746},
  urldate = {2023-05-18},
  abstract = {We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Representation Theory,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/FFQF273M/Liu et al. - 2023 - Seeing is Believing Brain-Inspired Modular Training for Mechanistic Interpretability.pdf}
}

@misc{liu2023d,
  title = {Omnigrok: {{Grokking Beyond Algorithmic Data}}},
  shorttitle = {Omnigrok},
  author = {Liu, Ziming and Michaud, Eric J. and Tegmark, Max},
  year = {2023},
  month = mar,
  number = {arXiv:2210.01117},
  eprint = {2210.01117},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.01117},
  urldate = {2024-11-06},
  abstract = {Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Physics - Data Analysis Statistics and Probability,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/nobr/Zotero/storage/WUQUF7MC/Liu et al. - 2023 - Omnigrok Grokking Beyond Algorithmic Data.pdf;/Users/nobr/Zotero/storage/ZPKZ8F54/2210.html}
}

@misc{liu2024a,
  title = {Improved {{Baselines}} with {{Visual Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  year = {2024},
  month = may,
  number = {arXiv:2310.03744},
  eprint = {2310.03744},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03744},
  urldate = {2024-05-23},
  abstract = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in {\textasciitilde}1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/HZ8QGVAR/Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf;/Users/nobr/Zotero/storage/JEZSCHHY/2310.html}
}

@misc{liu2024d,
  title = {Mind {{Your Step}} (by {{Step}}): {{Chain-of-Thought}} Can {{Reduce Performance}} on {{Tasks}} Where {{Thinking Makes Humans Worse}}},
  shorttitle = {Mind {{Your Step}} (by {{Step}})},
  author = {Liu, Ryan and Geng, Jiayi and Wu, Addison J. and Sucholutsky, Ilia and Lombrozo, Tania and Griffiths, Thomas L.},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21333},
  eprint = {2410.21333},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21333},
  urldate = {2024-10-30},
  abstract = {Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3\% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/ZVZBDMJ2/Liu et al. - 2024 - Mind Your Step (by Step) Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Huma.pdf;/Users/nobr/Zotero/storage/SGNIADV4/2410.html}
}

@misc{liuKANKolmogorovArnoldNetworks2024,
  title = {{{KAN}}: {{Kolmogorov-Arnold Networks}}},
  shorttitle = {{{KAN}}},
  author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v c}i{\'c}, Marin and Hou, Thomas Y. and Tegmark, Max},
  year = {2024},
  month = may,
  number = {arXiv:2404.19756},
  eprint = {2404.19756},
  primaryclass = {cond-mat, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.19756},
  urldate = {2024-05-06},
  abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/HUENEBFS/Liu et al. - 2024 - KAN Kolmogorov-Arnold Networks.pdf;/Users/nobr/Zotero/storage/HMFQYA8N/2404.html}
}

@misc{liuPromptingFrameworksLarge2023,
  title = {Prompting {{Frameworks}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Prompting {{Frameworks}} for {{Large Language Models}}},
  author = {Liu, Xiaoxia and Wang, Jingyi and Sun, Jun and Yuan, Xiaohan and Dong, Guoliang and Di, Peng and Wang, Wenhai and Wang, Dongxia},
  year = {2023},
  month = nov,
  number = {arXiv:2311.12785},
  eprint = {2311.12785},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-28},
  abstract = {Since the launch of ChatGPT, a powerful AI Chatbot developed by OpenAI, large language models (LLMs) have made significant advancements in both academia and industry, bringing about a fundamental engineering paradigm shift in many areas. While LLMs are powerful, it is also crucial to best use their power where ``prompt'' plays a core role. However, the booming LLMs themselves, including excellent APIs like ChatGPT, have several inherent limitations: 1) temporal lag of training data, and 2) the lack of physical capabilities to perform external actions. Recently, we have observed the trend of utilizing prompt-based tools to better utilize the power of LLMs for downstream tasks, but a lack of systematic literature and standardized terminology, partly due to the rapid evolution of this field. Therefore, in this work, we survey related prompting tools and promote the concept of the ``Prompting Framework" (PF), i.e. the framework for managing, simplifying, and facilitating interaction with large language models. We define the lifecycle of the PF as a hierarchical structure, from bottom to top, namely: Data Level, Base Level, Execute Level, and Service Level. We also systematically depict the overall landscape of the emerging PF field and discuss potential future research and challenges. To continuously track the developments in this area, we maintain a repository at https://github.com/lxx0628/Prompting-Framework-Survey, which can be a useful resource sharing platform for both academic and industry in this field. CCS Concepts: {$\bullet$} Computing methodologies {$\rightarrow$} Natural language processing; {$\bullet$} Software and its engineering {$\rightarrow$} Development frameworks and environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/nobr/Zotero/storage/4PQBPNR8/Liu et al. - 2023 - Prompting Frameworks for Large Language Models A Survey.pdf}
}

@misc{liuThinkinMemoryRecallingPostthinking2023,
  title = {Think-in-{{Memory}}: {{Recalling}} and {{Post-thinking Enable LLMs}} with {{Long-Term Memory}}},
  shorttitle = {Think-in-{{Memory}}},
  author = {Liu, Lei and Yang, Xiaoyan and Shen, Yue and Hu, Binbin and Zhang, Zhiqiang and Gu, Jinjie and Zhang, Guannan},
  year = {2023},
  month = nov,
  number = {arXiv:2311.08719},
  eprint = {2311.08719},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-06},
  abstract = {Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, {\textbackslash}textit\{i.e.\}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, ({\textbackslash}textit\{i.e.\}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/PLN5QUCR/Liu et al. - 2023 - Think-in-Memory Recalling and Post-thinking Enable LLMs with Long-Term Memory.pdf;/Users/nobr/Zotero/storage/FLB8HF38/2311.html}
}

@misc{liuVisualInstructionTuning2023,
  title = {Visual {{Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  year = {2023},
  month = dec,
  number = {arXiv:2304.08485},
  eprint = {2304.08485},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for generalpurpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/KMLDE6GG/Liu et al. - 2023 - Visual Instruction Tuning.pdf}
}

@article{loconte2023,
  title = {How to {{Turn Your Knowledge Graph Embeddings}} into {{Generative Models}}},
  author = {Loconte, Lorenzo and Di Mauro, Nicola and Peharz, Robert and Vergari, Antonio},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.15944},
  urldate = {2024-11-18},
  abstract = {Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/nobr/Zotero/storage/JTEQCLLZ/Loconte et al. - 2023 - How to Turn Your Knowledge Graph Embeddings into Generative Models.pdf}
}

@article{lore2024,
  title = {Strategic Behavior of Large Language Models and the Role of Game Structure versus Contextual Framing},
  author = {Lor{\`e}, Nunzio and Heydari, Babak},
  year = {2024},
  month = aug,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {18490},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-69032-z},
  urldate = {2024-12-04},
  abstract = {This paper investigates the strategic behavior of large language models (LLMs) across various game-theoretic settings, scrutinizing the interplay between game structure and contextual framing in decision-making. We focus our analysis on three advanced LLMs---GPT-3.5, GPT-4, and LLaMa-2---and how they navigate both the intrinsic aspects of different games and the nuances of their surrounding contexts. Our results highlight discernible patterns in each model's strategic approach. GPT-3.5 shows significant sensitivity to context but lags in its capacity for abstract strategic decision making. Conversely, both GPT-4 and LLaMa-2 demonstrate a more balanced sensitivity to game structures and contexts, albeit with crucial differences. Specifically, GPT-4 prioritizes the internal mechanics of the game over its contextual backdrop but does so with only a coarse differentiation among game types. In contrast, LLaMa-2 reflects a more granular understanding of individual game structures, while also giving due weight to contextual elements. This suggests that LLaMa-2 is better equipped to navigate the subtleties of different strategic scenarios while also incorporating context into its decision-making, whereas GPT-4 adopts a more generalized, structure-centric strategy.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational science,Psychology and behaviour},
  file = {/Users/nobr/Zotero/storage/DG8KMHAW/Lorè and Heydari - 2024 - Strategic behavior of large language models and the role of game structure versus contextual framing.pdf}
}

@misc{loreStrategicBehaviorLarge2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Strategic {{Behavior}} of {{Large Language Models}}: {{Game Structure}} vs. {{Contextual Framing}}},
  shorttitle = {Strategic {{Behavior}} of {{Large Language Models}}},
  author = {Lor{\`e}, Nunzio and Heydari, Babak},
  year = {2023},
  month = sep,
  number = {4569717},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4569717},
  urldate = {2023-12-15},
  abstract = {This paper investigates the strategic behavior of Large Language Models (LLMs) across various game-theoretic settings, scrutinizing the interplay between game structure and contextual framing in decision-making. We focus our analysis on three advanced LLMs---GPT-3.5, GPT-4, and LLaMa-2---and how they negotiate both the intrinsic aspects of different games and the nuances of their surrounding contexts.Our results highlight discernable patterns in each model's strategic approach. GPT-3.5 shows significant sensitivity to context but lags in its capacity for abstract strategic thinking. Conversely, both GPT-4 and LLaMa-2 demonstrate a more balanced sensitivity to game structures and contexts, albeit with distinct nuances. Specifically, GPT-4 prioritizes the internal mechanics of the game over its contextual backdrop but does so without nuanced differentiation between game types. In contrast, LLaMa-2 reflects a more granular understanding of individual game structures, while also giving due weight to contextual elements. This suggests that LLaMa-2 is better equipped to navigate the subtleties of different strategic scenarios while also incorporating context into its decision-making, whereas GPT-4 adopts a more generalized, structure-centric strategy.},
  langid = {english},
  keywords = {Context Effect,Game Theory,Generative AI,Large Language Models,Social Dilemma,Strategic Thinking},
  file = {/Users/nobr/Zotero/storage/8N5PQM2R/Lorè and Heydari - 2023 - Strategic Behavior of Large Language Models Game Structure vs. Contextual Framing.pdf}
}

@misc{loshchilov2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  number = {arXiv:1711.05101},
  eprint = {1711.05101},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.05101},
  urldate = {2024-11-13},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/nobr/Zotero/storage/5UMC3RWR/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/nobr/Zotero/storage/P3XZMFGS/1711.html}
}

@article{lovejoy2018amazon,
  title = {Amazon Tipping Point},
  author = {Lovejoy, Thomas E. and Nobre, Carlos},
  year = {2018},
  journal = {Science Advances},
  volume = {4},
  number = {2},
  pages = {eaat2340},
  doi = {10.1126/sciadv.aat2340}
}

@article{lu2016,
  title = {Learning from the Ubiquitous Language},
  author = {Lu, Xuan and Ai, Wei and Liu, Xuanzhe and Li, Qian and Wang, Ning and Huang, Gang and Mei, Qiaozhu},
  year = {2016},
  pages = {770--780},
  doi = {10.1145/2971648.2971724},
  file = {/Users/nobr/Zotero/storage/S2BRC52W/Lu et al. - 2016 - Learning from the ubiquitous language.pdf}
}

@article{lu2021,
  title = {Research on {{Dynamic Evolution Model}} and {{Method}} of {{Communication Network Based}} on {{Real War Game}}},
  author = {Lu, Tongliang and Chen, Kai and Zhang, Yan and Deng, Qiling},
  year = {2021},
  month = apr,
  journal = {Entropy},
  volume = {23},
  number = {4},
  pages = {487},
  issn = {1099-4300},
  doi = {10.3390/e23040487},
  urldate = {2023-11-19},
  abstract = {Based on the data in real combat games, the combat System-of-Systems is usually composed of a large number of armed equipment platforms (or systems) and a reasonable communication network to connect mutually independent weapons and equipment platforms to achieve tasks such as information collection, sharing, and collaborative processing. However, the generation algorithm of the combat system in the existing research is too simple and not suitable for reality. To overcome this problem, this paper proposes a communication network generation algorithm by adopting the joint distribution strategy of power law distribution and Poisson distribution to model the communication network. The simulation method is used to study the operation under continuous attack on communication nodes. The comprehensive experimental results of the dynamic evolution of the combat network in the battle scene verify the rationality and effectiveness of the communication network construction.},
  pmcid = {PMC8072527},
  pmid = {33923997},
  file = {/Users/nobr/Zotero/storage/5CI6HMKS/Lu et al. - 2021 - Research on Dynamic Evolution Model and Method of Communication Network Based on Real War Game.pdf}
}

@misc{lu2022,
  title = {Adversarial {{Cheap Talk}}},
  author = {Lu, Chris and Willi, Timon and Letcher, Alistair and Foerster, Jakob},
  year = {2022},
  month = nov,
  number = {arXiv:2211.11030},
  eprint = {2211.11030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.11030},
  urldate = {2023-05-01},
  abstract = {Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT can still significantly influence the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorithms. More specifically, we show that an ACT Adversary is capable of harming performance by interfering with the learner's function approximation, or instead helping the Victim's performance by outputting useful features. Finally, we show that an ACT Adversary can manipulate messages during train-time to directly and arbitrarily control the Victim at test-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/TVI3IQST/Lu et al. - 2022 - Adversarial Cheap Talk.pdf;/Users/nobr/Zotero/storage/3FW4CMEV/2211.html}
}

@misc{lu2022a,
  title = {Discovered {{Policy Optimisation}}},
  author = {Lu, Chris and Kuba, Jakub Grudzien and Letcher, Alistair and Metz, Luke and {de Witt}, Christian Schroeder and Foerster, Jakob},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05639},
  eprint = {2210.05639},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.05639},
  urldate = {2023-05-01},
  abstract = {Tremendous progress has been made in reinforcement learning (RL) over the past decade. Most of these advancements came through the continual development of new algorithms, which were designed using a combination of mathematical derivations, intuitions, and experimentation. Such an approach of creating algorithms manually is limited by human understanding and ingenuity. In contrast, meta-learning provides a toolkit for automatic machine learning method optimisation, potentially addressing this flaw. However, black-box approaches which attempt to discover RL algorithms with minimal prior structure have thus far not outperformed existing hand-crafted algorithms. Mirror Learning, which includes RL algorithms, such as PPO, offers a potential middle-ground starting point: while every method in this framework comes with theoretical guarantees, components that differentiate them are subject to design. In this paper we explore the Mirror Learning space by meta-learning a "drift" function. We refer to the immediate result as Learnt Policy Optimisation (LPO). By analysing LPO we gain original insights into policy optimisation which we use to formulate a novel, closed-form RL algorithm, Discovered Policy Optimisation (DPO). Our experiments in Brax environments confirm state-of-the-art performance of LPO and DPO, as well as their transfer to unseen settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/A7HRPNMB/Lu et al. - 2022 - Discovered Policy Optimisation.pdf;/Users/nobr/Zotero/storage/UL4M4SNA/2210.html}
}

@misc{lu2022b,
  title = {Model-{{Free Opponent Shaping}}},
  author = {Lu, Chris and Willi, Timon and {de Witt}, Christian Schroeder and Foerster, Jakob},
  year = {2022},
  month = nov,
  number = {arXiv:2205.01447},
  eprint = {2205.01447},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.01447},
  urldate = {2023-05-01},
  abstract = {In general-sum games, the interaction of self-interested learning agents commonly leads to collectively worst-case outcomes, such as defect-defect in the iterated prisoner's dilemma (IPD). To overcome this, some methods, such as Learning with Opponent-Learning Awareness (LOLA), shape their opponents' learning process. However, these methods are myopic since only a small number of steps can be anticipated, are asymmetric since they treat other agents as naive learners, and require the use of higher-order derivatives, which are calculated through white-box access to an opponent's differentiable learning algorithm. To address these issues, we propose Model-Free Opponent Shaping (M-FOS). M-FOS learns in a meta-game in which each meta-step is an episode of the underlying inner game. The meta-state consists of the inner policies, and the meta-policy produces a new inner policy to be used in the next episode. M-FOS then uses generic model-free optimisation methods to learn meta-policies that accomplish long-horizon opponent shaping. Empirically, M-FOS near-optimally exploits naive learners and other, more sophisticated algorithms from the literature. For example, to the best of our knowledge, it is the first method to learn the well-known Zero-Determinant (ZD) extortion strategy in the IPD. In the same settings, M-FOS leads to socially optimal outcomes under meta-self-play. Finally, we show that M-FOS can be scaled to high-dimensional settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/ZA4MNEVF/Lu et al. - 2022 - Model-Free Opponent Shaping.pdf;/Users/nobr/Zotero/storage/ZVIYHJGA/2205.html}
}

@misc{lu2024,
  title = {{{JaxLife}}: {{An Open-Ended Agentic Simulator}}},
  shorttitle = {{{JaxLife}}},
  author = {Lu, Chris and Beukman, Michael and Matthews, Michael and Foerster, Jakob},
  year = {2024},
  month = sep,
  number = {arXiv:2409.00853},
  eprint = {2409.00853},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.00853},
  urldate = {2025-01-09},
  abstract = {Human intelligence emerged through the process of natural selection and evolution on Earth. We investigate what it would take to re-create this process in silico. While past work has often focused on low-level processes (such as simulating physics or chemistry), we instead take a more targeted approach, aiming to evolve agents that can accumulate open-ended culture and technologies across generations. Towards this, we present JaxLife: an artificial life simulator in which embodied agents, parameterized by deep neural networks, must learn to survive in an expressive world containing programmable systems. First, we describe the environment and show that it can facilitate meaningful Turing-complete computation. We then analyze the evolved emergent agents' behavior, such as rudimentary communication protocols, agriculture, and tool use. Finally, we investigate how complexity scales with the amount of compute used. We believe JaxLife takes a step towards studying evolved behavior in more open-ended simulations. Our code is available at https://github.com/luchris429/JaxLife},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/DD5ZS32J/Lu et al. - 2024 - JaxLife An Open-Ended Agentic Simulator.pdf}
}

@article{lucas2020,
  title = {Recently Increased Prevalence of the Human Median Artery of the Forearm: {{A}} Microevolutionary Change},
  shorttitle = {Recently Increased Prevalence of the Human Median Artery of the Forearm},
  author = {Lucas, Teghan and Kumaratilake, Jaliya and Henneberg, Maciej},
  year = {2020},
  journal = {Journal of Anatomy},
  volume = {237},
  number = {4},
  pages = {623--631},
  issn = {1469-7580},
  doi = {10.1111/joa.13224},
  urldate = {2024-05-28},
  abstract = {The median artery has been considered as an embryonic structure, which normally regresses around the 8th week of gestation. However, various prevalences have been reported in adults since the 18th century. Furthermore, in a study by Henneberg and George (1995; Am J Phys Anthropol 96, 329--334), has suggested that increasing prevalence of the median artery during the 20th century was a `possible secular trend'. The present study, conducted nearly a quarter of a century later, is a continuation of that study. A total of 26 median arteries were found in 78 upper limbs obtained from Australians aged 51 to 101 years, who died in the period 2015--2016, a prevalence rate of 33.3\%. Analysis of the literature showed that the presence of the median artery has been significantly increasing (p = .001) over time, from approximately 10\% in people born in the mid-1880s to approximately 30\% by the end of the 20th century. The significance of the prevalence increased to a p value {$<$}.0001, when the results of the present study and other studies conducted by our research team were combined. After removal of the studies that were possibly biased, because of their specific focus on the evolutionary aspects of the median artery, the significance remained at p = .018. The present study provides an example of microevolutionary changes in the internal anatomy of the human body. Second-order polynomial regression of the median artery's prevalence on dates of birth shows that it is now present in 35\% of people and predicts that people born 80 years from now will all carry a median artery if the trend continues. When the median artery prevalence reaches 50\% or more, it should not be considered as a variant, but as a `normal' human structure.},
  langid = {english},
  keywords = {anatomical variation,median artery,microevolution,secular trend},
  file = {/Users/nobr/Zotero/storage/Y9NDKXCX/Lucas et al. - 2020 - Recently increased prevalence of the human median artery of the forearm A microevolutionary change.pdf}
}

@article{lucena2019,
  title = {Wind Energy in {{Brazil}}: An Overview and Perspectives under the Triple Bottom Line},
  shorttitle = {Wind Energy in {{Brazil}}},
  author = {Lucena, Juliana De Almeida Yanaguizawa and Lucena, Klayton {\^A}ngelo Azevedo},
  year = {2019},
  month = may,
  journal = {Clean Energy},
  volume = {3},
  number = {2},
  pages = {69--84},
  issn = {2515-4230, 2515-396X},
  doi = {10.1093/ce/zkz001},
  urldate = {2023-07-13},
  abstract = {In recent years, Brazilian wind energy has been growing on a fast trajectory, with the prospect of being the second main source of energy in the year 2019 after hydroelectricity. It should be reaching an installed capacity of more than 20 GW by 2022. In this context, this work presents the evolution and the current scenario of wind energy in Brazil, bringing a discussion on the issues surrounding wind energy and the environmental, social and economic pillars of the sustainability. This study highlights the Northeast region because it has more than 85\% of the wind capacity in the country and, at the same time, it contains the cities with the lowest human development indexes. Rotor-blade manufacturing methods and composite materials recycling after the turbine lifetime are considered. Construction, operation and maintenance aspects of wind turbines are also discussed. Wind-energy concepts are described from a bibliographic survey and previous experiences of the authors.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/C9VFEMER/Lucena and Lucena - 2019 - Wind energy in Brazil an overview and perspective.pdf}
}

@article{luo2023,
  title = {A Framework to Assess Multi-Hazard Physical Climate Risk for Power Generation Projects from Publicly-Accessible Sources},
  author = {Luo, Tianyi and Cheng, Yan and Falzon, James and K{\"o}lbel, Julian and Zhou, Lihuan and Wu, Yili and Habchi, Amir},
  year = {2023},
  month = apr,
  journal = {Communications Earth \& Environment},
  volume = {4},
  number = {1},
  pages = {1--13},
  publisher = {Nature Publishing Group},
  issn = {2662-4435},
  doi = {10.1038/s43247-023-00782-w},
  urldate = {2024-03-12},
  abstract = {Demand for information about physical climate risk is growing, particularly for the power generation sector, given its size and pronounced exposure to climate hazards. However, quantifying physical climate risks for a large number of assets remains challenging. Here we introduce a scalable and transparent methodology that enables multi-hazard physical climate risk assessments for any thermal or hydro power generation project. The methodology relies on basic power plant type and geolocation data inputs, publicly-available climate datasets, and hazard- and technology-specific vulnerability factors, to translate hazard severity into generation losses. We apply the methodology to the European Bank for Reconstruction and Development's early 2021 thermal and hydro power generation portfolios of 80 assets. We show that under the Representative Concentration Pathway 4.5 scenario, those 80 power plants could experience a 4.0-10.9 TWh loss in annual generation (or 1.87-5.07\% of total annual maximum generation) by 2030 compared to its baseline losses of 0.70--0.87 TWh (or 0.33--0.41\%). One of the largest drivers of the increased risk is rising water temperatures, which is currently overlooked by mainstream climate risk disclosure guidelines.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Climate-change impacts,Natural hazards,Projection and prediction},
  file = {/Users/nobr/Zotero/storage/E5UDAKSK/Luo et al. - 2023 - A framework to assess multi-hazard physical climate risk for power generation projects from publicly.pdf}
}

@misc{luo2024,
  title = {Addition Is {{All You Need}} for {{Energy-efficient Language Models}}},
  author = {Luo, Hongyin and Sun, Wei},
  year = {2024},
  month = oct,
  number = {arXiv:2410.00907},
  eprint = {2410.00907},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-17},
  abstract = {Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication (L-Mul) algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95\% energy cost by elementwise floating point tensor multiplications and 80\% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8 e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8 e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8 e4m3 as accumulation precision in both fine-tuning and inference.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/NAF7G67C/2410.00907v2.pdf}
}

@book{lupi2016,
  title = {Dear Data},
  author = {Lupi, Giorgia and Posavec, Stefanie and Popova, Maria},
  year = {2016},
  publisher = {Princeton Architectural Press},
  address = {New York},
  isbn = {978-1-61689-532-7},
  langid = {english},
  lccn = {QA76.9.I52 L87 2016b},
  keywords = {Communication,Communication of technical information,DESIGN,General,Graphic Arts,Graphic methods,Illustrated works,Information visualization,Life,Mail art,Miscellanea,Pictorial works,Postcards,Trivia and miscellanea,Visual communication},
  annotation = {OCLC: ocn958301068}
}

@book{lutkepohl2007,
  title = {New Introduction to Multiple Time Series Analysis: With ... 36 Tables},
  shorttitle = {New Introduction to Multiple Time Series Analysis},
  author = {L{\"u}tkepohl, Helmut},
  year = {2007},
  edition = {1. ed., corr. 2. print},
  publisher = {Springer},
  address = {Berlin Heidelberg},
  isbn = {978-3-540-40172-8},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IN2UVFXZ/Lütkepohl - 2007 - New introduction to multiple time series analysis with ... 36 tables.pdf}
}

@article{lutz2019,
  title = {The {{European}} Digital Single Market Strategy: {{Local}} Indicators of Spatial Association 2011--2016},
  shorttitle = {The {{European}} Digital Single Market Strategy},
  author = {Lutz, Sebastian Uljas},
  year = {2019},
  month = jun,
  journal = {Telecommunications Policy},
  volume = {43},
  number = {5},
  pages = {393--410},
  issn = {03085961},
  doi = {10.1016/j.telpol.2018.10.003},
  urldate = {2023-07-04},
  abstract = {This paper describes the space-time dynamics of the European digital divide. The frequencies of internet and E-commerce use are considered, along with a broad range of indicators associated with the European digital single market strategy. This paper aims to investigate the spatial structure underlying these aggregate outcome indicators. An exploratory spatial data analysis is conducted in a sample of 209 regions using Eurostat data. Strong evidence for both global and local spatial autocorrelation is found for the years 2011--2016. Consistently, a North-South polarization scheme is identified with little statistical significance in the centre of Europe. This contrasts with the high income cluster found in central Europe, while low values for digital indices and income are co-located more consistently in the South, but also in the North-East. Highlights from the specific results include: areas surrounding London as a dynamic high value cluster in E-commerce, Italy to conduct few cross-border purchases, France as a consistent adopter of E-governance, and broadband rates in general to closely reflect online activities.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/9FPWA2GD/Lutz - 2019 - The European digital single market strategy Local.pdf}
}

@misc{lykov2023,
  title = {{{LLM-BRAIn}}: {{AI-driven Fast Generation}} of {{Robot Behaviour Tree}} Based on {{Large Language Model}}},
  shorttitle = {{{LLM-BRAIn}}},
  author = {Lykov, Artem and Tsetserukou, Dzmitry},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {This paper presents a novel approach in autonomous robot control, named LLM-BRAIn, that makes possible robot behavior generation, based on operator's commands. LLM-BRAIn is a transformer-based Large Language Model (LLM) fine-tuned from Stanford Alpaca 7B model to generate robot behavior tree (BT) from the text description. We train the LLM-BRAIn on 8,5k instruction-following demonstrations, generated in the style of self-instruct using text-davinchi-003. The developed model accurately builds complex robot behavior while remaining small enough to be run on the robot's onboard microcomputer. The model gives structural and logical correct BTs and can successfully manage instructions that were not presented in training set. The experiment did not reveal any significant subjective differences between BTs generated by LLM-BRAIn and those created by humans (on average, participants were able to correctly distinguish between LLM-BRAIn generated BTs and human-created BTs in only 4.53 out of 10 cases, indicating that their performance was close to random chance). The proposed approach potentially can be applied to mobile robotics, drone operation, robot manipulator systems and Industry 4.0.},
  howpublished = {https://arxiv.org/abs/2305.19352v1},
  langid = {english},
  keywords = {/unread},
  file = {/Users/nobr/Zotero/storage/AZC32X28/Lykov and Tsetserukou - 2023 - LLM-BRAIn AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model.pdf}
}

@misc{lykovLLMMARSLargeLanguage2023,
  title = {{{LLM-MARS}}: {{Large Language Model}} for {{Behavior Tree Generation}} and {{NLP-enhanced Dialogue}} in {{Multi-Agent Robot Systems}}},
  shorttitle = {{{LLM-MARS}}},
  author = {Lykov, Artem and Dronova, Maria and Naglov, Nikolay and Litvinov, Mikhail and Satsevich, Sergei and Bazhenov, Artem and Berman, Vladimir and Shcherbak, Aleksei and Tsetserukou, Dzmitry},
  year = {2023},
  month = dec,
  number = {arXiv:2312.09348},
  eprint = {2312.09348},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09348},
  urldate = {2024-02-08},
  abstract = {This paper introduces LLM-MARS, first technology that utilizes a Large Language Model based Artificial Intelligence for Multi-Agent Robot Systems. LLM-MARS enables dynamic dialogues between humans and robots, allowing the latter to generate behavior based on operator commands and provide informative answers to questions about their actions. LLM-MARS is built on a transformer-based Large Language Model, fine-tuned from the Falcon 7B model. We employ a multimodal approach using LoRa adapters for different tasks. The first LoRa adapter was developed by fine-tuning the base model on examples of Behavior Trees and their corresponding commands. The second LoRa adapter was developed by fine-tuning on question-answering examples. Practical trials on a multi-agent system of two robots within the Eurobot 2023 game rules demonstrate promising results. The robots achieve an average task execution accuracy of 79.28\% in compound commands. With commands containing up to two tasks accuracy exceeded 90\%. Evaluation confirms the system's answers on operators questions exhibit high accuracy, relevance, and informativeness. LLM-MARS and similar multi-agent robotic systems hold significant potential to revolutionize logistics, enabling autonomous exploration missions and advancing Industry 5.0.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/XNT566AV/Lykov et al. - 2023 - LLM-MARS Large Language Model for Behavior Tree Generation and NLP-enhanced Dialogue in Multi-Agent.pdf;/Users/nobr/Zotero/storage/LRHXMZVA/2312.html}
}

@article{lymburn2021,
  title = {Reservoir Computing with Swarms},
  author = {Lymburn, Thomas and Algar, Shannon D. and Small, Michael and J{\"u}ngling, Thomas},
  year = {2021},
  month = mar,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {3},
  pages = {033121},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0039745},
  urldate = {2023-11-26},
  abstract = {We study swarms as dynamical systems for reservoir computing (RC). By example of a modified Reynolds boids model, the specific symmetries and dynamical properties of a swarm are explored with respect to a nonlinear time-series prediction task. Specifically, we seek to extract meaningful information about a predator-like driving signal from the swarm's response to that signal. We find that the na{\"i}ve implementation of a swarm for computation is very inefficient, as permutation symmetry of the individual agents reduces the computational capacity. To circumvent this, we distinguish between the computational substrate of the swarm and a separate observation layer, in which the swarm's response is measured for use in the task. We demonstrate the implementation of a radial basis-localized observation layer for this task. The behavior of the swarm is characterized by order parameters and measures of consistency and related to the performance of the swarm as a reservoir. The relationship between RC performance and swarm behavior demonstrates that optimal computational properties are obtained near a phase transition regime.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/N7S9NFCK/Lymburn et al. - 2021 - Reservoir computing with swarms.pdf}
}

@misc{madaan2022,
  title = {Language {{Models}} of {{Code}} Are {{Few-Shot Commonsense Learners}}},
  author = {Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming and Neubig, Graham},
  year = {2022},
  month = dec,
  number = {arXiv:2210.07128},
  eprint = {2210.07128},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-16},
  abstract = {We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches ``serialize'' the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the fewshot setting. Our code and data are available at https://github.com/madaan/ CoCoGen .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/ZWZQ22NH/Madaan et al. - 2022 - Language Models of Code are Few-Shot Commonsense L.pdf}
}

@misc{maEurekaHumanLevelReward2023,
  title = {Eureka: {{Human-Level Reward Design}} via {{Coding Large Language Models}}},
  shorttitle = {Eureka},
  author = {Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  year = {2023},
  month = oct,
  number = {arXiv:2310.12931},
  eprint = {2310.12931},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.12931},
  urldate = {2024-01-07},
  abstract = {Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83\% of the tasks, leading to an average normalized improvement of 52\%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/SPJIHVT3/Ma et al. - 2023 - Eureka Human-Level Reward Design via Coding Large Language Models.pdf;/Users/nobr/Zotero/storage/FMPIMJYL/2310.html}
}

@article{majidinezhad2022,
  title = {Sites Exploring Prioritisation of Offshore Wind Energy Potential and Mapping for Wind Farms Installation: {{Iranian}} Islands Case Studies},
  shorttitle = {Sites Exploring Prioritisation of Offshore Wind Energy Potential and Mapping for Wind Farms Installation},
  author = {Majidi Nezhad, Meysam and Neshat, Mehdi and Piras, Giuseppe and Astiaso Garcia, Davide},
  year = {2022},
  month = oct,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {168},
  pages = {112791},
  issn = {13640321},
  doi = {10.1016/j.rser.2022.112791},
  urldate = {2023-07-13},
  abstract = {Offshore Wind Energy (OWE) can be considered the Renewable Energy Sources (RESs) with a higher potential of newly installed power in marine areas more the following decades. As a primary phase of the Offshore Wind Farms (OWFs) development, focusing on the long-term Offshore Wind (OW) potential assessment and mapping is necessary to highlight the best areas for turbine generators installations. In this case, accurate assessment and mapping of long-term OWs can help pinpoint previously not considered marine areas. In this regard, the Iranian islands located in the Persian Gulf can be called one of these forgotten areas in dire need of energy supply due to their remoteness from the mainland. To these aims, the long-term Modern-Era Retrospective analysis for Research and Applications version 2 (MERRA-2) re-analysis dataset has been used to identify possible locations of the Offshore Wind Turbine Generators (OWTGs) installations. In particular, an OW classification based on the 40 years of monthly data and the last 10-years of hourly data highlighted the best areas for OWTGs potential installations in the 12 Iranian islands of the Persian Sea. The time-series method has been designed, tested, and developed to understand better and manage the OW potential and mapping of the Iranian islands decisionmaking process. Furthermore, the time-series method has been applied to the generated energy source based on the OW speed used in the Iranian islands. Finally, exploring results show Iranian islands, such as Kharg, Siri and Abu Musa islands, have attractive OWE potentials for OWTGs installations.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2W74XU6E/Majidi Nezhad et al. - 2022 - Sites exploring prioritisation of offshore wind en.pdf}
}

@misc{malagon2024,
  title = {Craftium: {{An Extensible Framework}} for {{Creating Reinforcement Learning Environments}}},
  shorttitle = {Craftium},
  author = {Malag{\'o}n, Mikel and Ceberio, Josu and Lozano, Jose A.},
  year = {2024},
  month = jul,
  number = {arXiv:2407.03969},
  eprint = {2407.03969},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.03969},
  urldate = {2024-12-17},
  abstract = {Most Reinforcement Learning (RL) environments are created by adapting existing physics simulators or video games. However, they usually lack the flexibility required for analyzing specific characteristics of RL methods often relevant to research. This paper presents Craftium, a novel framework for exploring and creating rich 3D visual RL environments that builds upon the Minetest game engine and the popular Gymnasium API. Minetest is built to be extended and can be used to easily create voxel-based 3D environments (often similar to Minecraft), while Gymnasium offers a simple and common interface for RL research. Craftium provides a platform that allows practitioners to create fully customized environments to suit their specific research requirements, ranging from simple visual tasks to infinite and procedurally generated worlds. We also provide five ready-to-use environments for benchmarking and as examples of how to develop new ones. The code and documentation are available at https://github.com/mikelma/craftium/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/2WL46CPN/Malagón et al. - 2024 - Craftium An Extensible Framework for Creating Reinforcement Learning Environments.pdf}
}

@book{malle2005,
  title = {Other Minds: How Humans Bridge the Divide between Self and Others},
  shorttitle = {Other Minds},
  editor = {Malle, Bertram F. and Hodges, Sara D.},
  year = {2005},
  publisher = {Guilford Press},
  address = {New York},
  isbn = {978-1-59385-187-3},
  langid = {english},
  lccn = {BF323.S63 O89 2005},
  keywords = {Other minds (Theory of knowledge),Social perception},
  file = {/Users/nobr/Zotero/storage/4ZYEX3Q2/Malle and Hodges - 2005 - Other minds how humans bridge the divide between self and others.pdf}
}

@article{manganaro2023,
  title = {Consensus Clustering Methodology to Improve Molecular Stratification of Non-Small Cell Lung Cancer},
  author = {Manganaro, L. and Bianco, S. and Bironzo, P. and Cipollini, F. and Colombi, D. and Cor{\`a}, D. and Corti, G. and Doronzo, G. and Errico, L. and Falco, P. and Gandolfi, L. and Guerrera, F. and Monica, V. and Novello, S. and Papotti, M. and Parab, S. and Pittaro, A. and Primo, L. and Righi, L. and Sabbatini, G. and Sandri, A. and Vattakunnel, S. and Bussolino, F. and Scagliotti, G.V.},
  year = {2023},
  month = may,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {7759},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-33954-x},
  urldate = {2023-05-16},
  abstract = {Abstract             Recent advances in machine learning research, combined with the reduced sequencing costs enabled by modern next-generation sequencing, paved the way to the implementation of precision medicine through routine multi-omics molecular profiling of tumours. Thus, there is an emerging need of reliable models exploiting such data to retrieve clinically useful information. Here, we introduce an original consensus clustering approach, overcoming the intrinsic instability of common clustering methods based on molecular data. This approach is applied to the case of non-small cell lung cancer (NSCLC), integrating data of an ongoing clinical study (PROMOLE) with those made available by The Cancer Genome Atlas, to define a molecular-based stratification of the patients beyond, but still preserving, histological subtyping. The resulting subgroups are biologically characterized by well-defined mutational and gene-expression profiles and are significantly related to disease-free survival (DFS). Interestingly, it was observed that (1) cluster B, characterized by a short DFS, is enriched in KEAP1 and SKP2 mutations, that makes it an ideal candidate for further studies with inhibitors, and (2) over- and under-representation of inflammation and immune systems pathways in squamous-cell carcinomas subgroups could be potentially exploited to stratify patients treated with immunotherapy.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RA2CXXDV/Manganaro et al. - 2023 - Consensus clustering methodology to improve molecu.pdf}
}

@article{manzano2024,
  title = {Thermodynamics of {{Computations}} with {{Absolute Irreversibility}}, {{Unidirectional Transitions}}, and {{Stochastic Computation Times}}},
  author = {Manzano, Gonzalo and Karde{\c s}, G{\"u}lce and Rold{\'a}n, {\'E}dgar and Wolpert, David H.},
  year = {2024},
  month = may,
  journal = {Physical Review X},
  volume = {14},
  number = {2},
  pages = {021026},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.14.021026},
  urldate = {2024-05-25},
  abstract = {Developing a thermodynamic theory of computation is a challenging task at the interface of nonequilibrium thermodynamics and computer science. In particular, this task requires dealing with difficulties such as stochastic halting times, unidirectional (possibly deterministic) transitions, and restricted initial conditions, features common in real-world computers. Here, we present a framework which tackles all such difficulties by extending the martingale theory of nonequilibrium thermodynamics to generic nonstationary Markovian processes, including those with broken detailed balance and/or absolute irreversibility. We derive several universal fluctuation relations and second-law-like inequalities that provide both lower and upper bounds on the intrinsic dissipation (mismatch cost) associated with any periodic process---in particular, the periodic processes underlying all current digital computation. Crucially, these bounds apply even if the process has stochastic stopping times, as it does in many computational machines. We illustrate our results with exhaustive numerical simulations of deterministic finite automata processing bit strings, one of the fundamental models of computation from theoretical computer science. We also provide universal equalities and inequalities for the acceptance probability of words of a given length by a deterministic finite automaton in terms of thermodynamic quantities, and outline connections between computer science and stochastic resetting. Our results, while motivated from the computational context, are applicable far more broadly.                                                                               Published by the American Physical Society                   2024},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/S7D4ZEM2/Manzano et al. - 2024 - Thermodynamics of Computations with Absolute Irreversibility, Unidirectional Transitions, and Stocha.pdf}
}

@article{marino2022,
  title = {Evolving Interpretable Strategies for Zero-Sum Games},
  author = {Mari{\~n}o, Julian R.H. and Toledo, Claudio F.M.},
  year = {2022},
  month = jun,
  journal = {Applied Soft Computing},
  volume = {122},
  pages = {108860},
  issn = {15684946},
  doi = {10.1016/j.asoc.2022.108860},
  urldate = {2023-11-12},
  abstract = {The present paper introduces Gesy, a genetic programming approach to script synthesis for zero-sum games. We will explore the sum-zero game context in Real-Time Strategy (RTS) games, where players must look for strategies (planning of actions) to maximize their gains or minimize their losses. The goal is to solve the script synthesis problem, which demands the synthesis of a computer program from a space of programs defined by a Domain-Specific Language (DSL). The synthesized program must encode a practical strategy for zero-sum games. Empirical results validate Gesy using the {\textmu}RTS platform, an academic test bed game that presents the main features found in RTS commercial games. The results show that our method provides interpretable strategies that are competitive with state-ofthe-art search-based approaches in terms of play strength. Moreover, once synthesized, scripts require only a tiny fraction of the time needed by search-based methods to decide on the agent's next action. {\copyright} 2022 Elsevier B.V. All rights reserved.},
  langid = {english}
}

@article{markovic2020,
  title = {Quantum Neuromorphic Computing},
  author = {Markovi{\'c}, Danijela and Grollier, Julie},
  year = {2020},
  month = oct,
  journal = {Applied Physics Letters},
  volume = {117},
  number = {15},
  eprint = {2006.15111},
  primaryclass = {quant-ph},
  pages = {150501},
  issn = {0003-6951, 1077-3118},
  doi = {10.1063/5.0020014},
  urldate = {2025-08-02},
  abstract = {Quantum neuromorphic computing physically implements neural networks in brain-inspired quantum hardware to speed up their computation. In this perspective article, we show that this emerging paradigm could make the best use of the existing and near future intermediate size quantum computers. Some approaches are based on parametrized quantum circuits, and use neural network-inspired algorithms to train them. Other approaches, closer to classical neuromorphic computing, take advantage of the physical properties of quantum oscillator assemblies to mimic neurons and compute. We discuss the different implementations of quantum neuromorphic networks with digital and analog circuits, highlight their respective advantages, and review exciting recent experimental results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Quantum Physics},
  file = {/Users/nobr/Zotero/storage/Z8BHC5NI/Marković and Grollier - 2020 - Quantum neuromorphic computing.pdf}
}

@book{marr2010,
  title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  shorttitle = {Vision},
  author = {Marr, David},
  year = {2010},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-51462-0},
  langid = {english},
  lccn = {QP475 .M27 2010},
  keywords = {Data processing,Human information processing,Mathematical models,Vision},
  annotation = {OCLC: ocn472791457},
  file = {/Users/nobr/Zotero/storage/Z9L3Q9D5/Marr - 2010 - Vision a computational investigation into the human representation and processing of visual informa.pdf}
}

@misc{marsden2014,
  title = {Category {{Theory Using String Diagrams}}},
  author = {Marsden, Daniel},
  year = {2014},
  month = nov,
  number = {arXiv:1401.7220},
  eprint = {1401.7220},
  primaryclass = {cs, math},
  doi = {10.48550/arXiv.1401.7220},
  urldate = {2024-06-29},
  abstract = {In work of Fokkinga and Meertens a calculational approach to category theory is developed. The scheme has many merits, but sacrifices useful type information in the move to an equational style of reasoning. By contrast, traditional proofs by diagram pasting retain the vital type information, but poorly express the reasoning and development of categorical proofs. In order to combine the strengths of these two perspectives, we propose the use of string diagrams, common folklore in the category theory community, allowing us to retain the type information whilst pursuing a calculational form of proof. These graphical representations provide a topological perspective on categorical proofs, and silently handle functoriality and naturality conditions that require awkward bookkeeping in more traditional notation. Our approach is to proceed primarily by example, systematically applying graphical techniques to many aspects of category theory. We develop string diagrammatic formulations of many common notions, including adjunctions, monads, Kan extensions, limits and colimits. We describe representable functors graphically, and exploit these as a uniform source of graphical calculation rules for many category theoretic concepts. These graphical tools are then used to explicitly prove many standard results in our proposed diagrammatic style.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science,Mathematics - Category Theory},
  file = {/Users/nobr/Zotero/storage/DJTNGAZ2/Marsden - 2014 - Category Theory Using String Diagrams.pdf}
}

@misc{martensOptimizingNeuralNetworks2020,
  title = {Optimizing {{Neural Networks}} with {{Kronecker-factored Approximate Curvature}}},
  author = {Martens, James and Grosse, Roger},
  year = {2020},
  month = jun,
  number = {arXiv:1503.05671},
  eprint = {1503.05671},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.05671},
  urldate = {2024-05-25},
  abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/WW95BNKW/Martens and Grosse - 2020 - Optimizing Neural Networks with Kronecker-factored Approximate Curvature.pdf;/Users/nobr/Zotero/storage/ZQDD6QWK/1503.html}
}

@inproceedings{masek2019,
  title = {Evolving Behaviour Trees for Automated Discovery of Novel Combat Strategy in Real-Time Strategy Wargames},
  booktitle = {El {{Sawah}}, {{S}}. (Ed.) {{MODSIM2019}}, 23rd {{International Congress}} on {{Modelling}} and {{Simulation}}.},
  author = {et al. Masek, Martin},
  year = {2019},
  month = dec,
  publisher = {{Modelling and Simulation Society of Australia and New Zealand}},
  doi = {10.36334/modsim.2019.B4.masek},
  urldate = {2023-11-12},
  abstract = {A wargame is defined as: ``A simulation, by whatever means, of a military operation involving two or more opposing forces using rules, data, and procedures designed to depict an actual or assumed real life situation'' (Gortney 2016, p503). Wargaming is used for several purposes, such as teaching strategic planning, practising the tasks associated with war, and analytic purposes (Burns et al., 2015). While wargaming is predominantly a human centric analytical activity, it is an area where artificial intelligence (AI) may play a useful role because of a computer's ability to play through a wider range of possible strategies. However, creating simulated AI participants for wargame scenarios is challenging because of the complexity and uncertainty in the environments in which they exist. The recent rise of automated behaviour discovery for agents in a variety of games traditionally dominated by humans offers a possible solution. Commercial real-time strategy (RTS) games provide an abstract simulation of a world where players aim to dominate and defeat other players by acquiring, using and managing resources, often including a mix of military, political, scientific and economic factors. As there is considerable overlap between the objectives found in RTS games to those that exist in military style wargames, RTS games are ideal platforms to conduct research and development in support of our AI-enabled wargaming research objectives.},
  isbn = {978-0-9758400-9-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HF52Y5TK/Masek - 2019 - Evolving behaviour trees for automated discovery of novel combat strategy in real-time strategy warg.pdf}
}

@article{masek2023,
  title = {Discovering Optimal Strategy in Tactical Combat Scenarios through the Evolution of Behaviour Trees},
  author = {Masek, Martin and Lam, Chiou Peng and Kelly, Luke and Wong, Martin},
  year = {2023},
  month = jan,
  journal = {Annals of Operations Research},
  volume = {320},
  number = {2},
  pages = {901--936},
  issn = {0254-5330, 1572-9338},
  doi = {10.1007/s10479-021-04225-7},
  urldate = {2023-11-12},
  abstract = {In this paper we address the problem of automatically discovering optimal tactics in a combat scenario in which two opposing sides control a number of fighting units. Our approach is based on the evolution of behaviour trees, combined with simulation-based evaluation of solutions to drive the evolution. Our behaviour trees use a small set of possible actions that can be assigned to a combat unit, along with standard behaviour tree constructs and a novel approach for selecting which action from the tree is performed. A set of test scenarios was designed for which an optimal strategy is known from the literature. These scenarios were used to explore and evaluate our approach. The results indicate that it is possible, from the small set of possible unit actions, for a complex strategy to emerge through evolution. Combat units with different capabilities were observed exhibiting coordinated team work and exploiting aspects of the environment.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/A45T8VNV/Masek et al. - 2023 - Discovering optimal strategy in tactical combat scenarios through the evolution of behaviour trees.pdf}
}

@inproceedings{masek2023a,
  title = {Interactive {{Evolutionary Computation}} for {{Strategy Discovery}} in {{Multi-Phase Operations}}},
  booktitle = {Proceedings of the {{Companion Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Masek, Martin and Luke, Kelly and Snell, Jacob and Wheat, Daniel and Lam, Chiou Peng},
  year = {2023},
  month = jul,
  pages = {743--746},
  publisher = {ACM},
  address = {Lisbon Portugal},
  doi = {10.1145/3583133.3590644},
  urldate = {2023-11-12},
  abstract = {Complex adversarial operations typically involve the allocation of finite resources to meet a set of objectives over a number of phases. This poses a challenge for AI-based strategy discovery. A strategy for one phase cannot be developed in isolation as the resources available in any one phase are dependent on the outcome of previous phases. Our proposed solution is to combine an evolutionary algorithm search with human-guided evaluation. The approach uses simulation-based fitness evaluation, where a human operator can view the fittest solution after every set number of generations. The operator can `lock in' strategies for particular phases, and `suggest' alternative strategies to guide further evolution. Key to our approach is a representation encoding that allows relative proportions of resources to be represented where actual levels may not be known a priori. We evaluate our solution on a three-phase scenario of a real-time strategy game and compare the effectiveness of strategies that were purely human-devised, purely evolved, and those resulting from the human-evolution collaboration. The collaborative approach shows promising results in being able to find an optimum solution earlier.},
  isbn = {979-8-4007-0120-7},
  langid = {english}
}

@article{mashayekhi2023,
  title = {A Reconfigurable Graphene Patch Antenna Inverse Design at Terahertz Frequencies},
  author = {Mashayekhi, Mohammad and Kabiri, Pooria and Nooramin, Amir Saman and Soleimani, Mohammad},
  year = {2023},
  month = may,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {8369},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-35036-4},
  urldate = {2023-06-19},
  abstract = {Abstract                            This article investigates the inverse design of a reconfigurable multi-band patch antenna based on graphene for terahertz applications to operate frequency range (2--5THz). In the first step, this article evaluates the dependence of the antenna radiation characteristics on its geometric parameters and the graphene properties. The simulation results show that it is possible to achieve up to 8.8 dB gain, 13 frequency bands, and 360                                                   \$\${\textasciicircum}{\textbackslash}circ\$\$                                                                                      {$\circ$}                                                                                       beam steering. Then and due to the complexity of the design of graphene antenna, a deep neural network (DNN) is used to predict the antenna parameters by given inputs like desired realized gain, main lobe direction, half power beam width, and return loss in each resonance frequency. The trained DNN model predicts almost with 93\% accuracy and 3\% mean square error in the shortest time. Then, this network was used to design five-band and three-band antennas, and it has been shown that the desired antenna parameters are achieved with negligible errors. Therefore, the proposed antenna finds many potential applications in the THz frequency band.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HEXZNXY8/Mashayekhi et al. - 2023 - A reconfigurable graphene patch antenna inverse de.pdf}
}

@misc{mastermanLandscapeEmergingAI2024,
  title = {The {{Landscape}} of {{Emerging AI Agent Architectures}} for {{Reasoning}}, {{Planning}}, and {{Tool Calling}}: {{A Survey}}},
  shorttitle = {The {{Landscape}} of {{Emerging AI Agent Architectures}} for {{Reasoning}}, {{Planning}}, and {{Tool Calling}}},
  author = {Masterman, Tula and Besen, Sandi and Sawtell, Mason and Chao, Alex},
  year = {2024},
  month = apr,
  number = {arXiv:2404.11584},
  eprint = {2404.11584},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.11584},
  urldate = {2024-04-22},
  abstract = {This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/DM7A4DGM/Masterman et al. - 2024 - The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling A Survey.pdf;/Users/nobr/Zotero/storage/WYUSFN2G/2404.html}
}

@article{mathewson2018,
  title = {Improbotics: {{Exploring}} the {{Imitation Game}} Using {{Machine Intelligence}} in {{Improvised Theatre}}},
  author = {Mathewson, Kory W. and Mirowski, Piotr},
  year = {2018},
  month = sep,
  journal = {Proceedings of the 14th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2018},
  eprint = {1809.01807},
  pages = {59--66},
  publisher = {AAAI press},
  issn = {2326-909X},
  doi = {10.1609/aiide.v14i1.13030},
  urldate = {2023-03-22},
  abstract = {Theatrical improvisation (impro or improv) is a demanding form of live, collaborative performance. Improv is a humorous and playful artform built on an open-ended narrative structure which simultaneously celebrates effort and failure. It is thus an ideal test bed for the development and deployment of interactive artificial intelligence (AI)-based conversational agents, or artificial improvisors. This case study introduces an improv show experiment featuring human actors and artificial improvisors. We have previously developed a deep-learning-based artificial improvisor, trained on movie subtitles, that can generate plausible, context-based, lines of dialogue suitable for theatre (Mathewson and Mirowski 2017). In this work, we have employed it to control what a subset of human actors say during an improv performance. We also give human-generated lines to a different subset of performers. All lines are provided to actors with headphones and all performers are wearing headphones. This paper describes a Turing test, or imitation game, taking place in a theatre, with both the audience members and the performers left to guess who is a human and who is a machine. In order to test scientific hypotheses about the perception of humans versus machines we collect anonymous feedback from volunteer performers and audience members. Our results suggest that rehearsal increases proficiency and possibility to control events in the performance. That said, consistency with real world experience is limited by the interface and the mechanisms used to perform the show. We also show that human-generated lines are shorter, more positive, and have less difficult words with more grammar and spelling mistakes than the artificial improvisor generated lines.},
  archiveprefix = {arXiv},
  isbn = {9781577358046},
  file = {/Users/nobr/Zotero/storage/V25IMU7Q/Mathewson and Mirowski - 2018 - Improbotics Exploring the Imitation Game using Ma.pdf}
}

@misc{mauna_loa,
  title = {Mauna Loa {{CO2}} Measurements}
}

@article{maurer,
  title = {Bounds for {{Linear Multi-Task Learning}}},
  author = {Maurer, Andreas},
  abstract = {We give dimension-free and data-dependent bounds for linear multi-task learning where a common linear operator is chosen to preprocess data for a vector of task specific linear-thresholding classifiers. The complexity penalty of multi-task learning is bounded by a simple expression involving the margins of the task-specific classifiers, the Hilbert-Schmidt norm of the selected preprocessor and the Hilbert-Schmidt norm of the covariance operator for the total mixture of all task distributions, or, alternatively, the Frobenius norm of the total Gramian matrix for the data-dependent version. The results can be compared to state-of-the-art results on linear single-task learning.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JSCIXGEN/Maurer - Bounds for Linear Multi-Task Learning.pdf}
}

@book{mcafee2019,
  title = {More from {{Less}}: {{The Surprising Story}} of {{How We Learned}} to {{Prosper Using Fewer Resources--And What Happens Next}}},
  shorttitle = {More from {{Less}}},
  author = {McAfee, Andrew},
  year = {2019},
  publisher = {Scribner},
  address = {New York},
  abstract = {Intro -- Title Page -- Dedication -- Epigraph -- Introduction: Readme -- Chapter 1: All the Malthusian Millennia -- Chapter 2: Power over the Earth: The Industrial Era -- Chapter 3: Industrial Errors -- Chapter 4: Earth Day and Its Debates -- Chapter 5: The Dematerialization Surprise -- Chapter 6: CRIB Notes -- Chapter 7: What Causes Dematerialization? Markets and Marvels -- Chapter 8: Adam Smith Said That: A Few Words about Capitalism -- Chapter 9: What Else Is Needed? People and Policies -- Chapter 10: The Global Gallop of the Four Horsemen -- Chapter 11: Getting So Much Better -- Chapter 12: Powers of Concentration -- Chapter 13 Stressed Be the Tie That Binds: Disconnection -- Chapter 14: Looking Ahead: The World Cleanses Itself This Way -- Chapter 15: Interventions: How to Be Good -- Conclusion: Our Next Planet -- Acknowledgments -- About the Author -- Notes -- Index -- Copyright},
  isbn = {978-1-9821-0357-6 978-1-9821-0359-0},
  langid = {english}
}

@misc{mccarthy2017,
  title = {The {{Kekul{\'e} Problem}}},
  author = {McCarthy, Cormac},
  year = {2017},
  month = apr,
  journal = {Nautilus},
  urldate = {2023-11-21},
  abstract = {Where did language come from?},
  howpublished = {https://nautil.us/the-kekul-problem-236574/},
  langid = {american},
  file = {/Users/nobr/Zotero/storage/E4QMUUGN/the-kekul-problem-236574.html}
}

@article{mccarthy2017a,
  title = {The {{Kekul{\'e} Problem}}: {{Where}} Did Language Come From?},
  shorttitle = {The {{Kekul{\'e} Problem}}},
  author = {McCarthy, Cormac},
  year = {2017},
  journal = {Nautilus},
  number = {47},
  pages = {22--31},
  urldate = {2024-01-24},
  file = {/Users/nobr/Zotero/storage/SS6WTAEM/Nautilus_Print_19.pdf}
}

@misc{mccarthy2017b,
  title = {Cormac {{McCarthy Returns}} to the {{Kekul{\'e} Problem}}},
  author = {McCarthy, Cormac},
  year = {2017},
  month = nov,
  journal = {Nautilus},
  urldate = {2024-01-24},
  abstract = {Answers to questions and questions that cannot be answered.},
  howpublished = {https://nautil.us/cormac-mccarthy-returns-to-the-kekul-problem-236896/},
  langid = {american},
  file = {/Users/nobr/Zotero/storage/KD6YYGE9/cormac-mccarthy-returns-to-the-kekul-problem-236896.html}
}

@misc{mccullough2021,
  title = {{{AFRL}}'s '{{Fight Tonight}}' to {{Prototype AI}} and {{Gaming Tech}} for {{Attack Planning}}},
  author = {McCullough, Amy},
  year = {2021},
  month = dec,
  journal = {Air \& Space Forces Magazine},
  urldate = {2024-09-09},
  abstract = {The Air Force is looking to combine AI and interactive gaming technology to shorten the process of planning complex air attack operations.},
  howpublished = {https://www.airandspaceforces.com/afrls-fight-tonight-to-prototype-ai-and-gaming-tech-for-attack-planning/},
  langid = {american},
  file = {/Users/nobr/Zotero/storage/FX8DCPTP/afrls-fight-tonight-to-prototype-ai-and-gaming-tech-for-attack-planning.html}
}

@misc{mcleishTransformersCanArithmetic2024,
  title = {Transformers {{Can Do Arithmetic}} with the {{Right Embeddings}}},
  author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
  year = {2024},
  month = may,
  number = {arXiv:2405.17399},
  eprint = {2405.17399},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.17399},
  urldate = {2024-05-28},
  abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99\% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/WFLXCZ54/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embeddings.pdf}
}

@article{mcmahan2016,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2016},
  month = feb,
  eprint = {1602.05629},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/X4NVQ7S3/McMahan et al. - 2016 - Communication-Efficient Learning of Deep Networks from Decentralized Data.pdf}
}

@article{mecattaf2024,
  title = {A Little Less Conversation, a Little More Action, Please: {{Investigating}} the Physical Common-Sense of {{LLMs}} in a {{3D}} Embodied Environment},
  shorttitle = {A Little Less Conversation, a Little More Action, Please},
  author = {Mecattaf, Matteo G. and Slater, Ben and Te{\v s}i{\'c}, Marko and Prunty, Jonathan and Voudouris, Konstantinos and Cheke, Lucy G.},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2410.23242},
  urldate = {2024-12-03},
  abstract = {As general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences},
  file = {/Users/nobr/Zotero/storage/H9BEZ553/Mecattaf et al. - 2024 - A little less conversation, a little more action, please Investigating the physical common-sense of.pdf}
}

@book{mehrtens1981,
  title = {Social {{History}} of {{Nineteenth Century Mathematics}}},
  editor = {Mehrtens, Herbert and Bos, Henk and Schneider, Ivo},
  year = {1981},
  publisher = {Birkh{\"a}user},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-9491-4},
  urldate = {2024-02-13},
  isbn = {978-0-8176-3033-1 978-1-4684-9491-4},
  langid = {english},
  keywords = {algebra,Cambridge University,history of mathematics,Mathematica,mathematics},
  file = {/Users/nobr/Zotero/storage/A74DWS2J/Mehrtens et al. - 1981 - Social History of Nineteenth Century Mathematics.pdf}
}

@misc{meila2023,
  title = {Manifold Learning: What, How, and Why},
  shorttitle = {Manifold Learning},
  author = {Meil{\u a}, Marina and Zhang, Hanyu},
  year = {2023},
  month = nov,
  number = {arXiv:2311.03757},
  eprint = {2311.03757},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.03757},
  urldate = {2025-08-02},
  abstract = {Manifold learning (ML), known also as non-linear dimension reduction, is a set of methods to find the low dimensional structure of data. Dimension reduction for large, high dimensional data is not merely a way to reduce the data; the new representations and descriptors obtained by ML reveal the geometric shape of high dimensional point clouds, and allow one to visualize, denoise and interpret them. This survey presents the principles underlying ML, the representative methods, as well as their statistical foundations from a practicing statistician's perspective. It describes the trade-offs, and what theory tells us about the parameter and algorithmic choices we make in order to obtain reliable conclusions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/AD52AKJB/Meilă and Zhang - 2023 - Manifold learning what, how, and why.pdf}
}

@misc{melnichenkoCholeskyQRRandomizationPivoting2024,
  title = {{{CholeskyQR}} with {{Randomization}} and {{Pivoting}} for {{Tall Matrices}} ({{CQRRPT}})},
  author = {Melnichenko, Maksim and Balabanov, Oleg and Murray, Riley and Demmel, James and Mahoney, Michael W. and Luszczek, Piotr},
  year = {2024},
  month = feb,
  number = {arXiv:2311.08316},
  eprint = {2311.08316},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.08316},
  urldate = {2024-05-12},
  abstract = {This paper develops and analyzes a new algorithm for QR decomposition with column pivoting (QRCP) of rectangular matrices with large row counts. The algorithm combines methods from randomized numerical linear algebra in a particularly careful way in order to accelerate both pivot decisions for the input matrix and the process of decomposing the pivoted matrix into the QR form. The source of the latter acceleration is a use of randomized preconditioning and CholeskyQR. Comprehensive analysis is provided in both exact and finite-precision arithmetic to characterize the algorithm's rank-revealing properties and its numerical stability granted probabilistic assumptions of the sketching operator. An implementation of the proposed algorithm is described and made available inside the open-source RandLAPACK library, which itself relies on RandBLAS - also available in open-source format. Experiments with this implementation on an Intel Xeon Gold 6248R CPU demonstrate order-of-magnitude speedups relative to LAPACK's standard function for QRCP, and comparable performance to a specialized algorithm for unpivoted QR of tall matrices, which lacks the strong rank-revealing properties of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/Users/nobr/Zotero/storage/PRSYS444/Melnichenko et al. - 2024 - CholeskyQR with Randomization and Pivoting for Tall Matrices (CQRRPT).pdf;/Users/nobr/Zotero/storage/77UWTMD6/2311.html}
}

@article{meng2022,
  title = {Decoding {{Visual fMRI Stimuli}} from {{Human Brain Based}} on {{Graph Convolutional Neural Network}}},
  author = {Meng, Lu and Ge, Kang},
  year = {2022},
  month = oct,
  journal = {Brain Sciences},
  volume = {12},
  number = {10},
  pages = {1394},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3425},
  doi = {10.3390/brainsci12101394},
  urldate = {2023-05-09},
  abstract = {Brain decoding is to predict the external stimulus information from the collected brain response activities, and visual information is one of the most important sources of external stimulus information. Decoding functional magnetic resonance imaging (fMRI) based on visual stimulation is helpful in understanding the working mechanism of the brain visual function regions. Traditional brain decoding algorithms cannot accurately extract stimuli features from fMRI. To address these shortcomings, this paper proposed a brain decoding algorithm based on a graph convolution network (GCN). Firstly, 11 regions of interest (ROI) were selected according to the human brain visual function regions, which can avoid the noise interference of the non-visual regions of the human brain; then, a deep three-dimensional convolution neural network was specially designed to extract the features of these 11 regions; next, the GCN was used to extract the functional correlation features between the different human brain visual regions. Furthermore, to avoid the problem of gradient disappearance when there were too many layers of graph convolutional neural network, the residual connections were adopted in our algorithm, which helped to integrate different levels of features in order to improve the accuracy of the proposed GCN. The proposed algorithm was tested on the public dataset, and the recognition accuracy reached 98.67\%. Compared with the other state-of-the-art algorithms, the proposed algorithm performed the best.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {brain decoding,convolutional neural network,functional magnetic resonance image,graph convolution},
  file = {/Users/nobr/Zotero/storage/JEUAYGEY/Meng and Ge - 2022 - Decoding Visual fMRI Stimuli from Human Brain Base.pdf}
}

@misc{merrick2019,
  title = {Randomized {{Ablation Feature Importance}}},
  author = {Merrick, Luke},
  year = {2019},
  month = oct,
  number = {arXiv:1910.00174},
  eprint = {1910.00174},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.00174},
  urldate = {2025-08-12},
  abstract = {Given a model \$f\$ that predicts a target \$y\$ from a vector of input features \${\textbackslash}pmb\{x\} = x\_1, x\_2, {\textbackslash}ldots, x\_M\$, we seek to measure the importance of each feature with respect to the model's ability to make a good prediction. To this end, we consider how (on average) some measure of goodness or badness of prediction (which we term "loss" \${\textbackslash}ell\$), changes when we hide or ablate each feature from the model. To ablate a feature, we replace its value with another possible value randomly. By averaging over many points and many possible replacements, we measure the importance of a feature on the model's ability to make good predictions. Furthermore, we present statistical measures of uncertainty that quantify how confident we are that the feature importance we measure from our finite dataset and finite number of ablations is close to the theoretical true importance value.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/DDU8X25L/Merrick - 2019 - Randomized Ablation Feature Importance.pdf}
}

@article{metoyer2010,
  title = {Explaining How to Play Real-Time Strategy Games},
  author = {Metoyer, Ronald and Stumpf, Simone and Neumann, Christoph and Dodge, Jonathan and Cao, Jill and Schnabel, Aaron},
  year = {2010},
  month = may,
  journal = {Knowledge-Based Systems},
  volume = {23},
  number = {4},
  pages = {295--301},
  issn = {09507051},
  doi = {10.1016/j.knosys.2009.11.006},
  urldate = {2023-11-12},
  abstract = {Real-time strategy games share many aspects with real situations in domains such as battle planning, air traffic control, and emergency response team management which makes them appealing test-beds for Artificial Intelligence (AI) and machine learning. End-user annotations could help to provide supplemental information for learning algorithms, especially when training data is sparse. This paper presents a formative study to uncover how experienced users explain game play in real-time strategy games. We report the results of our analysis of explanations and discuss their characteristics that could support the design of systems for use by experienced real-time strategy game users in specifying or annotating strategy-oriented behavior.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/YLGPDWMB/Metoyer et al. - 2010 - Explaining how to play real-time strategy games.pdf}
}

@misc{metzUnderstandingCorrectingPathologies2019,
  title = {Understanding and Correcting Pathologies in the Training of Learned Optimizers},
  author = {Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Freeman, C. Daniel and {Sohl-Dickstein}, Jascha},
  year = {2019},
  month = jun,
  number = {arXiv:1810.10180},
  eprint = {1810.10180},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.10180},
  urldate = {2024-02-13},
  abstract = {Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process resulting in gradients that are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance, allowing us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/9UWVP8AV/Metz et al. - 2019 - Understanding and correcting pathologies in the training of learned optimizers.pdf;/Users/nobr/Zotero/storage/EGFCZBVE/1810.html}
}

@book{michalewicz1994,
  title = {Genetic Algorithms + Data Structures = Evolution Programs},
  author = {Michalewicz, Zbigniew},
  year = {1994},
  edition = {2nd, extended ed},
  publisher = {Springer-Verlag},
  address = {Berlin ; New York},
  isbn = {978-3-540-58090-4 978-0-387-58090-6},
  lccn = {QA76.9.A43 M53 1994},
  keywords = {Data structures (Computer science),Evolutionary programming (Computer science),Genetic algorithms},
  file = {/Users/nobr/Zotero/storage/WHK2V8ZU/Genetic Algorithms Data Structures Evolution Programs -- Michalewicz Zbigniew -- 3 -- Springer -- 9783540580904 -- ddf3b9f50c2ca2b23f019cf8cfb9c35f -- Anna’s Archive.pdf}
}

@inproceedings{miconi2008,
  title = {Evosphere: {{Evolutionary}} Dynamics in a Population of Fighting Virtual Creatures},
  shorttitle = {Evosphere},
  booktitle = {2008 {{IEEE Congress}} on {{Evolutionary Computation}} ({{IEEE World Congress}} on {{Computational Intelligence}})},
  author = {Miconi, Thomas},
  year = {2008},
  month = jun,
  pages = {3066--3073},
  issn = {1941-0026},
  doi = {10.1109/CEC.2008.4631212},
  urldate = {2025-05-21},
  abstract = {It is often suggested that traditional models of artificial evolution, based on explicit, human-defined fitness functions, are fundamentally more restricted and less creative than natural evolution, in which no such constraint exists. After a discussion and refinement of this statement, we suggest a classification of evolutionary systems according to their evolutionary ldquocreativityrdquo. We describe an environment, called Evosphere, in which a population of 3D creatures interact, fight with each other, and evolve freely on the surface of a ldquomicroplanetrdquo. We demonstrate the onset of natural selection and adaptive evolution within this virtual world, both by visual inspection and statistical analysis. We show that the introduction of reproductively isolated species enriches the dynamics of the system, leading to simple evolutionary feedbacks among species.},
  keywords = {Adaptation model,Complexity theory,Evolution (biology),Evolutionary computation,Genetics,Neurons,Sensors},
  file = {/Users/nobr/Zotero/storage/ZCDD87SF/Miconi - 2008 - Evosphere Evolutionary dynamics in a population of fighting virtual creatures.pdf}
}

@misc{mikolov2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/686A7QKV/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf}
}

@article{milewski,
  title = {Category {{Theory}} for {{Programmers}}},
  author = {Milewski, Bartosz},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UDALVZGQ/category-theory-for-programmers.pdf}
}

@book{millington2019,
  title = {{{AI}} for Games},
  author = {Millington, Ian},
  year = {2019},
  edition = {Third edition},
  publisher = {CRC Press, Taylor \& Francis Group},
  address = {Boca Raton},
  isbn = {978-1-138-48397-2},
  langid = {english},
  lccn = {QA76.76.C672 M549 2019},
  keywords = {Artificial intelligence,Computer animation,Programming,Video games}
}

@article{mills2018,
  title = {Correlated {{Gene Expression}} and {{Anatomical Communication Support Synchronized Brain Activity}} in the {{Mouse Functional Connectome}}},
  author = {Mills, Brian D. and Grayson, David S. and Shunmugavel, Anandakumar and {Miranda-Dominguez}, Oscar and Feczko, Eric and Earl, Eric and Neve, Kim A. and Fair, Damien A.},
  year = {2018},
  month = jun,
  journal = {The Journal of Neuroscience},
  volume = {38},
  number = {25},
  pages = {5774--5787},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.2910-17.2018},
  urldate = {2023-05-13},
  abstract = {Cognition and behavior depend on synchronized intrinsic brain activity that is organized into functional networks across the brain. Research has investigated how anatomical connectivity both shapes and is shaped by these networks, but not how anatomical connectivity interacts with intra-areal molecular properties to drive functional connectivity. Here, we present a novel linear model to explain functional connectivity by integrating systematically obtained measurements of axonal connectivity, gene expression, and resting-state functional connectivity MRI in the mouse brain. The model suggests that functional connectivity arises from both anatomical links and inter-areal similarities in gene expression. By estimating these effects, we identify anatomical modules in which correlated gene expression and anatomical connectivity support functional connectivity. Along with providing evidence that not all genes equally contribute to functional connectivity, this research establishes new insights regarding the biological underpinnings of coordinated brain activity measured by BOLD fMRI., SIGNIFICANCE STATEMENT Efforts at characterizing the functional connectome with fMRI have risen exponentially over the last decade. Yet despite this rise, the biological underpinnings of these functional measurements are still primarily unknown. The current report begins to fill this void by investigating the molecular underpinnings of the functional connectome through an integration of systematically obtained structural information and gene expression data throughout the rodent brain. We find that both white matter connectivity and similarity in regional gene expression relate to resting-state functional connectivity. The current report furthers our understanding of the biological underpinnings of the functional connectome and provides a linear model that can be used to streamline preclinical animal studies of disease.},
  pmcid = {PMC6010566},
  pmid = {29789379},
  file = {/Users/nobr/Zotero/storage/9S6HKV9R/Mills et al. - 2018 - Correlated Gene Expression and Anatomical Communic.pdf}
}

@book{milton2007,
  title = {Paradise Lost},
  author = {Milton, John and Lewalski, Barbara Kiefer},
  year = {2007},
  publisher = {Blackwell Pub},
  address = {Malden, MA},
  isbn = {978-1-4051-2928-2 978-1-4051-2929-9},
  langid = {english},
  lccn = {PR3560.A2 L49 2007},
  keywords = {Adam,Bible,Devil,Eve,Fall of man,History of Biblical events,Poetry},
  file = {/Users/nobr/Zotero/storage/WZ8RCS8Z/Milton and Lewalski - 2007 - Paradise lost.pdf}
}

@article{min2023,
  title = {Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: {{A}} Survey},
  author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
  year = {2023},
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {2},
  pages = {1--40},
  publisher = {ACM New York, NY}
}

@techreport{ministry_finance2018,
  title = {Study by the Secretariat of Economic Policy},
  year = {2018},
  month = jun,
  institution = {Ministry of Finance, Brazil}
}

@book{mitchell1997,
  title = {Machine {{Learning}}},
  author = {Mitchell, Tom M.},
  year = {1997},
  series = {{{McGraw-Hill}} Series in Computer Science},
  publisher = {McGraw-Hill},
  address = {New York},
  isbn = {978-0-07-042807-2},
  lccn = {Q325.5 .M58 1997},
  keywords = {Computer algorithms,Machine learning}
}

@misc{mitchell2024,
  title = {The {{Genomic Code}}: {{The}} Genome Instantiates a Generative Model of the Organism},
  shorttitle = {The {{Genomic Code}}},
  author = {Mitchell, Kevin J. and Cheney, Nick},
  year = {2024},
  month = jul,
  number = {arXiv:2407.15908},
  eprint = {2407.15908},
  primaryclass = {q-bio},
  doi = {10.48550/arXiv.2407.15908},
  urldate = {2024-08-01},
  abstract = {How does the genome encode the form of the organism? What is the nature of this genomic code? Common metaphors, such as a blueprint or program, fail to capture the complex, indirect, and evolutionarily dynamic relationship between the genome and organismal form, or the constructive, interactive processes that produce it. Such metaphors are also not readily formalised, either to treat empirical data or to simulate genomic encoding of form in silico. Here, we propose a new analogy, inspired by recent work in machine learning and neuroscience: that the genome encodes a generative model of the organism. In this scheme, by analogy with variational autoencoders, the genome does not encode either organismal form or developmental processes directly, but comprises a compressed space of latent variables. These latent variables are the DNA sequences that specify the biochemical properties of encoded proteins and the relative affinities between trans-acting regulatory factors and their target sequence elements. Collectively, these comprise a connectionist network, with weights that get encoded by the learning algorithm of evolution and decoded through the processes of development. The latent variables collectively shape an energy landscape that constrains the self-organising processes of development so as to reliably produce a new individual of a certain type, providing a direct analogy to Waddingtons famous epigenetic landscape. The generative model analogy accounts for the complex, distributed genetic architecture of most traits and the emergent robustness and evolvability of developmental processes. It also provides a new way to explain the independent selectability of specific traits, drawing on the idea of multiplexed disentangled representations observed in artificial and neural systems and lends itself to formalisation.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Other Quantitative Biology},
  file = {/Users/nobr/Zotero/storage/68J6MKZD/Mitchell and Cheney - 2024 - The Genomic Code The genome instantiates a generative model of the organism.pdf}
}

@book{mitra2001,
  title = {Digital Signal Processing: A Computer-Based Approach},
  shorttitle = {Digital Signal Processing},
  author = {Mitra, Sanjit Kumar},
  year = {2001},
  series = {{{McGraw-Hill}} Series in Electrical and Computer Engineering},
  edition = {2nd ed},
  publisher = {McGraw-Hill/Irwin},
  address = {Boston, Mass.},
  isbn = {978-0-07-232105-0 978-0-07-252261-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/USP54EEI/(McGraw) Digital Signal Processing--Computer Based Approach (2nd Ed) (Mitra).2.pdf}
}

@misc{mitraOrca2Teaching2023,
  title = {Orca 2: {{Teaching Small Language Models How}} to {{Reason}}},
  shorttitle = {Orca 2},
  author = {Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes, Clarisse and Agrawal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and Palangi, Hamid and Zheng, Guoqing and Rosset, Corby and Khanpour, Hamed and Awadallah, Ahmed},
  year = {2023},
  month = nov,
  number = {arXiv:2311.11045},
  eprint = {2311.11045},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2311.11045},
  urldate = {2023-11-21},
  abstract = {Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. We open-source Orca 2 to encourage further research on the development, evaluation, and alignment of smaller LMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/YKHCDQ7H/Mitra et al. - 2023 - Orca 2 Teaching Small Language Models How to Reason.pdf}
}

@article{miyawaki2008,
  title = {Visual {{Image Reconstruction}} from {{Human Brain Activity}} Using a {{Combination}} of {{Multiscale Local Image Decoders}}},
  author = {Miyawaki, Yoichi and Uchida, Hajime and Yamashita, Okito and Sato, Masa-aki and Morito, Yusuke and Tanabe, Hiroki C. and Sadato, Norihiro and Kamitani, Yukiyasu},
  year = {2008},
  month = dec,
  journal = {Neuron},
  volume = {60},
  number = {5},
  pages = {915--929},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.11.004},
  urldate = {2023-05-08},
  abstract = {Perceptual experience consists of an enormous number of possible states. Previous fMRI studies have predicted a perceptual state by classifying brain activity into prespecified categories. Constraint-free visual image reconstruction is more challenging, as it is impractical to specify brain activity for all possible images. In this study, we reconstructed visual images by combining local image bases of multiple scales, whose contrasts were independently decoded from fMRI activity by automatically selecting relevant voxels and exploiting their correlated patterns. Binary-contrast, 10 {\texttimes} 10-patch images (2100 possible states) were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images. Reconstruction was also used to identify the presented image among millions of candidates. The results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in multivoxel patterns.},
  langid = {english},
  keywords = {SYSNEURO},
  file = {/Users/nobr/Zotero/storage/9ZXPJCX9/Miyawaki et al. - 2008 - Visual Image Reconstruction from Human Brain Activ.pdf;/Users/nobr/Zotero/storage/ZWRHDLCM/S0896627308009586.html}
}

@article{mnih2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, K. and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A.},
  year = {2013},
  month = dec,
  journal = {ArXiv},
  urldate = {2023-12-31},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  file = {/Users/nobr/Zotero/storage/QUB5KNWG/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf}
}

@misc{mohammadiCreativityHasLeft2024,
  title = {Creativity {{Has Left}} the {{Chat}}: {{The Price}} of {{Debiasing Language Models}}},
  shorttitle = {Creativity {{Has Left}} the {{Chat}}},
  author = {Mohammadi, Behnam},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05587},
  eprint = {2406.05587},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.05587},
  urldate = {2024-06-17},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing but can exhibit biases and may generate toxic content. While alignment techniques like Reinforcement Learning from Human Feedback (RLHF) reduce these issues, their impact on creativity, defined as syntactic and semantic diversity, remains unexplored. We investigate the unintended consequences of RLHF on the creativity of LLMs through three experiments focusing on the Llama-2 series. Our findings reveal that aligned models exhibit lower entropy in token predictions, form distinct clusters in the embedding space, and gravitate towards "attractor states", indicating limited output diversity. Our findings have significant implications for marketers who rely on LLMs for creative tasks such as copywriting, ad creation, and customer persona generation. The trade-off between consistency and creativity in aligned models should be carefully considered when selecting the appropriate model for a given application. We also discuss the importance of prompt engineering in harnessing the creative potential of base models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/XL8P9JHM/Mohammadi - 2024 - Creativity Has Left the Chat The Price of Debiasing Language Models.pdf}
}

@misc{molybog2023,
  title = {A {{Theory}} on {{Adam Instability}} in {{Large-Scale Machine Learning}}},
  author = {Molybog, Igor and Albert, Peter and Chen, Moya and DeVito, Zachary and Esiobu, David and Goyal, Naman and Koura, Punit Singh and Narang, Sharan and Poulton, Andrew and Silva, Ruan and Tang, Binh and Liskovich, Diana and Xu, Puxin and Zhang, Yuchen and Kambadur, Melanie and Roller, Stephen and Zhang, Susan},
  year = {2023},
  month = apr,
  number = {arXiv:2304.09871},
  eprint = {2304.09871},
  primaryclass = {cs, math},
  doi = {10.48550/arXiv.2304.09871},
  urldate = {2023-07-18},
  abstract = {We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/nobr/Zotero/storage/V55TPJYK/Molybog et al. - 2023 - A Theory on Adam Instability in Large-Scale Machine Learning.pdf}
}

@misc{monizReALMReferenceResolution2024,
  title = {{{ReALM}}: {{Reference Resolution As Language Modeling}}},
  shorttitle = {{{ReALM}}},
  author = {Moniz, Joel Ruben Antony and Krishnan, Soundarya and Ozyildirim, Melis and Saraf, Prathamesh and Ates, Halim Cagri and Zhang, Yuan and Yu, Hong and Rajshree, Nidhi},
  year = {2024},
  month = mar,
  number = {arXiv:2403.20329},
  eprint = {2403.20329},
  primaryclass = {cs},
  urldate = {2024-04-02},
  abstract = {Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5\% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/977S5Q53/Moniz et al. - 2024 - ReALM Reference Resolution As Language Modeling.pdf}
}

@book{moreira2006,
  title = {Essentials of {{Error}}-{{Control Coding}}},
  author = {Moreira, Jorge Casti{\~n}eira and Farrell, Patrick Guy},
  year = {2006},
  month = jul,
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9780470035726},
  urldate = {2023-08-20},
  isbn = {978-0-470-02920-6 978-0-470-03572-6},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/G6YXISDY/Castiñeira Moreira and Farrell - 2006 - Essentials of Error‐Control Coding.pdf}
}

@misc{mosinFineTuningTransformersVocabulary2022,
  title = {Fine-{{Tuning Transformers}}: {{Vocabulary Transfer}}},
  shorttitle = {Fine-{{Tuning Transformers}}},
  author = {Mosin, Vladislav and Samenko, Igor and Tikhonov, Alexey and Kozlovskii, Borislav and Yamshchikov, Ivan P.},
  year = {2022},
  month = dec,
  number = {arXiv:2112.14569},
  eprint = {2112.14569},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-21},
  abstract = {Transformers are responsible for the vast majority of recent advances in natural language processing. The majority of practical natural language processing applications of these models are typically enabled through transfer learning. This paper studies if corpus-specific tokenization used for fine-tuning improves the resulting performance of the model. Through a series of experiments, we demonstrate that such tokenization combined with the initialization and fine-tuning strategy for the vocabulary tokens speeds up the transfer and boosts the performance of the fine-tuned model. We call this aspect of transfer facilitation vocabulary transfer.},
  archiveprefix = {arXiv},
  keywords = {68T50 91F20,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {/Users/nobr/Zotero/storage/NBQZ8IZH/Mosin et al. - 2022 - Fine-Tuning Transformers Vocabulary Transfer.pdf;/Users/nobr/Zotero/storage/WCXWRKI7/2112.html}
}

@article{mount,
  title = {{{CMSC}} 425: {{Lecture}} 14 {{Procedural Generation}}: {{Perlin Noise}}},
  author = {Mount, Dave},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/9Y4SXFJB/Mount - CMSC 425 Lecture 14 Procedural Generation Perlin Noise.pdf}
}

@misc{mouret2015,
  title = {Illuminating Search Spaces by Mapping Elites},
  author = {Mouret, Jean-Baptiste and Clune, Jeff},
  year = {2015},
  month = apr,
  number = {arXiv:1504.04909},
  eprint = {1504.04909},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-04-05},
  abstract = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Populations and Evolution},
  file = {/Users/nobr/Zotero/storage/C4QL8U53/Mouret and Clune - 2015 - Illuminating search spaces by mapping elites.pdf;/Users/nobr/Zotero/storage/87F6KBH8/1504.html}
}

@article{mueller2016,
  title = {Siamese Recurrent Architectures for Learning Sentence Similarity},
  author = {Mueller, Jonas and Thyagarajan, Aditya},
  year = {2016},
  journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
  pages = {2786--2792},
  publisher = {AAAI press},
  issn = {2159-5399},
  doi = {10.1609/AAAI.V30I1.10350},
  urldate = {2023-04-18},
  abstract = {We present a siamese adaptation of the Long Short-Term Memory (LSTM) network for labeled data comprised of pairs of variable-length sequences. Our model is applied to assess semantic similarity between sentences, where we exceed state of the art, outperforming carefully handcrafted features and recently proposed neural network systems of greater complexity. For these applications, we provide wordembedding vectors supplemented with synonymic information to the LSTMs, which use a fixed size vector to encode the underlying meaning expressed in a sentence (irrespective of the particular wording/syntax). By restricting subsequent operations to rely on a simple Manhattan metric, we compel the sentence representations learned by our model to form a highly structured space whose geometry reflects complex semantic relationships. Our results are the latest in a line of findings that showcase LSTMs as powerful language models capable of tasks requiring intricate understanding.},
  isbn = {9781577357605},
  file = {/Users/nobr/Zotero/storage/CQSK4RQV/full-text.pdf}
}

@misc{mukherjeeOrcaProgressiveLearning2023,
  title = {Orca: {{Progressive Learning}} from {{Complex Explanation Traces}} of {{GPT-4}}},
  shorttitle = {Orca},
  author = {Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
  year = {2023},
  month = jun,
  number = {arXiv:2306.02707},
  eprint = {2306.02707},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.02707},
  urldate = {2024-01-15},
  abstract = {Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100\% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42\% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/X4XU54ZA/Mukherjee et al. - 2023 - Orca Progressive Learning from Complex Explanation Traces of GPT-4.pdf;/Users/nobr/Zotero/storage/MF9V6N8T/2306.html}
}

@misc{muLearningCompressPrompts2024,
  title = {Learning to {{Compress Prompts}} with {{Gist Tokens}}},
  author = {Mu, Jesse and Li, Xiang Lisa and Goodman, Noah},
  year = {2024},
  month = feb,
  number = {arXiv:2304.08467},
  eprint = {2304.08467},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08467},
  urldate = {2024-03-01},
  abstract = {Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of "gist" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40\% FLOPs reductions, 4.2\% wall time speedups, and storage savings, all with minimal loss in output quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/N4NU7NEU/Mu et al. - 2024 - Learning to Compress Prompts with Gist Tokens.pdf;/Users/nobr/Zotero/storage/BJ4KRYEB/2304.html}
}

@article{muller2017,
  title = {What {{Is Morphological Computation}}? {{On How}} the {{Body Contributes}} to {{Cognition}} and {{Control}}},
  author = {M{\"u}ller, Vincent C. and Hoffmann, Matej},
  year = {2017},
  month = feb,
  journal = {Artificial Life},
  volume = {23},
  number = {1},
  pages = {1--24},
  publisher = {MIT Press Journals},
  issn = {15309185},
  doi = {10.1162/ARTL_a_00219},
  abstract = {The contribution of the body to cognition and control in natural and artificial agents is increasingly described as "offloading computation from the brain to the body," where the body is said to perform "morphological computation." Our investigation of four characteristic cases of morphological computation in animals and robots shows that the "offloading" perspective is misleading. Actually, the contribution of body morphology to cognition and control is rarely computational, in any useful sense of the word. We thus distinguish (1) morphology that facilitates control, (2) morphology that facilitates perception, and the rare cases of (3) morphological computation proper, such as reservoir computing, where the body is actually used for computation. This result contributes to the understanding of the relation between embodiment and computation: The question for robot design and cognitive science is not whether computation is offloaded to the body, but to what extent the body facilitates cognition and control - how it contributes to the overall orchestration of intelligent behavior.},
  pmid = {28140632},
  keywords = {Body,Cognition,Computation,Control,Embodiment,Soft robotics},
  file = {/Users/nobr/Zotero/storage/HUFFJMB8/Müller and Hoffmann - 2017 - What Is Morphological Computation On How the Body Contributes to Cognition and Control.pdf}
}

@misc{mundler2025,
  title = {Type-{{Constrained Code Generation}} with {{Language Models}}},
  author = {M{\"u}ndler, Niels and He, Jingxuan and Wang, Hao and Sen, Koushik and Song, Dawn and Vechev, Martin},
  year = {2025},
  month = may,
  eprint = {2504.09246},
  primaryclass = {cs},
  doi = {10.1145/3729274},
  urldate = {2025-05-14},
  abstract = {Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic features of general-purpose programming languages. However, LLMs frequently generate code with typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. For this purpose, we develop novel prefix automata and a search over inhabitable types, forming a sound approach to enforce well-typedness on LLM-generated code. We formalize our approach on a foundational simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on the HumanEval and MBPP datasets shows that our approach reduces compilation errors by more than half and significantly increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including state-of-the-art open-weight models with more than 30B parameters. The results demonstrate the generality and effectiveness of our approach in constraining LLM code generation with formal rules of type systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/nobr/Zotero/storage/PNNVDG2J/Mündler et al. - 2025 - Type-Constrained Code Generation with Language Models.pdf}
}

@article{mundler2025a,
  title = {Type-{{Constrained Code Generation}} with {{Language Models}}},
  author = {M{\"u}ndler, Niels and He, Jingxuan and Wang, Hao and Sen, Koushik and Song, Dawn and Vechev, Martin},
  year = {2025},
  month = jun,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {9},
  number = {PLDI},
  eprint = {2504.09246},
  primaryclass = {cs},
  pages = {601--626},
  issn = {2475-1421},
  doi = {10.1145/3729274},
  urldate = {2025-08-02},
  abstract = {Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic features of general-purpose programming languages. However, LLMs frequently generate code with typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. For this purpose, we develop novel prefix automata and a search over inhabitable types, forming a sound approach to enforce well-typedness on LLM-generated code. We formalize our approach on a foundational simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on the HumanEval and MBPP datasets shows that our approach reduces compilation errors by more than half and significantly increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including state-of-the-art open-weight models with more than 30B parameters. The results demonstrate the generality and effectiveness of our approach in constraining LLM code generation with formal rules of type systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/nobr/Zotero/storage/AJ7VJK8Z/Mündler et al. - 2025 - Type-Constrained Code Generation with Language Models.pdf}
}

@book{murphy2022,
  title = {Probabilistic Machine Learning: An Introduction},
  shorttitle = {Probabilistic Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2022},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR"--},
  isbn = {978-0-262-04682-4},
  langid = {english},
  lccn = {Q325.5 .M872 2022},
  keywords = {Machine learning,Probabilities}
}

@book{murphy2023,
  title = {Probabilistic Machine Learning: Advanced Topics},
  shorttitle = {Probabilistic Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2023},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts London, England},
  isbn = {978-0-262-04843-9},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/WPZGGZRA/Murphy - 2023 - Probabilistic machine learning advanced topics.pdf}
}

@book{murray2013,
  title = {Interactive Data Visualization for the Web: An Introduction to Designing with {{D3}}},
  shorttitle = {Interactive Data Visualization for the Web},
  author = {Murray, Scott},
  year = {2013},
  edition = {1. ed},
  publisher = {O'Reilly},
  address = {Beijing K{\"o}ln},
  isbn = {978-1-4493-3973-9},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/724L9VKC/Murray - 2013 - Interactive data visualization for the web an introduction to designing with D3.pdf}
}

@misc{murray2023,
  title = {Randomized {{Numerical Linear Algebra}} : {{A Perspective}} on the {{Field With}} an {{Eye}} to {{Software}}},
  shorttitle = {Randomized {{Numerical Linear Algebra}}},
  author = {Murray, Riley and Demmel, James and Mahoney, Michael W. and Erichson, N. Benjamin and Melnichenko, Maksim and Malik, Osman Asif and Grigori, Laura and Luszczek, Piotr and Derezi{\'n}ski, Micha{\l} and Lopes, Miles E. and Liang, Tianyu and Luo, Hengrui and Dongarra, Jack},
  year = {2023},
  month = apr,
  number = {arXiv:2302.11474},
  eprint = {2302.11474},
  primaryclass = {cs, math},
  doi = {10.48550/arXiv.2302.11474},
  urldate = {2024-08-24},
  abstract = {Randomized numerical linear algebra - RandNLA, for short - concerns the use of randomization as a resource to develop improved algorithms for large-scale linear algebra computations. The origins of contemporary RandNLA lay in theoretical computer science, where it blossomed from a simple idea: randomization provides an avenue for computing approximate solutions to linear algebra problems more efficiently than deterministic algorithms. This idea proved fruitful in the development of scalable algorithms for machine learning and statistical data analysis applications. However, RandNLA's true potential only came into focus upon integration with the fields of numerical analysis and "classical" numerical linear algebra. Through the efforts of many individuals, randomized algorithms have been developed that provide full control over the accuracy of their solutions and that can be every bit as reliable as algorithms that might be found in libraries such as LAPACK. Recent years have even seen the incorporation of certain RandNLA methods into MATLAB, the NAG Library, NVIDIA's cuSOLVER, and SciKit-Learn. For all its success, we believe that RandNLA has yet to realize its full potential. In particular, we believe the scientific community stands to benefit significantly from suitably defined "RandBLAS" and "RandLAPACK" libraries, to serve as standards conceptually analogous to BLAS and LAPACK. This 200-page monograph represents a step toward defining such standards. In it, we cover topics spanning basic sketching, least squares and optimization, low-rank approximation, full matrix decompositions, leverage score sampling, and sketching data with tensor product structures (among others). Much of the provided pseudo-code has been tested via publicly available MATLAB and Python implementations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {/Users/nobr/Zotero/storage/UV3GPY9D/Murray et al. - 2023 - Randomized Numerical Linear Algebra  A Perspective on the Field With an Eye to Software.pdf}
}

@incollection{myhre2013anthropogenic,
  title = {Anthropogenic and Natural Radiative Forcing},
  booktitle = {Climate Change 2013: {{The}} Physical Science Basis. {{Contribution}} of Working Group {{I}} to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change},
  author = {Myhre, G. and Shindell, D. and Br{\'e}on, F.-M. and Collins, W. and Fuglestvedt, J. and Huang, J. and Koch, D. and Lamarque, J.-F. and Lee, D. and Mendoza, B. and Nakajima, T. and Robock, A. and Stephens, G. and Takemura, T. and Zhang, H.},
  editor = {Stocker, T. F. and Qin, D. and Plattner, G.-K. and Tignor, M. and Allen, S. K. and Boschung, J. and Nauels, A. and Xia, Y. and Bex, V. and Midgley, P. M.},
  year = {2013},
  pages = {659--740},
  publisher = {Cambridge University Press},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  doi = {10.1017/CBO9781107415324.018}
}

@inproceedings{nachum2018,
  title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Nachum, Ofir and Gu, S. and Lee, Honglak and Levine, S.},
  year = {2018},
  month = may,
  urldate = {2024-01-22},
  abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
  file = {/Users/nobr/Zotero/storage/9Y3UJEQM/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf}
}

@article{nagel1974,
  title = {What {{Is It Like}} to {{Be}} a {{Bat}}?},
  author = {Nagel, Thomas},
  year = {1974},
  month = oct,
  journal = {The Philosophical Review},
  volume = {83},
  number = {4},
  eprint = {2183914},
  eprinttype = {jstor},
  pages = {435},
  issn = {00318108},
  doi = {10.2307/2183914},
  urldate = {2025-08-02},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZKKR3PJK/Nagel - 1974 - What Is It Like to Be a Bat.pdf}
}

@misc{najarroSelfAssemblingArtificialNeural2023,
  title = {Towards {{Self-Assembling Artificial Neural Networks}} through {{Neural Developmental Programs}}},
  author = {Najarro, Elias and Sudhakaran, Shyam and Risi, Sebastian},
  year = {2023},
  month = jul,
  number = {arXiv:2307.08197},
  eprint = {2307.08197},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.08197},
  urldate = {2024-02-02},
  abstract = {Biological nervous systems are created in a fundamentally different way than current artificial neural networks. Despite its impressive results in a variety of different domains, deep learning often requires considerable engineering effort to design high-performing neural architectures. By contrast, biological nervous systems are grown through a dynamic self-organizing process. In this paper, we take initial steps toward neural networks that grow through a developmental process that mirrors key properties of embryonic development in biological organisms. The growth process is guided by another neural network, which we call a Neural Developmental Program (NDP) and which operates through local communication alone. We investigate the role of neural growth on different machine learning benchmarks and different optimization methods (evolutionary training, online RL, offline RL, and supervised learning). Additionally, we highlight future research directions and opportunities enabled by having self-organization driving the growth of neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/UAVR696C/Najarro et al. - 2023 - Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs.pdf;/Users/nobr/Zotero/storage/BS96FH3B/2307.html}
}

@misc{nandaEmergentLinearRepresentations2023,
  title = {Emergent {{Linear Representations}} in {{World Models}} of {{Self-Supervised Sequence Models}}},
  author = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  year = {2023},
  month = sep,
  number = {arXiv:2309.00941},
  eprint = {2309.00941},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.00941},
  urldate = {2024-05-27},
  abstract = {How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/6IIQ223C/Nanda et al. - 2023 - Emergent Linear Representations in World Models of Self-Supervised Sequence Models.pdf;/Users/nobr/Zotero/storage/2XTLUSDS/2309.html}
}

@misc{nandaProgressMeasuresGrokking2023,
  title = {Progress Measures for Grokking via Mechanistic Interpretability},
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  year = {2023},
  month = oct,
  number = {arXiv:2301.05217},
  eprint = {2301.05217},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-16},
  abstract = {Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous {\textbackslash}textit\{progress measures\} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/KAIQJ3PJ/Nanda et al. - 2023 - Progress measures for grokking via mechanistic interpretability.pdf;/Users/nobr/Zotero/storage/45ARCBR2/2301.html}
}

@inproceedings{narasimhan2015,
  title = {Language {{Understanding}} for {{Text-based Games}} Using {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Narasimhan, Karthik and Kulkarni, Tejas and Barzilay, Regina},
  year = {2015},
  pages = {1--11},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1001},
  urldate = {2025-06-27},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/N3MVSC9G/Narasimhan et al. - 2015 - Language Understanding for Text-based Games using Deep Reinforcement Learning.pdf}
}

@article{narvekar2017,
  title = {Curriculum {{Learning}} in {{Reinforcement Learning}}},
  author = {Narvekar, Sanmit},
  year = {2017},
  pages = {5195--5196},
  doi = {10.24963/ijcai.2017/757},
  file = {/Users/nobr/Zotero/storage/7BCKSV65/Narvekar - 2017 - Curriculum Learning in Reinforcement Learning.pdf}
}

@misc{nasa_earth_observatory,
  title = {World of Change: {{Global}} Temperatures}
}

@misc{nascimento2023,
  title = {{{GPT-in-the-Loop}}: {{Adaptive Decision-Making}} for {{Multiagent Systems}}},
  shorttitle = {{{GPT-in-the-Loop}}},
  author = {Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
  year = {2023},
  month = aug,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {This paper introduces the ``GPT-in-the-loop'' approach, a novel method combining the advanced reasoning capabilities of Large Language Models (LLMs) like Generative Pretrained Transformers (GPT) with multiagent (MAS) systems. Venturing beyond traditional adaptive approaches that generally require long training processes, our framework employs GPT-4 for enhanced problem-solving and explanation skills. Our experimental backdrop is the smart streetlight Internet of Things (IoT) application. Here, agents use sensors, actuators, and neural networks to create an energy-efficient lighting system. By integrating GPT-4, these agents achieve superior decision-making and adaptability without the need for extensive training. We compare this approach with both traditional neuroevolutionary methods and solutions provided by software engineers, underlining the potential of GPT-driven multiagent systems in IoT. Structurally, the paper outlines the incorporation of GPT into the agent-driven Framework for the Internet of Things (FIoT), introduces our proposed GPTin-the-loop approach, presents comparative results in the IoT context, and concludes with insights and future directions.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,Computer Science - Neural and Evolutionary Computing,Computer Science - Software Engineering},
  file = {/Users/nobr/Zotero/storage/JDYG774E/Nascimento et al. - 2023 - GPT-in-the-Loop Adaptive Decision-Making for Multiagent Systems.pdf}
}

@misc{nascimento2023a,
  title = {Self-{{Adaptive Large Language Model}} ({{LLM}})-{{Based Multiagent Systems}}},
  author = {Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
  year = {2023},
  month = jul,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {In autonomic computing, self-adaptation has been proposed as a fundamental paradigm to manage the complexity of multiagent systems (MASs). This achieved by extending a system with support to monitor and adapt itself to achieve specific concerns of interest. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, improving the expressiveness of the interaction communication with MASs is not without challenges. In this sense, the interplay between self-adaptive systems and effective communication is crucial for future MAS advancements. In this paper, we propose the integration of large language models (LLMs) such as GPTbased technologies into multiagent systems. We anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynamic environments. We also present a practical illustration of the proposed approach, in which we implement and assess a basic MAS-based application. The approach significantly advances the state-of-the-art of selfadaptive systems by proposing a new paradigm for MAS selfadaptation of autonomous systems based on LLM capabilities.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/M5SKLBUT/Nascimento et al. - 2023 - Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems.pdf}
}

@article{naselaris2009,
  title = {Bayesian {{Reconstruction}} of {{Natural Images}} from {{Human Brain Activity}}},
  author = {Naselaris, Thomas and Prenger, Ryan J. and Kay, Kendrick N. and Oliver, Michael and Gallant, Jack L.},
  year = {2009},
  month = sep,
  journal = {Neuron},
  volume = {63},
  number = {6},
  pages = {902--915},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.09.006},
  urldate = {2023-05-13},
  abstract = {Recent studies have used fMRI signals from early visual areas to reconstruct simple geometric patterns. Here, we demonstrate a new Bayesian decoder that uses fMRI signals from early and anterior visual areas to reconstruct complex natural images. Our decoder combines three elements: a structural encoding model that characterizes responses in early visual areas, a semantic encoding model that characterizes responses in anterior visual areas, and prior information about the structure and semantic content of natural images. By combining all these elements, the decoder produces reconstructions that accurately reflect both the spatial structure and semantic category of the objects contained in the observed natural image. Our results show that prior information has a substantial effect on the quality of natural image reconstructions. We also demonstrate that much of~the variance in the responses of anterior visual areas to complex natural images is explained by the semantic category of the image alone.},
  langid = {english},
  keywords = {SYSNEURO},
  file = {/Users/nobr/Zotero/storage/AEBNM5JU/Naselaris et al. - 2009 - Bayesian Reconstruction of Natural Images from Hum.pdf;/Users/nobr/Zotero/storage/TT5NCVFD/S0896627309006850.html}
}

@article{naveed2023,
  title = {A {{Comprehensive Overview}} of {{Large Language Models}}},
  author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  year = {2023},
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our selfcontained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/QWENX93E/Naveed et al. - A Comprehensive Overview of Large Language Models.pdf}
}

@article{nemirovsky2023,
  title = {An Implementation of Integrated Information Theory in Resting-State {{fMRI}}},
  author = {Nemirovsky, Idan E. and Popiel, Nicholas J. M. and Rudas, Jorge and Caius, Matthew and Naci, Lorina and Schiff, Nicholas D. and Owen, Adrian M. and Soddu, Andrea},
  year = {2023},
  month = jul,
  journal = {Communications Biology},
  volume = {6},
  number = {1},
  pages = {692},
  issn = {2399-3642},
  doi = {10.1038/s42003-023-05063-y},
  urldate = {2023-08-21},
  abstract = {Abstract                            Integrated Information Theory was developed to explain and quantify consciousness, arguing that conscious systems consist of elements that are integrated through their causal properties. This study presents an implementation of Integrated Information Theory 3.0, the latest version of this framework, to functional MRI data. Data were acquired from 17 healthy subjects who underwent sedation with propofol, a short-acting anaesthetic. Using the PyPhi software package, we systematically analyze how {$\Phi$}               max               , a measure of integrated information, is modulated by the sedative in different resting-state networks. We compare {$\Phi$}               max               to other proposed measures of conscious level, including the previous version of integrated information, Granger causality, and correlation-based functional connectivity. Our results indicate that {$\Phi$}               max               presents a variety of sedative-induced behaviours for different networks. Notably, changes to {$\Phi$}               max               closely reflect changes to subjects' conscious level in the frontoparietal and dorsal attention networks, which are responsible for higher-order cognitive functions. In conclusion, our findings present important insight into different measures of conscious level that will be useful in future implementations to functional MRI and other forms of neuroimaging.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IEZ38II2/Nemirovsky et al. - 2023 - An implementation of integrated information theory.pdf}
}

@inproceedings{nerella2021,
  title = {{{GraphEDM}}: {{A Graph-Based Approach}} to {{Disambiguate Entities}} in {{Microposts}}},
  shorttitle = {{{GraphEDM}}},
  booktitle = {2021 8th {{Swiss Conference}} on {{Data Science}} ({{SDS}})},
  author = {Nerella, Prathyusha and Bhardwaj, Akansha and Rosso, Paolo and {Cudre-Mauroux}, Philippe},
  year = {2021},
  month = jun,
  pages = {20--25},
  publisher = {IEEE},
  address = {Lucerne, Switzerland},
  doi = {10.1109/SDS51136.2021.00011},
  urldate = {2023-04-25},
  abstract = {The use of microblogging platforms such as Twitter has been growing rapidly. With about 500M tweets published per day, tweets are becoming a valuable source of information for several tasks such as event detection, sentiment analysis, or opinion mining, and are being leveraged by many prominent organizations. However, one must first be able to correctly capture the semantic content of a tweet prior to leveraging it for any automated analysis. Automatically understanding tweets is extremely challenging, as the information they contain is limited and insufficient for algorithms that need a larger context. In this work, we propose an approach that extends the context of a micropost by leveraging graph-based algorithms to further disambiguate the entities present in it. Our approach, GraphEDM, is divided into two phases. First, we use unsupervised clustering approaches to regroup tweets in semantic neighborhoods using embedding approaches. Next, each ambiguous entity in a cluster is iteratively disambiguated by leveraging a graph-based algorithm. Our experimental results reveal that GraphEDM outperforms the state of the art in tweet entity disambiguation by up to 15.13\% on several gold standard datasets.},
  isbn = {978-1-6654-3874-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7B2JLTQP/Nerella et al. - 2021 - GraphEDM A Graph-Based Approach to Disambiguate E.pdf}
}

@book{newman2010,
  title = {Networks: An Introduction},
  shorttitle = {Networks},
  author = {Newman, M. E. J.},
  year = {2010},
  publisher = {Oxford University Press},
  address = {Oxford ; New York},
  abstract = {"The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks. The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together for the first time the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks"--},
  isbn = {978-0-19-920665-0},
  langid = {english},
  lccn = {T57.85 .N523 2010},
  keywords = {Engineering systems,Network analysis (Planning),Social systems,System analysis,Systems biology},
  annotation = {OCLC: ocn456837194},
  file = {/Users/nobr/Zotero/storage/TRAJU3CX/Newman - 2010 - Networks an introduction.pdf}
}

@inproceedings{nguyen2015,
  title = {Deep Neural Networks Are Easily Fooled: {{High}} Confidence Predictions for Unrecognizable Images},
  shorttitle = {Deep Neural Networks Are Easily Fooled},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  year = {2015},
  month = jun,
  pages = {427--436},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298640},
  urldate = {2024-06-16},
  isbn = {978-1-4673-6964-0},
  file = {/Users/nobr/Zotero/storage/T687AP47/Nguyen et al. - 2015 - Deep neural networks are easily fooled High confidence predictions for unrecognizable images.pdf}
}

@misc{nickelPoincareEmbeddingsLearning2017,
  title = {Poincar{\textbackslash}'e {{Embeddings}} for {{Learning Hierarchical Representations}}},
  author = {Nickel, Maximilian and Kiela, Douwe},
  year = {2017},
  month = may,
  number = {arXiv:1705.08039},
  eprint = {1705.08039},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-09},
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar{\textbackslash}'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar{\textbackslash}'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/SCUW97AG/Nickel and Kiela - 2017 - Poincar'e Embeddings for Learning Hierarchical Representations.pdf;/Users/nobr/Zotero/storage/9T45U9S5/1705.html}
}

@book{nickoloff2016,
  title = {Docker in Action},
  author = {Nickoloff, Jeff},
  year = {2016},
  publisher = {Manning},
  address = {Shelter Island, NY},
  abstract = {Welcome to Docker -- Running software in containers -- Software installation simplified -- Persistent storage and shared state with volumes -- Network exposure -- Limiting risk with isolation -- Packaging software in images -- Build automation and advanced image considerations -- Public and private software distribution -- Running customized registries -- Declarative environments with Docker Compose -- Clusters with Machine and Swarm},
  isbn = {978-1-63343-023-5},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ICYUZ2ND/Nickoloff - 2016 - Docker in action.pdf}
}

@article{nicolau2017,
  title = {Evolutionary {{Behavior Tree Approaches}} for {{Navigating Platform Games}}},
  author = {Nicolau, Miguel and {Perez-Liebana}, Diego and O'Neill, Michael and Brabazon, Anthony},
  year = {2017},
  month = sep,
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {9},
  number = {3},
  pages = {227--238},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2016.2543661},
  urldate = {2024-01-14},
  abstract = {Computer games are highly dynamic environments, where players are faced with a multitude of potentially unseen scenarios. In this article, AI controllers are applied to the Mario AI Benchmark platform, by using the Grammatical Evolution system to evolve Behavior Tree structures. These controllers are either evolved to both deal with navigation and reactiveness to elements of the game, or used in conjunction with a dynamic A* approach. The results obtained highlight the applicability of Behavior Trees as representations for evolutionary computation, and their flexibility for incorporation of diverse algorithms to deal with specific aspects of bot control in game environments.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XY6NDG5B/Nicolau et al. - 2017 - Evolutionary Behavior Tree Approaches for Navigating Platform Games.pdf}
}

@article{nishimoto2011,
  title = {Reconstructing {{Visual Experiences}} from {{Brain Activity Evoked}} by {{Natural Movies}}},
  author = {Nishimoto, Shinji and Vu, An T. and Naselaris, Thomas and Benjamini, Yuval and Yu, Bin and Gallant, Jack L.},
  year = {2011},
  month = oct,
  journal = {Current Biology},
  volume = {21},
  number = {19},
  pages = {1641--1646},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2011.08.031},
  urldate = {2023-05-08},
  abstract = {Quantitative modeling of human brain activity can provide crucial insights about cortical representations [1, 2] and can form the basis for brain decoding devices [3, 4, 5]. Recent functional magnetic resonance imaging (fMRI) studies have modeled brain activity elicited by static visual patterns and have reconstructed these patterns from brain activity [6, 7, 8]. However, blood oxygen level-dependent (BOLD) signals measured via fMRI are very slow [9], so it has been difficult to model brain activity elicited by dynamic stimuli such as natural movies. Here we present a new motion-energy [10, 11] encoding model that largely overcomes this limitation. The model describes fast visual information and slow hemodynamics by separate components. We recorded BOLD signals in occipitotemporal visual cortex of human subjects who watched natural movies and fit the model separately to~individual voxels. Visualization of the fit models reveals how early visual areas represent the information in movies. To demonstrate the power of our approach, we also constructed a Bayesian decoder [8] by combining estimated encoding models with a sampled natural movie prior. The decoder provides remarkable reconstructions of the viewed movies. These results demonstrate that dynamic brain activity measured under naturalistic conditions can be decoded using current fMRI technology.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/CSRWTXJY/Nishimoto et al. - 2011 - Reconstructing Visual Experiences from Brain Activ.pdf;/Users/nobr/Zotero/storage/X6FZSDTC/S0960982211009377.html}
}

@article{niu2021,
  title = {The {{Development Tendency}} of {{Artificial Intelligence}} in {{Command}} and {{Control}}: {{A Brief Survey}}},
  shorttitle = {The {{Development Tendency}} of {{Artificial Intelligence}} in {{Command}} and {{Control}}},
  author = {Niu, Yukai and Jin, Xiaoxi and Li, Jinhui and Ji, Gang and Hu, Kai},
  year = {2021},
  month = apr,
  journal = {Journal of Physics: Conference Series},
  volume = {1883},
  number = {1},
  pages = {012152},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1883/1/012152},
  urldate = {2023-11-12},
  abstract = {Google's DeepMind's AlphaStar has achieved great success in the real-time strategy game StarCraft 2, and it has also made the combination of artificial intelligence and command and control, attracted widespread attention. Artificial intelligence theory and its engineering practice accumulated over many years have injected new development momentum into traditional fields, such as command and control. We download more than 900 high-level papers from Web of Science (WoS) that combine artificial intelligence and command and control as the research object, and employ bibliometric, CiteSpace, VOSviewer, and other bibliometric analysis software. From the number of documents, co-citing, co-occurrence of keywords and research hotspots, the four aspects are studied and visualized analysis, which clarifies the research status and key documents of artificial intelligence in command and control, and reveals the frontier hotspots and research directions in this field. Finally, a summary of the four visual analysis aspects points out important references for choosing research directions, exploring the frontiers of disciplines, and assisting scientific decision-making in this field.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PTUKMQLT/Niu et al. - 2021 - The Development Tendency of Artificial Intelligence in Command and Control A Brief Survey.pdf}
}

@misc{noaa_cooperative,
  title = {Cooperative Air Sampling Network},
  author = {{National Oceanic and Atmospheric Administration}},
  year = {1967}
}

@misc{NOAA_GWH_2019,
  title = {Global Warming and Hurricanes},
  author = {{NOAA Geophysical Fluid Dynamics Laboratory}},
  year = {2019}
}

@techreport{noaa2019global,
  title = {Global Climate Report - December 2019},
  author = {{NOAA National Centers for Environmental Information}},
  year = {2019},
  month = dec,
  institution = {{National Oceanic and Atmospheric Administration}}
}

@misc{nociShapedTransformerAttention2023,
  title = {The {{Shaped Transformer}}: {{Attention Models}} in the {{Infinite Depth-and-Width Limit}}},
  shorttitle = {The {{Shaped Transformer}}},
  author = {Noci, Lorenzo and Li, Chuning and Li, Mufan Bill and He, Bobby and Hofmann, Thomas and Maddison, Chris and Roy, Daniel M.},
  year = {2023},
  month = dec,
  number = {arXiv:2306.17759},
  eprint = {2306.17759},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.17759},
  urldate = {2024-02-12},
  abstract = {In deep learning theory, the covariance matrix of the representations serves as a proxy to examine the network's trainability. Motivated by the success of Transformers, we study the covariance matrix of a modified Softmax-based attention model with skip connections in the proportional limit of infinite-depth-and-width. We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio. To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter. We examine the stability of the network through the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly controlled with the aid of residual connections. The existence of a stable SDE implies that the covariance structure is well-behaved, even for very large depth and width, thus preventing the notorious issues of rank degeneracy in deep attention models. Finally, we show, through simulations, that the SDE provides a surprisingly good description of the corresponding finite-size model. We coin the name shaped Transformer for these architectural modifications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/G84HINP7/Noci et al. - 2023 - The Shaped Transformer Attention Models in the Infinite Depth-and-Width Limit.pdf;/Users/nobr/Zotero/storage/DQGXZ8MF/2306.html}
}

@misc{nolan2023,
  title = {Oppenheimer: {{The Official Screenplay}}},
  shorttitle = {Oppenheimer},
  author = {Nolan, Christopher},
  year = {2023},
  month = aug,
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HSHEPYPP/Oppenheimer.pdf}
}

@article{noorollahi2016,
  title = {Multi-Criteria Decision Support System for Wind Farm Site Selection Using {{GIS}}},
  author = {Noorollahi, Younes and Yousefi, Hossein and Mohammadi, Mohammad},
  year = {2016},
  month = feb,
  journal = {Sustainable Energy Technologies and Assessments},
  volume = {13},
  pages = {38--50},
  issn = {22131388},
  doi = {10.1016/j.seta.2015.11.007},
  urldate = {2023-07-13},
  abstract = {The present study analyzed a multi-criteria decision support system to define wind energy resources in western Iran. Clean domestic renewable energy can be the best option in consideration of intense economic development and its accompanying increase in energy consumption. The most important barrier to wider deployment of renewable resources in Iran is the price of fossil fuels, which is the lowest in the world. The government has recently decided to remove subsidies for fossil fuel, meaning that its price will increase and will make the cost of green energies more attractive.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/K226AL8N/Noorollahi et al. - 2016 - Multi-criteria decision support system for wind fa.pdf}
}

@article{oates1996,
  title = {Learning {{Planning Operators}} with {{Conditional}} and {{Probabilistic Effects}}},
  author = {Oates, Tim and Cohen, Paul R},
  year = {1996},
  abstract = {Providing a complete and accurate domain model for an agent situated in a complex environment can be an extremely difficult task. Actions may have different effects depending on the context in which they are taken, and actions mayor may not induce their intended effects, with the probability of success again depending on context. In addition, the contexts and probabilities that govern the effects and success of actions may change over time. We present an algorithm for automatically learning planning operators with context-dependent and probabilistic effects in environments where exogenous events change the state of the world. Our approach assumes that a situated agent has knowledge of the types of actions that it can take, but initially knows nothing of the contexts in which an action produces change in the environment, nor what that change is likely to be. The algorithm accepts as input a history of state descriptions observed by an agent while taking actions in its domain, and produces as output descriptions of planning operators that capture structure in the agent's interactions with its environment. We present results for a sample domain showing that the computational requirements of our algorithm scale approximately linearly with the size of the agent's state vector, and that the algorithm successfully locates operators that capture true structure and avoids those that incorporate noise.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BUVZKWRE/Oates and Cohen - Learning Planning Operators with Conditional and Probabilistic Effects.pdf}
}

@misc{observatorio_clima2017,
  title = {Emissions from Brazil Rise 9\% in 2016},
  year = {2017},
  month = oct
}

@article{oh2017,
  title = {Playing Real-Time Strategy Games by Imitating Human Players' Micromanagement Skills Based on Spatial Analysis},
  author = {Oh, In-Seok and Cho, Hochul and Kim, Kyung-Joong},
  year = {2017},
  month = apr,
  journal = {Expert Systems with Applications},
  volume = {71},
  pages = {192--205},
  issn = {09574174},
  doi = {10.1016/j.eswa.2016.11.026},
  urldate = {2023-11-12},
  langid = {english}
}

@article{oh2025,
  title = {Evaluating {{Mathematical Problem-Solving Abilities}} of {{Generative AI Models}}: {{Performance Analysis}} of O1-Preview and Gpt-4o {{Using}} the {{Korean College Scholastic Ability Test}}},
  shorttitle = {Evaluating {{Mathematical Problem-Solving Abilities}} of {{Generative AI Models}}},
  author = {Oh, Sejun},
  year = {2025},
  journal = {IEEE Access},
  volume = {13},
  pages = {1227--1235},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3523703},
  urldate = {2025-03-03},
  abstract = {This study utilized the Korean College Scholastic Ability Test questions to evaluate the mathematical problem-solving abilities of the latest Generative AI models, o1-preview and gpt-4o. The performance of the AI models was analyzed using 92 questions from the mathematics sections of the 2023 and 2024 tests and compared with the performance of human learners. The results showed that the o1-preview model achieved an average accuracy rate of 81.52\%, performing at a level comparable to top-tier human learners. The gpt-4o model demonstrated mid to lower-tier performance with an average accuracy rate of 49.46\%. When analyzing different problem types, this study found that both models did better on multiple-choice questions, but their accuracy decreased as the problems got harder. AI uses reasoning processes similar to those of humans when solving mathematical problems. This study is significant because it offers new insights into AI's mathematical abilities and shows the potential for using AI in education.},
  keywords = {Accuracy,AI educational applications,Analytical models,Artificial intelligence,Chatbots,Cognition,college scholastic ability test,Data models,Education,Generative AI,Mathematical models,mathematical problem-solving,o1-preview,Problem-solving},
  file = {/Users/nobr/Zotero/storage/6FMDC3ZH/Oh - 2025 - Evaluating Mathematical Problem-Solving Abilities of Generative AI Models Performance Analysis of o.pdf}
}

@article{olah2017,
  title = {Feature {{Visualization}}},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  year = {2017},
  month = nov,
  journal = {Distill},
  volume = {2},
  number = {11},
  pages = {10.23915/distill.00007},
  issn = {2476-0757},
  doi = {10.23915/distill.00007},
  urldate = {2024-06-16}
}

@article{olah2018,
  title = {The {{Building Blocks}} of {{Interpretability}}},
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  year = {2018},
  month = mar,
  journal = {Distill},
  volume = {3},
  number = {3},
  pages = {10.23915/distill.00010},
  issn = {2476-0757},
  doi = {10.23915/distill.00010},
  urldate = {2024-06-16}
}

@misc{olin-ammentorpDeepPhasorNetworks2021,
  title = {Deep {{Phasor Networks}}: {{Connecting Conventional}} and {{Spiking Neural Networks}}},
  shorttitle = {Deep {{Phasor Networks}}},
  author = {{Olin-Ammentorp}, Wilkie and Bazhenov, Maxim},
  year = {2021},
  month = sep,
  number = {arXiv:2106.11908},
  eprint = {2106.11908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.11908},
  urldate = {2024-02-02},
  abstract = {In this work, we extend standard neural networks by building upon an assumption that neuronal activations correspond to the angle of a complex number lying on the unit circle, or 'phasor.' Each layer in such a network produces new activations by taking a weighted superposition of the previous layer's phases and calculating the new phase value. This generalized architecture allows models to reach high accuracy and carries the singular advantage that mathematically equivalent versions of the network can be executed with or without regard to a temporal variable. Importantly, the value of a phase angle in the temporal domain can be sparsely represented by a periodically repeating series of delta functions or 'spikes'. We demonstrate the atemporal training of a phasor network on standard deep learning tasks and show that these networks can then be executed in either the traditional atemporal domain or spiking temporal domain with no conversion step needed. This provides a novel basis for constructing deep networkswhich operate via temporal, spike-based calculations suitable for neuromorphic computing hardware.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/YR3VKVKG/Olin-Ammentorp and Bazhenov - 2021 - Deep Phasor Networks Connecting Conventional and Spiking Neural Networks.pdf;/Users/nobr/Zotero/storage/D2XVSZTW/2106.html}
}

@inbook{onnes1991,
  title = {Further Experiments with {{Liquid Helium}}. {{G}}. {{On}} the {{Electrical Resistance}} of {{Pure Metals}}, Etc. {{VI}}. {{On}} the {{Sudden Change}} in the {{Rate}} at Which the {{Resistance}} of {{Mercury Disappears}}.},
  booktitle = {Through {{Measurement}} to {{Knowledge}}},
  author = {Onnes, H. Kamerlingh},
  year = {1991},
  volume = {124},
  pages = {267--272},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-009-2079-8_17},
  urldate = {2025-02-16},
  collaborator = {Onnes, Heike Kamerlingh},
  isbn = {978-94-010-7433-9 978-94-009-2079-8},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/A5ZJ4DIM/Onnes - 1991 - Further experiments with Liquid Helium. G. On the Electrical Resistance of Pure Metals, etc. VI. On.pdf}
}

@article{ontanon2013,
  title = {The {{Combinatorial Multi-Armed Bandit Problem}} and {{Its Application}} to {{Real-Time Strategy Games}}},
  author = {Ontanon, Santiago},
  year = {2013},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume = {9},
  number = {1},
  pages = {58--64},
  issn = {2334-0924, 2326-909X},
  doi = {10.1609/aiide.v9i1.12681},
  urldate = {2024-01-14},
  abstract = {Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Na{\textasciidieresis}{\i}ve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS algorithm based on Na{\textasciidieresis}{\i}ve Sampling called Na{\textasciidieresis}{\i}veMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, Na{\i}{\textasciidieresis}veMCTS performs significantly better than other algorithms.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/6PUAMQYV/Ontanon - 2021 - The Combinatorial Multi-Armed Bandit Problem and Its Application to Real-Time Strategy Games.pdf}
}

@article{ontanon2018,
  title = {The {{First microRTS Artificial Intelligence Competition}}},
  author = {Onta{\~n}{\'o}n, Santiago and Barriga, Nicolas A. and Silva, Cleyton R. and Moraes, Rubens O. and Lelis, Levi H. S.},
  year = {2018},
  month = mar,
  journal = {AI Magazine},
  volume = {39},
  number = {1},
  pages = {75--83},
  issn = {2371-9621},
  doi = {10.1609/aimag.v39i1.2777},
  urldate = {2024-01-14},
  abstract = {This article presents the results of the first edition of the microRTS ({$\mu$}RTS) AI competition, which was hosted by the IEEE Computational Intelligence in Games (CIG) 2017 conference. The goal of the competition is to spur research on AI techniques for real-time strategy (RTS) games. In this first edition, the competition received three submissions, focusing on address- ing problems such as balancing long-term and short-term search, the use of machine learning to learn how to play against certain opponents, and finally, dealing with partial observability in RTS games.},
  copyright = {Copyright (c) 2018 AI Magazine},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RABXWRS8/Ontañón et al. - 2018 - The First microRTS Artificial Intelligence Competition.pdf}
}

@book{oosterlee2020,
  title = {Mathematical Modeling and Computation in Finance: With Exercises and {{Python}} and {{Matlab}} Computer Codes},
  shorttitle = {Mathematical Modeling and Computation in Finance},
  author = {Oosterlee, Cornelis Willebrordus and Grzelak, Lech A.},
  year = {2020},
  publisher = {World Scientific},
  address = {New Jersey London Singapore Beijing Shanghai Hong Kong Taipei Chennai Tokyo},
  isbn = {978-1-78634-794-7 978-1-78634-805-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LZWJKHTM/Oosterlee and Grzelak - 2020 - Mathematical modeling and computation in finance .pdf}
}

@book{oppenheim1999,
  title = {Discrete-Time Signal Processing},
  author = {Oppenheim, Alan V. and Schafer, Ronald W. and Buck, John R.},
  year = {1999},
  edition = {2nd ed},
  publisher = {Prentice Hall},
  address = {Upper Saddle River, N.J},
  isbn = {978-0-13-754920-7},
  lccn = {TK5102.9 .O67 1999},
  keywords = {Discrete-time systems,Mathematics,Signal processing},
  file = {/Users/nobr/Zotero/storage/WMG7429E/Discrete-Time Digital Signal Processing - Oppenheim, Schafer & Buck.pdf}
}

@techreport{oppenheimer2014emergent,
  title = {Emergent Risks and Key Vulnerabilities},
  author = {Oppenheimer, M. and Campos, M. and Warren, R. and Birkmann, J. and Luber, G. and O'Neill, B. and Takahashi, K.},
  year = {2014},
  journal = {Climate change 2014: Impacts, adaptation, and vulnerability. Part a: Global and sectoral aspects. Contribution of working group II to the fifth assessment report of the intergovernmental panel on climate change},
  pages = {1039--1099},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  institution = {Cambridge University Press}
}

@article{orbach,
  title = {Debunking the {{Genocide Allegations}}: {{A Reexamination}} of the {{Israel-Hamas War}} from {{October}} 7, 2023 to {{June}} 1, 2025},
  author = {Orbach, Danny and Boxman, Jonathan and Henkin, Yagil and Braverman, Jonathan},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PZZRLEI7/Orbach et al. - Debunking the Genocide Allegations A Reexamination of the Israel-Hamas War from October 7, 2023 to.pdf}
}

@misc{ordonez2025,
  title = {Embedding-{{Aware Quantum-Classical SVMs}} for {{Scalable Quantum Machine Learning}}},
  author = {Ord{\'o}{\~n}ez, Sebasti{\'a}n Andr{\'e}s Cajas and Torres, Luis Fernando Torres and Bifulco, Mario and Dur{\'a}n, Carlos Andr{\'e}s and Bosch, Cristian and Carbajo, Ricardo Sim{\'o}n},
  year = {2025},
  month = jul,
  number = {arXiv:2508.00024},
  eprint = {2508.00024},
  primaryclass = {quant-ph},
  doi = {10.48550/arXiv.2508.00024},
  urldate = {2025-08-06},
  abstract = {Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an embedding-aware quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings. Our key finding: ViT embeddings uniquely enable quantum advantage, achieving up to 8.02\% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42\% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces. This provides a practical pathway for scalable quantum machine learning that leverages modern neural architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantum Physics},
  file = {/Users/nobr/Zotero/storage/DZ3W2NF2/Ordóñez et al. - 2025 - Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning.pdf}
}

@article{oreilly2024,
  title = {The {{Evolution}} of {{Warfare}} and {{Weapons}} in {{Japan}}, 792--1392},
  author = {O'Reilly, Sean},
  year = {2024},
  month = jan,
  journal = {Histories},
  volume = {4},
  number = {1},
  pages = {24--37},
  issn = {2409-9252},
  doi = {10.3390/histories4010002},
  urldate = {2024-01-24},
  abstract = {The fearsome Japanese samurai, a legendary figure whose primary attribute was loyalty or honor, needs no introduction. He is strongly associated with the equally famous katana. The popular image of the samurai probably would appear wearing armor but certainly does not carry a shield. This figure, many assume, must have dominated medieval Japan. Yet is this samurai image accurate? Can it withstand sustained scrutiny? What was Japanese warfare really like 1000 years ago? In this article, I evaluate the key sources on medieval warfare in Japan, identifying the contributions of each and pointing out some methodological problems they face. The most prominent casualty of this synthetic analysis is the pop culture image of the heroic and honorable sword-wielding samurai.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/CLR6PVB7/O’Reilly - 2024 - The Evolution of Warfare and Weapons in Japan, 792.pdf}
}

@article{orrell2017,
  title = {The Impact of Individual {{Cognitive Stimulation Therapy}} ({{iCST}}) on Cognition, Quality of Life, Caregiver Health, and Family Relationships in Dementia: {{A}} Randomised Controlled Trial},
  shorttitle = {The Impact of Individual {{Cognitive Stimulation Therapy}} ({{iCST}}) on Cognition, Quality of Life, Caregiver Health, and Family Relationships in Dementia},
  author = {Orrell, Martin and Yates, Lauren and Leung, Phuong and Kang, Sujin and Hoare, Zoe and Whitaker, Chris and Burns, Alistair and Knapp, Martin and Leroi, Iracema and {Moniz-Cook}, Esme and Pearson, Stephen and Simpson, Stephen and Spector, Aimee and Roberts, Steven and Russell, Ian and De Waal, Hugo and Woods, Robert T. and Orgeta, Vasiliki},
  editor = {Brayne, Carol},
  year = {2017},
  month = mar,
  journal = {PLOS Medicine},
  volume = {14},
  number = {3},
  pages = {e1002269},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1002269},
  urldate = {2023-10-20},
  abstract = {Background Cognitive stimulation therapy (CST) is a well-established group psychosocial intervention for people with dementia. There is evidence that home-based programmes of cognitive stimulation delivered by family caregivers may benefit both the person and the caregiver. However, no previous studies have evaluated caregiver-delivered CST. This study aimed to evaluate the effectiveness of a home-based, caregiver-led individual cognitive stimulation therapy (iCST) program in (i) improving cognition and quality of life (QoL) for the person with dementia and (ii) mental and physical health (well-being) for the caregiver.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2UMCDXAB/Orrell et al. - 2017 - The impact of individual Cognitive Stimulation Therapy (iCST) on cognition, quality of life, caregiv.pdf}
}

@article{osinenko2022,
  title = {Reinforcement Learning with Guarantees: A Review},
  shorttitle = {Reinforcement Learning with Guarantees},
  author = {Osinenko, Pavel and Dobriborsci, Dmitrii and Aumer, Wolfgang},
  year = {2022},
  journal = {IFAC-PapersOnLine},
  volume = {55},
  number = {15},
  pages = {123--128},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2022.07.619},
  urldate = {2023-11-12},
  langid = {english}
}

@article{ouessai2022,
  title = {Evolving Action Pre-Selection Parameters for {{MCTS}} in Real-Time Strategy Games},
  author = {Ouessai, Abdessamed and Salem, Mohammed and Mora, Antonio M.},
  year = {2022},
  month = may,
  journal = {Entertainment Computing},
  volume = {42},
  pages = {100493},
  issn = {18759521},
  doi = {10.1016/j.entcom.2022.100493},
  urldate = {2023-11-12},
  abstract = {Real-Time Strategy (RTS) games are well-known for their substantially large combinatorial decision and state spaces, responsible for creating significant challenges for search and machine learning techniques. Exploiting domain knowledge to assist in navigating the expansive decision and state spaces could facilitate the emergence of competitive RTS game-playing agents. Usually, domain knowledge can take the form of expert traces or expert-authored scripts. A script encodes a strategy conceived by a human expert and can be used to steer a search algorithm, such as Monte Carlo Tree Search (MCTS), towards high-value states. However, a script is coarse by nature, meaning that it could be subject to exploitation and poor low-level tactical performance. We propose to perceive scripts as a collection of heuristics that can be parameterized and combined to form a wide array of strategies. The parameterized heuristics mold and filter the decision space in favor of a strategy expressed in terms of parameters. The proposed agent, ParaMCTS, implements several common heuristics and uses Na{\"i}veMCTS to search the downsized decision space; however, it requires a preceding manual parameterization step. A genetic algorithm is proposed for use in an optimization phase that aims to replace manual tuning and find an optimal set of parameters for use by EvoPMCTS, the evolutionary counterpart of ParaMCTS. Experi\- mentation results using the {$\mu$}RTS testbed show that EvoPMCTS outperforms several state-of-the-art agents across multiple maps of distinct layouts.},
  langid = {english}
}

@article{ouyang2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2203.02155},
  urldate = {2023-12-31},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/nobr/Zotero/storage/LKKTAH93/Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf}
}

@article{ouyang2025,
  title = {An {{Empirical Study}} of the {{Non-Determinism}} of {{ChatGPT}} in {{Code Generation}}},
  author = {Ouyang, Shuyin and Zhang, Jie M. and Harman, Mark and Wang, Meng},
  year = {2025},
  month = jan,
  journal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {34},
  number = {2},
  pages = {42:1--42:28},
  issn = {1049-331X},
  doi = {10.1145/3697010},
  urldate = {2025-03-03},
  abstract = {There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; non-deterministically returning very different code for the same prompt. Such non-determinism affects the correctness and consistency of the generated code, undermines developers' trust in LLMs, and yields low reproducibility in LLM-based papers. Nevertheless, there is no work investigating how serious this non-determinism threat is.To fill this gap, this article conducts an empirical study on the non-determinism of ChatGPT in code generation. We chose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems across three code generation benchmarks (i.e., CodeContests, APPS and HumanEval) with three aspects of code similarities: semantic similarity, syntactic similarity, and structural similarity. Our results reveal that ChatGPT exhibits a high degree of non-determinism under the default setting: the ratio of coding tasks with zero equal test output across different requests is 75.76\%, 51.00\% and 47.56\% for three different code generation datasets (i.e., CodeContests, APPS and HumanEval), respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature  {\textbackslash}(={\textbackslash})  1). In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.},
  file = {/Users/nobr/Zotero/storage/7WG6WFGR/Ouyang et al. - 2025 - An Empirical Study of the Non-Determinism of ChatGPT in Code Generation.pdf}
}

@article{owen1997,
  title = {The Origins of 60-{{Hz}} as a Power Frequency},
  author = {Owen, E.L.},
  year = {1997},
  month = nov,
  journal = {IEEE Industry Applications Magazine},
  volume = {3},
  number = {6},
  pages = {8--14},
  issn = {1077-2618, 1558-0598},
  doi = {10.1109/2943.628099},
  urldate = {2025-03-03},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LJPM7XJK/Owen - 1997 - The origins of 60-Hz as a power frequency.pdf}
}

@inproceedings{oxman2007,
  title = {Material {{Based Design Computation}}},
  booktitle = {{{CAADRIA}} 2007: {{Digitization}} and {{Globalization}}},
  author = {Oxman, Neri and Rosenberg, Jesse L.},
  year = {2007},
  pages = {XX-XX},
  address = {Nanjing, China},
  doi = {10.52842/conf.caadria.2007.x.d2j},
  urldate = {2023-12-29},
  abstract = {The institutionalized separation between form, structure and material, deeply embedded in modernist design theory, paralleled by a methodological partitioning between modeling, analysis and fabrication, resulted in geometric-driven form generation. Such prioritization of form over material was carried into the development and design logic of CAD. Today, under the imperatives and growing recognition of the failures and environmental liabilities of this approach, modern design culture is experiencing a shift to material aware design.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/nobr/Zotero/storage/6BFWM3G4/Oxman and Rosenberg - 2007 - Material Based Design Computation.pdf}
}

@misc{ozcelik2022,
  title = {Reconstruction of {{Perceived Images}} from {{fMRI Patterns}} and {{Semantic Brain Exploration}} Using {{Instance-Conditioned GANs}}},
  author = {Ozcelik, Furkan and Choksi, Bhavin and Mozafari, Milad and Reddy, Leila and VanRullen, Rufin},
  year = {2022},
  month = feb,
  number = {arXiv:2202.12692},
  eprint = {2202.12692},
  primaryclass = {cs, eess, q-bio},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {Reconstructing perceived natural images from fMRI signals is one of the most engaging topics of neural decoding research. Prior studies had success in reconstructing either the low-level image features or the semantic/high-level aspects, but rarely both. In this study, we utilized an Instance-Conditioned GAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate semantic attributes and preserved low-level details. The IC-GAN model takes as input a 119-dim noise vector and a 2048-dim instance feature vector extracted from a target image via a self-supervised learning model (SwAV ResNet-50); these instance features act as a conditioning for IC-GAN image generation, while the noise vector introduces variability between samples. We trained ridge regression models to predict instance features, noise vectors, and dense vectors (the output of the first dense layer of the IC-GAN generator) of stimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to reconstruct novel test images based on these fMRI-predicted variables. The generated images presented state-of-the-art results in terms of capturing the semantic attributes of the original test images while remaining relatively faithful to low-level image details. Finally, we use the learned regression model and the IC-GAN generator to systematically explore and visualize the semantic features that maximally drive each of several regions-of-interest in the human brain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/3TI6T6SH/Ozcelik et al. - 2022 - Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-.pdf;/Users/nobr/Zotero/storage/JUEZLDJW/Ozcelik et al. - 2022 - Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-.pdf;/Users/nobr/Zotero/storage/4IHE4V2J/2202.html}
}

@misc{ozcelik2023,
  title = {Brain-{{Diffuser}}: {{Natural}} Scene Reconstruction from {{fMRI}} Signals Using Generative Latent Diffusion},
  shorttitle = {Brain-{{Diffuser}}},
  author = {Ozcelik, Furkan and VanRullen, Rufin},
  year = {2023},
  month = mar,
  number = {arXiv:2303.05334},
  eprint = {2303.05334},
  primaryclass = {cs, q-bio},
  doi = {10.48550/arXiv.2303.05334},
  urldate = {2023-05-19},
  abstract = {In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called ``Brain-Diffuser''. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second stage, we use the image-to-image framework of a latent diffusion model (Versatile Diffusion) conditioned on predicted multimodal (text and visual) features, to generate final reconstructed images. On the publicly available Natural Scenes Dataset benchmark, our method outperforms previous models both qualitatively and quantitatively. When applied to synthetic fMRI patterns generated from individual ROI (region-of-interest) masks, our trained model creates compelling ``ROI-optimal'' scenes consistent with neuroscientific knowledge. Thus, the proposed methodology can have an impact on both applied (e.g. brain-computer interface) and fundamental neuroscience.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/WIACLV3S/Ozcelik and VanRullen - 2023 - Brain-Diffuser Natural scene reconstruction from fMRI signals using generative latent diffusion.pdf}
}

@book{paar2010,
  title = {Understanding Cryptography: A Textbook for Students and Practitioners},
  shorttitle = {Understanding Cryptography},
  author = {Paar, Christof and Pelzl, Jan},
  year = {2010},
  publisher = {Springer},
  address = {Heidelberg ; New York},
  isbn = {978-3-642-04100-6 978-3-642-04101-3},
  langid = {english},
  lccn = {Z104 .P33 2010},
  keywords = {Cryptography,Data encryption (Computer science),Mathematics},
  annotation = {OCLC: ocn527339793},
  file = {/Users/nobr/Zotero/storage/WQ2ZSGB6/Paar and Pelzl - 2010 - Understanding cryptography a textbook for students and practitioners.pdf}
}

@article{padakandla2020,
  title = {A {{Survey}} of {{Reinforcement Learning Algorithms}} for {{Dynamically Varying Environments}}},
  author = {Padakandla, Sindhu},
  year = {2020},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  eprint = {2005.10619v1},
  publisher = {Association for Computing Machinery},
  doi = {10.1145/3459991},
  urldate = {2023-03-22},
  abstract = {Reinforcement learning (RL) algorithms find applications in inventory control, recommender systems, vehicular traffic management, cloud computing and robotics. The real-world complications of many tasks arising in these domains makes them difficult to solve with the basic assumptions underlying classical RL algorithms. RL agents in these applications often need to react and adapt to changing operating conditions. A significant part of research on single-agent RL techniques focuses on developing algorithms when the underlying assumption of stationary environment model is relaxed. This paper provides a survey of RL methods developed for handling dynamically varying environment models. The goal of methods not limited by the stationarity assumption is to help autonomous agents adapt to varying operating conditions. This is possible either by minimizing the rewards lost during learning by RL agent or by finding a suitable policy for the RL agent which leads to efficient operation of the underlying system. A representative collection of these algorithms is discussed in detail in this work along with their categorization and their relative merits and demerits. Additionally we also review works which are tailored to application domains. Finally, we discuss future enhancements for this field.},
  archiveprefix = {arXiv},
  keywords = {context detection,Markov decision processes,meta-learning,non-stationary environments,regret computation,Reinforcement learning,sequential decision-making},
  file = {/Users/nobr/Zotero/storage/LLTV7XLP/Padakandla - 2020 - A Survey of Reinforcement Learning Algorithms for .pdf}
}

@article{padgett1993,
  title = {Robust {{Action}} and the {{Rise}} of the {{Medici}}, 1400-1434},
  author = {Padgett, John F. and Ansell, Christopher K.},
  year = {1993},
  month = may,
  journal = {American Journal of Sociology},
  volume = {98},
  number = {6},
  pages = {1259--1319},
  issn = {0002-9602, 1537-5390},
  doi = {10.1086/230190},
  urldate = {2023-11-09},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5LGSSZEY/Padgett and Ansell - 1993 - Robust Action and the Rise of the Medici, 1400-1434.pdf}
}

@misc{paglieri2024,
  title = {{{BALROG}}: {{Benchmarking Agentic LLM}} and {{VLM Reasoning On Games}}},
  shorttitle = {{{BALROG}}},
  author = {Paglieri, Davide and Cupia{\l}, Bart{\l}omiej and Coward, Samuel and Piterbarg, Ulyana and Wolczyk, Maciej and Khan, Akbir and Pignatelli, Eduardo and Kuci{\'n}ski, {\L}ukasz and Pinto, Lerrel and Fergus, Rob and Foerster, Jakob Nicolaus and {Parker-Holder}, Jack and Rockt{\"a}schel, Tim},
  year = {2024},
  month = nov,
  number = {arXiv:2411.13543},
  eprint = {2411.13543},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.13543},
  urldate = {2024-12-03},
  abstract = {Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/XN8FEGC6/Paglieri et al. - 2024 - BALROG Benchmarking Agentic LLM and VLM Reasoning On Games.pdf;/Users/nobr/Zotero/storage/9LXSMFRS/2411.html}
}

@misc{pan2024,
  title = {{{AgentCoord}}: {{Visually Exploring Coordination Strategy}} for {{LLM-based Multi-Agent Collaboration}}},
  shorttitle = {{{AgentCoord}}},
  author = {Pan, Bo and Lu, Jiaying and Wang, Ke and Zheng, Li and Wen, Zhen and Feng, Yingchaojie and Zhu, Minfeng and Chen, Wei},
  year = {2024},
  month = apr,
  number = {arXiv:2404.11943},
  eprint = {2404.11943},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.11943},
  urldate = {2024-12-03},
  abstract = {The potential of automatic task-solving through Large Language Model (LLM)-based multi-agent collaboration has recently garnered widespread attention from both the research community and industry. While utilizing natural language to coordinate multiple agents presents a promising avenue for democratizing agent technology for general users, designing coordination strategies remains challenging with existing coordination frameworks. This difficulty stems from the inherent ambiguity of natural language for specifying the collaboration process and the significant cognitive effort required to extract crucial information (e.g. agent relationship, task dependency, result correspondence) from a vast amount of text-form content during exploration. In this work, we present a visual exploration framework to facilitate the design of coordination strategies in multi-agent collaboration. We first establish a structured representation for LLM-based multi-agent coordination strategy to regularize the ambiguity of natural language. Based on this structure, we devise a three-stage generation method that leverages LLMs to convert a user's general goal into an executable initial coordination strategy. Users can further intervene at any stage of the generation process, utilizing LLMs and a set of interactions to explore alternative strategies. Whenever a satisfactory strategy is identified, users can commence the collaboration and examine the visually enhanced execution result. We develop AgentCoord, a prototype interactive system, and conduct a formal user study to demonstrate the feasibility and effectiveness of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/nobr/Zotero/storage/QG6GSN7S/Pan et al. - 2024 - AgentCoord Visually Exploring Coordination Strategy for LLM-based Multi-Agent Collaboration.pdf;/Users/nobr/Zotero/storage/QUJ3GK78/2404.html}
}

@misc{panIntegrationSelfAttentionConvolution2022,
  title = {On the {{Integration}} of {{Self-Attention}} and {{Convolution}}},
  author = {Pan, Xuran and Ge, Chunjiang and Lu, Rui and Song, Shiji and Chen, Guanfu and Huang, Zeyi and Huang, Gao},
  year = {2022},
  month = mar,
  number = {arXiv:2111.14556},
  eprint = {2111.14556},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-16},
  abstract = {Convolution and self-attention are two powerful techniques for representation learning, and they are usually considered as two peer approaches that are distinct from each other. In this paper, we show that there exists a strong underlying relation between them, in the sense that the bulk of computations of these two paradigms are in fact done with the same operation. Specifically, we first show that a traditional convolution with kernel size k x k can be decomposed into k{\textasciicircum}2 individual 1x1 convolutions, followed by shift and summation operations. Then, we interpret the projections of queries, keys, and values in self-attention module as multiple 1x1 convolutions, followed by the computation of attention weights and aggregation of the values. Therefore, the first stage of both two modules comprises the similar operation. More importantly, the first stage contributes a dominant computation complexity (square of the channel size) comparing to the second stage. This observation naturally leads to an elegant integration of these two seemingly distinct paradigms, i.e., a mixed model that enjoys the benefit of both self-Attention and Convolution (ACmix), while having minimum computational overhead compared to the pure convolution or self-attention counterpart. Extensive experiments show that our model achieves consistently improved results over competitive baselines on image recognition and downstream tasks. Code and pre-trained models will be released at https://github.com/LeapLabTHU/ACmix and https://gitee.com/mindspore/models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/754Q5BPR/Pan et al. - 2022 - On the Integration of Self-Attention and Convolution.pdf;/Users/nobr/Zotero/storage/25HR6DVT/2111.html}
}

@article{pape2012,
  title = {Learning Tactile Skills through Curious Exploration},
  author = {Pape, Leo and Oddo, Calogero M. and Controzzi, Marco and Cipriani, Christian and F{\"o}rster, Alexander and Carrozza, Maria C. and Schmidhuber, J{\"u}rgen},
  year = {2012},
  journal = {Frontiers in Neurorobotics},
  number = {JULY},
  issn = {16625218},
  doi = {10.3389/FNBOT.2012.00006},
  urldate = {2023-03-15},
  abstract = {We present curiosity-driven, autonomous acquisition of tactile exploratory skills on a biomimetic robot finger equipped with an array of microelectromechanical touch sensors. Instead of building tailored algorithms for solving a specific tactile task, we employ a more general curiosity-driven reinforcement learning approach that autonomously learns a set of motor skills in absence of an explicit teacher signal. In this approach, the acquisition of skills is driven by the information content of the sensory input signals relative to a learner that aims at representing sensory inputs using fewer and fewer computational resources. We show that, from initially random exploration of its environment, the robotic system autonomously develops a small set of basic motor skills that lead to different kinds of tactile input. Next, the system learns how to exploit the learned motor skills to solve supervised texture classification tasks. Our approach demonstrates the feasibility of autonomous acquisition of tactile skills on physical robotic platforms through curiosity-driven reinforcement learning, overcomes typical difficulties of engineered solutions for active tactile exploration and underactuated control, and provides a basis for studying developmental learning through intrinsic motivation in robots. {\copyright} 2012 Pape, Oddo, Controzzi, Cipriani, F{\"o}rster, Carrozza and Schmidhuber.},
  pmid = {22837748},
  keywords = {Active learning,Biomimetic robotics,Curiosity,Intrinsic motivation,Reinforcement learning,Skill learning,Tactile sensing},
  file = {/Users/nobr/Zotero/storage/Q2KCUX6P/Pape et al. - 2012 - Learning tactile skills through curious exploration.pdf}
}

@article{papier2024a,
  title = {Identifying Proteomic Risk Factors for Cancer Using Prospective and Exome Analyses of 1463 Circulating Proteins and Risk of 19 Cancers in the {{UK Biobank}}},
  author = {Papier, Keren and Atkins, Joshua R. and Tong, Tammy Y. N. and Gaitskell, Kezia and Desai, Trishna and Ogamba, Chibuzor F. and Parsaeian, Mahboubeh and Reeves, Gillian K. and Mills, Ian G. and Key, Tim J. and {Smith-Byrne}, Karl and Travis, Ruth C.},
  year = {2024},
  month = may,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {4010},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-48017-6},
  urldate = {2024-05-15},
  abstract = {The availability of protein measurements and whole exome sequence data in the UK Biobank enables investigation of potential observational and genetic protein-cancer risk associations. We investigated associations of 1463 plasma proteins with incidence of 19 cancers and 9 cancer subsites in UK Biobank participants (average 12 years follow-up). Emerging protein-cancer associations were further explored using two genetic approaches, cis-pQTL and exome-wide protein genetic scores~(exGS). We identify 618 protein-cancer associations, of which 107 persist for cases diagnosed more than seven years after blood draw, 29 of 618~were associated in genetic analyses, and four had support from long time-to-diagnosis (\,{$>$}\,7 years) and both cis-pQTL and exGS analyses: CD74 and TNFRSF1B with NHL, ADAM8 with leukemia, and SFTPA2 with lung cancer. We present multiple blood protein-cancer risk associations, including many detectable more than seven years before cancer diagnosis and that~had concordant evidence from genetic analyses, suggesting a possible~role in cancer development.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Oncology,Risk factors},
  file = {/Users/nobr/Zotero/storage/YZQH7CFJ/Papier et al. - 2024 - Identifying proteomic risk factors for cancer using prospective and exome analyses of 1463 circulating proteins and risk of 19 cancers in the UK Biobank.pdf}
}

@book{papoulis2002,
  title = {Probability, Random Variables, and Stochastic Processes},
  author = {Papoulis, Athanasios and Pillai, S. Unnikrishna},
  year = {2002},
  series = {{{McGraw-Hill}} Series in Electrical and Computer Engineering},
  edition = {Fourth edition},
  publisher = {McGraw-Hill},
  address = {Boston New Delhi},
  isbn = {978-0-07-366011-0 978-0-07-048658-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/QGMVNPAZ/Papoulis and Pillai - 2002 - Probability, random variables, and stochastic processes.pdf}
}

@misc{paranjapeARTAutomaticMultistep2023,
  title = {{{ART}}: {{Automatic}} Multi-Step Reasoning and Tool-Use for Large Language Models},
  shorttitle = {{{ART}}},
  author = {Paranjape, Bhargavi and Lundberg, Scott and Singh, Sameer and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Ribeiro, Marco Tulio},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09014},
  eprint = {2303.09014},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-19},
  abstract = {Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multistep reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/WDC86VV7/Paranjape et al. - 2023 - ART Automatic multi-step reasoning and tool-use for large language models.pdf}
}

@article{parenteau2011,
  title = {The Modifiable Areal Unit Problem ({{MAUP}}) in the Relationship between Exposure to {{NO2}} and Respiratory Health},
  author = {Parenteau, Marie-Pierre and Sawada, Michael C},
  year = {2011},
  journal = {International Journal of Health Geographics},
  volume = {10},
  number = {1},
  pages = {58},
  issn = {1476-072X},
  doi = {10.1186/1476-072X-10-58},
  urldate = {2023-07-04},
  abstract = {Background: Many Canadian population health studies, including those focusing on the relationship between exposure to air pollution and health, have operationalized neighbourhoods at the census tract scale. At the same time, the conceptualization of place at the local scale is one of the weakest theoretical aspects in health geography. The modifiable areal unit problem (MAUP) raises issues when census tracts are used as neighbourhood proxies, and no other alternate spatial structure is used for sensitivity analysis. In the literature, conclusions on the relationship between NO2 and health outcomes are divided, and this situation may in part be due to the selection of an inappropriate spatial structure for analysis. Here, we undertake an analysis of NO2 and respiratory health in Ottawa, Canada using three different spatial structures in order to elucidate the effects that the spatial unit of analysis can have on analytical results. Results: Using three different spatial structures to examine and quantify the relationship between NO2 and respiratory morbidity, we offer three main conclusions: 1) exploratory spatial analytical methods can serve as an indication of the potential effect of the MAUP; 2) OLS regression results differ significantly using different spatial representations, and this could be a contributing factor to the lack of consensus in studies that focus on the relation between NO2 and respiratory health at the area-level; and 3) the use of three spatial representations confirms no measured effect of NO2 exposure on respiratory health in Ottawa. Conclusions: Area units used in population health studies should be delineated so as to represent the a priori scale of the expected scale interaction between neighbourhood processes and health. A thorough understanding of the role of the MAUP in the study of the relationship between NO2 and respiratory health is necessary for research into disease pathways based on statistical models, and for decision-makers to assess the scale at which interventions will have maximum benefit. In general, more research on the role of spatial representation in health studies is needed.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/F9MR925Z/Parenteau and Sawada - 2011 - The modifiable areal unit problem (MAUP) in the re.pdf}
}

@misc{park2024,
  title = {Generative {{Agent Simulations}} of 1,000 {{People}}},
  author = {Park, Joon Sung and Zou, Carolyn Q. and Shaw, Aaron and Hill, Benjamin Mako and Cai, Carrie and Morris, Meredith Ringel and Willer, Robb and Liang, Percy and Bernstein, Michael S.},
  year = {2024},
  month = nov,
  number = {arXiv:2411.10109},
  eprint = {2411.10109},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.10109},
  urldate = {2024-12-03},
  abstract = {The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85\% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/5SQ7HTE8/Park et al. - 2024 - Generative Agent Simulations of 1,000 People.pdf;/Users/nobr/Zotero/storage/3C9TIZ8M/2411.html}
}

@misc{park2025,
  title = {{{ICLR}}: {{In-Context Learning}} of {{Representations}}},
  shorttitle = {{{ICLR}}},
  author = {Park, Core Francisco and Lee, Andrew and Lubana, Ekdeep Singh and Yang, Yongyi and Okawa, Maya and Nishi, Kento and Wattenberg, Martin and Tanaka, Hidenori},
  year = {2025},
  month = may,
  number = {arXiv:2501.00070},
  eprint = {2501.00070},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.00070},
  urldate = {2025-08-02},
  abstract = {Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy ``graph tracing'' task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/ENZPVIDP/Park et al. - 2025 - ICLR In-Context Learning of Representations.pdf}
}

@misc{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2023},
  month = aug,
  number = {arXiv:2304.03442},
  eprint = {2304.03442},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.03442},
  urldate = {2024-01-25},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/HTXTYDFY/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf;/Users/nobr/Zotero/storage/63RJNJDT/2304.html}
}

@book{parr2022,
  title = {Active Inference: The Free Energy Principle in Mind, Brain, and Behavior},
  shorttitle = {Active Inference},
  author = {Parr, Thomas and Pezzulo, Giovanni and Friston, Karl J.},
  year = {2022},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts London, England},
  isbn = {978-0-262-04535-3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JYTS74NA/Parr et al. - 2022 - Active inference the free energy principle in mind, brain, and behavior.pdf}
}

@misc{partlan2022,
  title = {{{EvolvingBehavior}}: {{Towards Co-Creative Evolution}} of {{Behavior Trees}} for {{Game NPCs}}},
  shorttitle = {{{EvolvingBehavior}}},
  author = {Partlan, Nathan and Soto, Luis and Howe, Jim and Shrivastava, Sarthak and {El-Nasr}, Magy Seif and Marsella, Stacy},
  year = {2022},
  month = sep,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {To assist game developers in crafting game NPCs, we present EvolvingBehavior, a novel tool for genetic programming to evolve behavior trees in Unreal{\textregistered}Engine 4. In an initial evaluation, we compare evolved behavior to hand-crafted trees designed by our researchers, and to randomly-grown trees, in a 3D survival game. We find that EvolvingBehavior is capable of producing behavior approaching the designer's goals in this context. Finally, we discuss implications and future avenues of exploration for co-creative game AI design tools, as well as challenges and difficulties in behavior tree evolution.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Neural and Evolutionary Computing,D.2.2,I.2.1},
  file = {/Users/nobr/Zotero/storage/AMIB3634/Partlan et al. - 2022 - EvolvingBehavior Towards Co-Creative Evolution of Behavior Trees for Game NPCs.pdf}
}

@book{patterson2018,
  title = {Slavery and Social Death: A Comparative Study},
  shorttitle = {Slavery and Social Death},
  author = {Patterson, Orlando},
  year = {2018},
  edition = {First Harvard University Press paperback edition - with a new preface},
  publisher = {Harvard University Press},
  address = {Cambridge, Massachusetts London, England},
  isbn = {978-0-674-81082-2 978-0-674-98690-9},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/MK9LTL83/Patterson - 2018 - Slavery and social death a comparative study.pdf}
}

@misc{pearce2018geoengineering,
  title = {Geoengineering the Planet? {{More}} Scientists Now Say It Must Be an Option},
  author = {Pearce, Fred},
  year = {2018},
  month = sep
}

@misc{pedersen2023,
  title = {Learning to {{Act}} through {{Evolution}} of {{Neural Diversity}} in {{Random Neural Networks}}},
  author = {Pedersen, Joachim Winther and Risi, Sebastian},
  year = {2023},
  month = may,
  eprint = {2305.15945},
  primaryclass = {cs},
  doi = {10.1145/3583131.3590460},
  urldate = {2023-06-08},
  abstract = {Biological nervous systems consist of networks of diverse, sophisticated information processors in the form of neurons of different classes. In most artificial neural networks (ANNs), neural computation is abstracted to an activation function that is usually shared between all neurons within a layer or even the whole network; training of ANNs focuses on synaptic optimization. In this paper, we propose the optimization of neuro-centric parameters to attain a set of diverse neurons that can perform complex computations. Demonstrating the promise of the approach, we show that evolving neural parameters alone allows agents to solve various reinforcement learning tasks without optimizing any synaptic weights. While not aiming to be an accurate biological model, parameterizing neurons to a larger degree than the current common practice, allows us to ask questions about the computational abilities afforded by neural diversity in random neural networks. The presented results open up interesting future research directions, such as combining evolved neural diversity with activity-dependent plasticity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/J3R9GJD3/Pedersen and Risi - 2023 - Learning to Act through Evolution of Neural Divers.pdf;/Users/nobr/Zotero/storage/62IT9F2C/2305.html}
}

@article{peng2021,
  title = {Cross-{{Lingual Word Embedding Refinement}} by \${\textbackslash}ell\_\{1\}\$ {{Norm Optimisation}}},
  author = {Peng, Xutan and Lin, Chenghua and Stevenson, Mark},
  year = {2021},
  month = apr,
  eprint = {2104.04916},
  doi = {10.18653/v1/2021.naacl-main.214},
  abstract = {Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the \${\textbackslash}ell\_\{2\}\$ norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. \${\textbackslash}ell\_\{1\}\$ norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the \${\textbackslash}ell\_\{1\}\$ refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/IT9QX4IN/Peng et al. - 2021 - Cross-Lingual Word Embedding Refinement by $ell_ 1 $ Norm Optimisation.pdf}
}

@misc{peng2023,
  title = {{{RWKV}}: {{Reinventing RNNs}} for the {{Transformer Era}}},
  shorttitle = {{{RWKV}}},
  author = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and He, Xuzheng and Hou, Haowen and Kazienko, Przemyslaw and Kocon, Jan and Kong, Jiaming and Koptyra, Bartlomiej and Lau, Hayden and Mantri, Krishna Sri Ipsit and Mom, Ferdinand and Saito, Atsushi and Tang, Xiangru and Wang, Bolun and Wind, Johan S. and Wozniak, Stansilaw and Zhang, Ruichong and Zhang, Zhenyuan and Zhao, Qihang and Zhou, Peng and Zhu, Jian and Zhu, Rui-Jie},
  year = {2023},
  month = may,
  number = {arXiv:2305.13048},
  eprint = {2305.13048},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-19},
  abstract = {Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/AV5TXZ4B/Peng et al. - 2023 - RWKV Reinventing RNNs for the Transformer Era.pdf;/Users/nobr/Zotero/storage/ZIEHHYXD/Peng et al. - 2023 - RWKV Reinventing RNNs for the Transformer Era.pdf}
}

@misc{pereira2015,
  title = {A {{Framework}} for {{Constrained}} and {{Adaptive Behavior-Based Agents}}},
  author = {Pereira, Renato de Pontes and Engel, Paulo Martins},
  year = {2015},
  month = jun,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Behavior Trees are commonly used to model agents for robotics and games, where constrained behaviors must be designed by human experts in order to guarantee that these agents will execute a specific chain of actions given a specific set of perceptions. In such application areas, learning is a desirable feature to provide agents with the ability to adapt and improve interactions with humans and environment, but often discarded due to its unreliability. In this paper, we propose a framework that uses Reinforcement Learning nodes as part of Behavior Trees to address the problem of adding learning capabilities in constrained agents. We show how this framework relates to Options in Hierarchical Reinforcement Learning, ensuring convergence of nested learning nodes, and we empirically show that the learning nodes do not affect the execution of other nodes in the tree.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/nobr/Zotero/storage/XIICR8HF/Pereira and Engel - 2015 - A Framework for Constrained and Adaptive Behavior-Based Agents.pdf}
}

@article{perez-liebana2020,
  title = {Tribes: {{A New Turn-Based Strategy Game}} for {{AI Research}}},
  shorttitle = {Tribes},
  author = {{Perez-Liebana}, Diego and Hsu, Yu-Jhen and Emmanouilidis, Stavros and Khaleque, Bobby and Gaina, Raluca},
  year = {2020},
  month = oct,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume = {16},
  number = {1},
  pages = {252--258},
  issn = {2334-0924},
  doi = {10.1609/aiide.v16i1.7438},
  urldate = {2023-12-15},
  abstract = {This paper introduces Tribes, a new turn-based strategy game framework. Tribes is a multi-player, multi-agent, stochastic and partially observable game that involves strategic and tactical combat decisions. A good playing strategy requires the management of a technology tree, build orders and economy. The framework provides a Forward Model, which can be used by Statistical Forward Planning methods. This paper describes the framework and the opportunities for Game AI research it brings. We further provide an analysis on the action space of this game, as well as benchmarking a series of agents (rule based, one step look-ahead, Monte Carlo, Monte Carlo Tree Search, and Rolling Horizon Evolution) to study their relative playing strength. Results show that although some of these agents can play at a decent level, they are still far from human playing strength.},
  copyright = {Copyright (c) 2020 Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/C5EY85H8/Perez-Liebana et al. - 2020 - Tribes A New Turn-Based Strategy Game for AI Research.pdf}
}

@article{perla2011,
  title = {Why {{Wargaming Works}}},
  author = {Perla, Peter P and McGrady, {\relax ED}},
  year = {2011},
  volume = {64},
  number = {3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZFPG7FSI/Perla and McGrady - 2011 - Why Wargaming Works.pdf}
}

@article{pesnell2021,
  title = {Using {{Hilbert}} Curves to Organize, Sample, and Sonify Solar Data},
  author = {Pesnell, W. Dean and {Ingram-Johnson}, Kyle and Addison, Kevin},
  year = {2021},
  month = oct,
  journal = {American Journal of Physics},
  volume = {89},
  number = {10},
  pages = {943},
  issn = {0002-9505},
  doi = {10.1119/10.0005403},
  urldate = {2023-10-06},
  abstract = {How many ways can we explore the Sun? We have images in many wavelengths and squiggly lines of many parameters that we can use to characterize the Sun. We know that while the Sun is blindingly bright to the naked eye, it also has regions that are dark in some wavelengths of light. All of those classifications are based on vision. Hearing is another sense that can be used to explore solar data. Some data, such as the sunspot number or the extreme ultraviolet spectral irradiance, can be readily sonified by converting the data values to musical pitches. Images are more difficult. Using a raster scan algorithm to convert a full-disk image of the Sun to a stream of pixel values creates variations that are dominated by the pattern of moving on and off the limb of the Sun. A sonification of such a raster scan will contain discontinuities at the limbs that mask the information contained in the image. As an alternative, Hilbert curves are continuous space-filling curves that map a linear variable onto the two-dimensional coordinates of an image. We have investigated using Hilbert curves as a way to sample and analyze solar images. Reading the image along a Hilbert curve keeps most neighborhoods close together as the resolution (i.e., the order of the Hilbert curve) increases. It also removes most of the detector size periodicities and may reveal larger-scale features. We present several examples of sonified solar data, including sunspot number, extreme ultraviolet (EUV) spectral irradiances, an EUV image, and a sequence of EUV images during a filament eruption.},
  file = {/Users/nobr/Zotero/storage/PV467SC5/Pesnell et al. - 2021 - Using Hilbert curves to organize, sample, and sonify solar data.pdf;/Users/nobr/Zotero/storage/KW3WXHHE/Using-Hilbert-curves-to-organize-sample-and-sonify.html}
}

@misc{peters2017china,
  title = {How {{China}} Reduced {{Europe}}'s Carbon Footprint},
  author = {Peters, Glen and Andrew, Robbie and Korsbakken, Jan Ivar},
  year = {2017},
  month = nov
}

@book{petzold2008,
  title = {The Annotated {{Turing}}: A Guided Tour through {{Alan Turing}}'s Historic Paper on Computability and the {{Turing}} Machine},
  shorttitle = {The Annotated {{Turing}}},
  author = {Petzold, Charles},
  year = {2008},
  publisher = {Wiley publ},
  address = {Indianapolis (Ind.)},
  abstract = {Mathematician Alan Turing invented an imaginary computer known as the Turing Machine; in an age before computers, he explored the concept of what it meant to be computable, creating the field of computability theory in the process, a foundation of present-day computer programming. The book expands Turing's original 36-page paper with additional background chapters and extensive annotations; the author elaborates on and clarifies many of Turing's statements, making the original difficult-to-read document accessible to present day programmers, computer science majors, math geeks, and others. Interwoven into the narrative are the highlights of Turing's own life: his years at Cambridge and Princeton, his secret work in cryptanalysis during World War II, his involvement in seminal computer projects, his speculations about artificial intelligence, his arrest and prosecution for the crime of "gross indecency," and his early death by apparent suicide at the age of 41. - Publisher},
  isbn = {978-0-470-22905-7},
  langid = {english},
  lccn = {511.35},
  file = {/Users/nobr/Zotero/storage/V4EKR7QG/Petzold - 2008 - The annotated Turing a guided tour through Alan Turing's historic paper on computability and the Tu.pdf}
}

@book{pickering2010,
  title = {The Cybernetic Brain: Sketches of Another Future},
  shorttitle = {The Cybernetic Brain},
  author = {Pickering, Andrew},
  year = {2010},
  publisher = {University of Chicago Press},
  address = {Chicago ; London},
  isbn = {978-0-226-66789-8},
  langid = {english},
  lccn = {Q310 .P53 2010},
  keywords = {Brain,Cybernetics,History,Self-organizing systems}
}

@article{pienaar2015,
  title = {A Graph-Separation Theorem for Quantum Causal Models},
  author = {Pienaar, Jacques and Brukner, {\v C}aslav},
  year = {2015},
  month = jul,
  journal = {New Journal of Physics},
  volume = {17},
  number = {7},
  pages = {073020},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/17/7/073020},
  urldate = {2024-04-05},
  abstract = {A causal model is an abstract representation of a physical system as a directed acyclic graph (DAG), where the statistical dependencies are encoded using a graphical criterion called `d-separation'. Recent work by Wood and Spekkens shows that causal models cannot, in general, provide a faithful representation of quantum systems. Since d-separation encodes a form of Reichenbach's common cause principle (RCCP), whose validity is questionable in quantum mechanics, we propose a generalized graph separation rule that does not assume the RCCP. We prove that the new rule faithfully captures the statistical dependencies between observables in a quantum network, encoded as a DAG, and reduces to d-separation in a classical limit.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LITDDRZE/Pienaar and Brukner - 2015 - A graph-separation theorem for quantum causal models.pdf}
}

@book{pincus-witten1977,
  title = {Postminimalism},
  author = {{Pincus-Witten}, Robert},
  year = {1977},
  publisher = {Out of London Press},
  address = {New York},
  isbn = {978-0-915570-07-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PJ6MENFR/Pincus-Witten - 1977 - Postminimalism.pdf}
}

@inproceedings{plantec2023,
  title = {Flow-{{Lenia}}: {{Towards}} Open-Ended Evolution in Cellular Automata through Mass Conservation and Parameter Localization},
  shorttitle = {Flow-{{Lenia}}},
  booktitle = {The 2023 {{Conference}} on {{Artificial Life}}},
  author = {Plantec, Erwan and Hamon, Gautier and Etcheverry, Mayalen and Oudeyer, Pierre-Yves and {Moulin-Frier}, Cl{\'e}ment and Chan, Bert Wang-Chak},
  year = {2023},
  publisher = {MIT Press},
  doi = {10.1162/isal_a_00651},
  urldate = {2023-10-23},
  abstract = {The design of complex self-organising systems producing life-like phenomena, such as the open-ended evolution of virtual creatures, is one of the main goals of artificial life. Lenia, a family of cellular automata (CA) generalizing Conway's Game of Life to continuous space, time and states, has attracted a lot of attention because of the wide diversity of self-organizing patterns it can generate. Among those, some spatially localized patterns (SLPs) resemble life-like artificial creatures and display complex behaviors. However, those creatures are found in only a small subspace of the Lenia parameter space and are not trivial to discover, necessitating advanced search algorithms. Furthermore, each of these creatures exist only in worlds governed by specific update rules and thus cannot interact in the same one. This paper proposes as mass-conservative extension of Lenia, called Flow Lenia, that solve both of these issues. We present experiments demonstrating its effectiveness in generating SLPs with complex behaviors and show that the update rule parameters can be optimized to generate SLPs showing behaviors of interest. Finally, we show that Flow Lenia enables the integration of the parameters of the CA update rules within the CA dynamics, making them dynamic and localized, allowing for multi-species simulations, with locally coherent update rules that define properties of the emerging creatures, and that can be mixed with neighbouring rules. We argue that this paves the way for the intrinsic evolution of selforganized artificial life forms within continuous CAs. A notebook with Flow Lenia implementation and demo are available at https://tinyurl.com/mr2ncy3h.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HBEZYVUY/Plantec et al. - 2023 - Flow-Lenia Towards open-ended evolution in cellular automata through mass conservation and paramete.pdf}
}

@article{plick2013,
  title = {Quantum Orbital Angular Momentum of Elliptically Symmetric Light},
  author = {Plick, William N and Krenn, Mario and Fickler, Robert and Ramelow, Sven and Zeilinger, Anton},
  year = {2013},
  journal = {PHYSICAL REVIEW A},
  volume = {87},
  pages = {33806},
  doi = {10.1103/PhysRevA.87.033806},
  urldate = {2023-04-08},
  abstract = {We present a quantum-mechanical analysis of the orbital angular momentum of a class of recently discovered elliptically symmetric stable light fields-the so-called Ince-Gauss modes. We study, in a fully quantum formalism, how the orbital angular momentum of these beams varies with their ellipticity, and we discover several compelling features, including nonmonotonic behavior, stable beams with real continuous (noninteger) orbital angular momenta, and orthogonal modes with the same orbital angular momenta. We explore, and explain in detail, the reasons for this behavior. These features may have applications in quantum key distribution, atom trapping, and quantum informatics in general-as the ellipticity opens up an alternative way of navigating the spatial photonic Hilbert space.},
  keywords = {0367Hk,4250Ex,number(s): 4250Tx},
  file = {/Users/nobr/Zotero/storage/2C44GB7C/full-text.pdf}
}

@article{poli2023,
  title = {Hyena {{Hierarchy}}: {{Towards Larger Convolutional Language Models}}},
  author = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  year = {2023},
  month = feb,
  eprint = {2302.10866},
  urldate = {2023-04-09},
  abstract = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20\% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/35DJNDMF/full-text.pdf}
}

@misc{pope2013papers,
  title = {Papers, Please},
  author = {Pope, Lucas},
  year = {2013},
  publisher = {3909 LLC}
}

@misc{pope2018obra,
  title = {Return of the Obra Dinn},
  author = {Pope, Lucas},
  year = {2018},
  publisher = {3909 LLC}
}

@misc{popeHierarchicalReinforcementLearning2021,
  title = {Hierarchical {{Reinforcement Learning}} for {{Air-to-Air Combat}}},
  author = {Pope, Adrian P. and Ide, Jaime S. and Micovic, Daria and Diaz, Henry and Rosenbluth, David and Ritholtz, Lee and Twedt, Jason C. and Walker, Thayne T. and Alcedo, Kevin and Javorsek, Daniel},
  year = {2021},
  month = jun,
  number = {arXiv:2105.00990},
  eprint = {2105.00990},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-15},
  abstract = {Artificial Intelligence (AI) is becoming a critical component in the defense industry, as recently demonstrated by DARPA's AlphaDogfight Trials (ADT). ADT sought to vet the feasibility of AI algorithms capable of piloting an F-16 in simulated air-to-air combat. As a participant in ADT, Lockheed Martin`s (LM) approach combines a hierarchical architecture with maximum-entropy reinforcement learning (RL), integrates expert knowledge through reward shaping, and supports modularity of policies. This approach achieved a 2nd place finish in the final ADT event (among eight total competitors) and defeated a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course in match play.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/JXI4JTWQ/Pope et al. - 2021 - Hierarchical Reinforcement Learning for Air-to-Air Combat.pdf}
}

@article{popov2017,
  title = {The Relational Luring Effect: {{Retrieval}} of Relational Information during Associative Recognition.},
  shorttitle = {The Relational Luring Effect},
  author = {Popov, Vencislav and Hristova, Penka and Anders, Royce},
  year = {2017},
  journal = {Journal of Experimental Psychology: General},
  volume = {146},
  number = {5},
  pages = {722--745},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000305},
  urldate = {2024-01-08},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/KNCJYW4I/Popov et al. - 2017 - The relational luring effect Retrieval of relational information during associative recognition..pdf}
}

@misc{porter2024,
  title = {{{LLMD}}: {{A Large Language Model}} for {{Interpreting Longitudinal Medical Records}}},
  shorttitle = {{{LLMD}}},
  author = {Porter, Robert and Diehl, Adam and Pastel, Benjamin and Hinnefeld, J. Henry and Nerenberg, Lawson and Maung, Pye and Kerbrat, Sebastien and Hanson, Gillian and Astorino, Troy and Tarsa, Stephen J.},
  year = {2024},
  month = oct,
  number = {arXiv:2410.12860},
  eprint = {2410.12860},
  doi = {10.48550/arXiv.2410.12860},
  urldate = {2024-10-18},
  abstract = {We introduce LLMD, a large language model designed to analyze a patient's medical history based on their medical records. Along with domain knowledge, LLMD is trained on a large corpus of records collected over time and across facilities, as well as tasks and labels that make nuanced connections among them. This approach is critical to an accurate picture of patient health, and has distinctive advantages over models trained on knowledge alone, unlabeled records, structured EHR data, or records from a single health system. The recipe for LLMD continues pretraining a foundational model on both domain knowledge and the contents of millions of records. These span an average of 10 years of care and as many as 140 care sites per patient. LLMD is then instruction fine-tuned on structuring and abstraction tasks. The former jointly identify and normalize document metadata, provenance information, clinical named-entities, and ontology mappings, while the latter roll these into higher-level representations, such a continuous era of time a patient was on a medication. LLMD is deployed within a layered validation system that includes continual random audits and review by experts, e.g. based on uncertainty, disease-specific rules, or use-case. LLMD exhibits large gains over both more-powerful generalized models and domain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state of the art accuracy on PubMedQA text responses, besting orders-of-magnitude larger models. On production tasks, we show that LLMD significantly outperforms all other models evaluated, and among alternatives, large general purpose LLMs like GPT-4o are more accurate than models emphasizing medical knowledge. We find strong evidence that accuracy on today's medical benchmarks is not the most significant factor when analyzing real-world patient data, an insight with implications for future medical LLMs.'},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/EUN3JAWE/Porter et al. - 2024 - LLMD A Large Language Model for Interpreting Longitudinal Medical Records.pdf}
}

@incollection{portner2014ocean,
  title = {Ocean Systems},
  booktitle = {Climate Change 2014: {{Impacts}}, Adaptation, and Vulnerability. {{Part}} a: {{Global}} and Sectoral Aspects. {{Contribution}} of Working Group {{II}} to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change},
  author = {P{\"o}rtner, H.-O. and Karl, D. M. and Boyd, P. W. and Cheung, W. W. L. and {Lluch-Cota}, S. E. and Nojiri, Y. and Schmidt, D. N. and Zavialov, P. O.},
  editor = {Field, C. B. and Barros, V. R. and Dokken, D. J. and Mach, K. J. and Mastrandrea, M. D. and Bilir, T. E. and Chatterjee, M. and Ebi, K. L. and Estrada, Y. O. and Genova, R. C. and Girma, B. and Kissel, E. S. and Levy, A. N. and MacCracken, S. and Mastrandrea, P. R. and White, L. L.},
  year = {2014},
  pages = {411--484},
  publisher = {Cambridge University Press},
  address = {Cambridge, United Kingdom and New York, NY, USA}
}

@misc{postol2023,
  title = {Algebraic {{Topology}} for {{Data Scientists}}},
  author = {Postol, Michael S.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.10825},
  eprint = {2308.10825},
  primaryclass = {math},
  publisher = {arXiv},
  urldate = {2023-09-16},
  abstract = {This book gives a thorough introduction to topological data analysis (TDA), the application of algebraic topology to data science. Algebraic topology is traditionally a very specialized field of math, and most mathematicians have never been exposed to it, let alone data scientists, computer scientists, and analysts. I have three goals in writing this book. The first is to bring people up to speed who are missing a lot of the necessary background. I will describe the topics in point-set topology, abstract algebra, and homology theory needed for a good understanding of TDA. The second is to explain TDA and some current applications and techniques. Finally, I would like to answer some questions about more advanced topics such as cohomology, homotopy, obstruction theory, and Steenrod squares, and what they can tell us about data. It is hoped that readers will acquire the tools to start to think about these topics and where they might fit in.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {55N31 (Primary) 55-01 68T05 68Q32 (Secondary),I.2.6,I.5,Mathematics - Algebraic Topology,Mathematics - History and Overview},
  file = {/Users/nobr/Zotero/storage/2DYLLRZX/Postol - 2023 - Algebraic Topology for Data Scientists.pdf}
}

@misc{potthast2017,
  title = {A {{Stylometric Inquiry}} into {{Hyperpartisan}} and {{Fake News}}},
  author = {Potthast, Martin and Kiesel, Johannes and Reinartz, Kevin and Bevendorff, Janek and Stein, Benno},
  year = {2017},
  month = feb,
  number = {arXiv:1702.05638},
  eprint = {1702.05638},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.05638},
  urldate = {2023-06-11},
  abstract = {This paper reports on a writing style analysis of hyperpartisan (i.e., extremely one-sided) news in connection to fake news. It presents a large corpus of 1,627 articles that were manually fact-checked by professional journalists from BuzzFeed. The articles originated from 9 well-known political publishers, 3 each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing. In sum, the corpus contains 299 fake news, 97\% of which originated from hyperpartisan publishers. We propose and demonstrate a new way of assessing style similarity between text categories via Unmasking---a meta-learning approach originally devised for authorship verification---, revealing that the style of left-wing and right-wing news have a lot more in common than any of the two have with the mainstream. Furthermore, we show that hyperpartisan news can be discriminated well by its style from the mainstream (F1=0.78), as can be satire from both (F1=0.81). Unsurprisingly, style-based fake news detection does not live up to scratch (F1=0.46). Nevertheless, the former results are important to implement pre-screening for fake news detectors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/YDTXFFPR/Potthast et al. - 2017 - A Stylometric Inquiry into Hyperpartisan and Fake .pdf;/Users/nobr/Zotero/storage/II6LVJWC/1702.html}
}

@article{pouplin2022,
  title = {Identifying Latent Distances with {{Finslerian}} Geometry},
  author = {Pouplin, Alison and Eklund, David and Ek, Carl Henrik and Hauberg, S{\o}ren},
  year = {2022},
  abstract = {Riemannian geometry provides us with powerful tools to explore the latent space of generative models while preserving the underlying structure of the data. The latent space can be equipped it with a Riemannian metric, pulled back from the data manifold. With this metric, we can systematically navigate the space relying on geodesics defined as the shortest curves between two points.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GB8XA6C8/Pouplin et al. - Identifying latent distances with Finslerian geometry.pdf}
}

@misc{power2022,
  title = {Grokking: {{Generalization Beyond Overfitting}} on {{Small Algorithmic Datasets}}},
  shorttitle = {Grokking},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  year = {2022},
  month = jan,
  number = {arXiv:2201.02177},
  eprint = {2201.02177},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.02177},
  urldate = {2024-02-13},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/RYSEV6U4/Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Small Algorithmic Datasets.pdf;/Users/nobr/Zotero/storage/8UBPAVHA/2201.html}
}

@book{pressfield2002,
  title = {The War of Art: Winning the Inner Creative Battle},
  shorttitle = {The War of Art},
  author = {Pressfield, Steven},
  year = {2002},
  edition = {1st ed},
  publisher = {Rugged Land},
  address = {[New York, NY},
  isbn = {978-1-59071-003-6},
  langid = {english},
  lccn = {BF408 .P69 2002},
  keywords = {Creation (Literary artistic etc.),Creative thinking,Inhibition,Pressfield Steven,Procrastination,Resistance (Psychoanalysis)},
  file = {/Users/nobr/Zotero/storage/BWAPEKWI/Pressfield - 2002 - The war of art winning the inner creative battle.pdf}
}

@book{prince2023,
  title = {Understanding Deep Learning},
  author = {Prince, Simon J. D.},
  year = {2023},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts London, England},
  abstract = {"This book covers modern deep learning and tackles supervised learning, model architecture, unsupervised learning, and deep reinforcement learning"--},
  isbn = {978-0-262-04864-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/84JDKWTE/Prince - Understanding Deep Learning.pdf}
}

@misc{prodes_system,
  title = {{{PRODES}} - Programa de C{\'a}lculo Do Desflorestamento Da Amaz{\^o}nia},
  author = {{INPE}}
}

@misc{prokopov2024,
  title = {Tonsky/{{FiraCode}}},
  author = {Prokopov, Nikita},
  year = {2024},
  month = dec,
  urldate = {2024-12-25},
  abstract = {Free monospaced font with programming ligatures},
  copyright = {OFL-1.1},
  keywords = {font,ligatures,programming-ligatures}
}

@article{prophet,
  title = {Authority: {{EO}} 13526 {{Chief}}, {{Records}} \&{{Declass Div}}, {{WHS DatevEC}} 2 0 2012},
  author = {Prophet, Proud},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TMHCFEV2/Prophet - Authority EO 13526 Chief, Records &Declass Div, WHS DatevEC 2 0 2012.pdf}
}

@phdthesis{proy2023,
  title = {The {{Classification}} of {{Civilians}} as {{Human Shields}}: A {{Means}} to {{Justify Violence}}?},
  author = {Proy, Coline},
  year = {2023},
  address = {Sweden},
  abstract = {Human shields have been increasingly documented in contemporary theatres of war. In this context, it is interesting to examine the circumstances in which the attacking party classifies the civilians they face as human shields. Therefore, this thesis focuses on the use of the classification of human shields by belligerents facing civilians during armed conflicts. The paper sets forth the argument that this legal category has been instrumentalized to justify civilian casualties, assigning the entire responsibility to the adversary. To support this claim, the research provides legal clarifications on the concept of human shields, the status of civilians used as such, and the obligations of the attacking party facing them. This involves uncovering the gaps and uncertainties in the law regarding human shields on the one hand, and the principle of proportionality on the other hand. Despite those uncertainties, this thesis maintains that human shields should be treated as civilians and that the principle of proportionality should apply normally.},
  langid = {english},
  school = {Lund University Faculty of Law},
  file = {/Users/nobr/Zotero/storage/WGHGW5SK/Proy - The Classification of Civilians as Human Shields a Means to Justify Violence.pdf}
}

@inproceedings{qian2017,
  title = {Deep {{Learning}} Based {{Authorship Identification}}},
  author = {Qian, Chen and He, Ting and Zhang, R.},
  year = {2017},
  urldate = {2023-06-11},
  abstract = {Authorship identification is an important topic in the field of Natural Language Processing (NLP). It enables us to identify the most likely author of articles, news or messages. Authorship identification can be applied to tasks such as identifying anonymous author, detecting plagiarism or finding ghost writer. In this project, we tackled this problem at different levels, with different deep learning models and on different datasets. Among all models we tested, article-level GRU achieves the best result of 69.1\% accuracy on C50 dataset and 89.2\% on Guternberg dataset. We further studied authorship verification, on which task our Siamese network-based model outputs 99.8\% accuracy on both C50 and Gutenberg.},
  file = {/Users/nobr/Zotero/storage/5AZKUIVV/Qian et al. - 2017 - Deep Learning based Authorship Identification.pdf}
}

@techreport{radford2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  file = {/Users/nobr/Zotero/storage/CGRFX6IU/language_understanding_paper.pdf}
}

@inproceedings{radford2018a,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik},
  year = {2018},
  urldate = {2024-11-26},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  file = {/Users/nobr/Zotero/storage/7CLQ888Z/Radford and Narasimhan - 2018 - Improving Language Understanding by Generative Pre-Training.pdf}
}

@misc{rafailovDirectPreferenceOptimization2023,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  year = {2023},
  month = dec,
  number = {arXiv:2305.18290},
  eprint = {2305.18290},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.18290},
  urldate = {2023-12-31},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/BDKV66KZ/Rafailov et al. - 2023 - Direct Preference Optimization Your Language Model is Secretly a Reward Model.pdf}
}

@article{raguso2021,
  title = {Coevolution as an Engine of Biodiversity and a Cornucopia of Ecosystem Services},
  author = {Raguso, Robert A.},
  year = {2021},
  month = jan,
  journal = {PLANTS, PEOPLE, PLANET},
  volume = {3},
  number = {1},
  pages = {61--73},
  issn = {2572-2611, 2572-2611},
  doi = {10.1002/ppp3.10127},
  urldate = {2025-08-02},
  abstract = {Societal Impact Statement             Coevolutionary processes, which have governed interactions between organisms throughout the history of life, also serve as an engine of ecosystem services for humans. The escalating arms races between plants and herbivores, flowers and pollinators, have generated a cornucopia of foods, raw materials, perfumes, spices, ornamentals, medicines, and drugs. Human history is replete with aesthetic as well as economic inspiration drawn from such plants. Our future may depend on similar inspiration, as we confront novel health, agricultural, and environmental challenges in the face of global change.                            Summary                                         ``Coevolution'' was coined to conceptualize escalating arms races between plants and herbivores in evolutionary time, often mediated by natural products. Our current view embraces broader coevolutionary relationships between obligate mutualists, symbionts, parasites, and enemies, which frequently increase rates of diversification in coevolving lineages. Because humans benefit from the foods, materials, and drugs produced by plants in response to reciprocal selective pressures, coevolutionary ``escape and radiate'' diversification may amplify ecosystem services along with species richness, with humans as beneficiaries. For example, coevolutionary escalation of defenses between Burseraceae and their herbivores resulted in hundreds of resinous tree species, anchoring the trade of copal, frankincense, and myrrh across the ancient world. Examination of three diverse angiosperm orders (Asparagales, Malpighiales, and Gentianales), reveals ecosystem services in the form of alkaloids and hallucinogens, perfumes, spices, coffee, and rubber. Pollinator-mediated selection by hawk moths and bats gave rise to heavily perfumed ``moonflowers'' (gardenias and jasmines) with aesthetic appeal to humans, and to immense blooming displays by agave plants, co-opted by humans as a source of tequila and mezcal. Even when pollinator-mediated diversification does not arise through coevolution, the resulting biotic richness provides evolutionary insights as well as ecosystem services. The convergent evolution of ``kettle-trap'' flowers in species-rich plant lineages (               Aristolochia               and               Ceropegia               ) reveals the surprising value of small flies as pollinators and the opportunity to develop biocontrol that leverages floral features attractive to agricultural pests and disease vectors. This article highlights coevolution as a source of ecosystem services and potential solutions to the emerging challenges of global change.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/PEZJJQEQ/Raguso - 2021 - Coevolution as an engine of biodiversity and a cornucopia of ecosystem services.pdf}
}

@misc{rahmanzadehgervi2024,
  title = {Vision Language Models Are Blind},
  author = {Rahmanzadehgervi, Pooyan and Bolton, Logan and Taesiri, Mohammad Reza and Nguyen, Anh Totti},
  year = {2024},
  month = jul,
  number = {arXiv:2407.06581},
  eprint = {2407.06581},
  primaryclass = {cs},
  urldate = {2024-07-10},
  abstract = {Large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini 1.5 Pro are powering countless image-text applications and scoring high on many vision-understanding benchmarks. Yet, we find that VLMs fail on 7 visual tasks absurdly easy to humans such as identifying (a) whether two circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d) counting the number of circles in a Olympic-like logo. The shockingly poor performance of four state-of-the-art VLMs suggests their vision is, at best, like of a person with myopia seeing fine details as blurry, and at worst, like an intelligent person that is blind making educated guesses. Code is available at: https://vlmsareblind.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/RS94FDE6/Rahmanzadehgervi et al. - 2024 - Vision language models are blind.pdf}
}

@inproceedings{rahmanzadehgervi2025,
  title = {Vision Language Models Are Blind},
  booktitle = {Computer Vision -- {{ACCV}} 2024},
  author = {Rahmanzadehgervi, Pooyan and Bolton, Logan and Taesiri, Mohammad Reza and Nguyen, Anh Totti},
  editor = {Cho, Minsu and Laptev, Ivan and Tran, Du and Yao, Angela and Zha, Hongbin},
  year = {2025},
  pages = {293--309},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  abstract = {While large language models with vision capabilities (VLMs), e.g., and , are powering various image-text applications and scoring high on many vision-understanding benchmarks, they are still surprisingly struggling with low-level vision tasks that are easy to humans. Specifically, on BlindTest, our suite of 7 very simple tasks such as identifying (a) whether two circles overlap; (b) how many times two lines intersect; (c) which letter is being circled in a word; and (d) the number of circles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07\% accurate on average.  performs the best at 77.84\% accuracy, but this is still far from the human expected accuracy of 100\% . Across different image resolutions and line widths, VLMs consistently struggle with those tasks that require precise spatial information when geometric primitives overlap or are close together.Code and data are at: vlmsareblind.github.io.},
  isbn = {978-981-96-0917-8}
}

@article{rai2018,
  title = {Cognitive {{Stimulation Therapy}} for {{Dementia}}},
  author = {Rai, Harleen and Yates, Lauren and Orrell, Martin},
  year = {2018},
  month = nov,
  journal = {Clinics in Geriatric Medicine},
  volume = {34},
  number = {4},
  pages = {653--665},
  issn = {07490690},
  doi = {10.1016/j.cger.2018.06.010},
  urldate = {2023-10-20},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/Q44QGV6K/Rai et al. - 2018 - Cognitive Stimulation Therapy for Dementia.pdf}
}

@misc{rakhimberdina2021,
  title = {Natural {{Image Reconstruction}} from {{fMRI}} Using {{Deep Learning}}: {{A Survey}}},
  shorttitle = {Natural {{Image Reconstruction}} from {{fMRI}} Using {{Deep Learning}}},
  author = {Rakhimberdina, Zarina and Jodelet, Quentin and Liu, Xin and Murata, Tsuyoshi},
  year = {2021},
  month = oct,
  journal = {arXiv.org},
  urldate = {2023-05-10},
  abstract = {With the advent of brain imaging techniques and machine learning tools, much effort has been devoted to building computational models to capture the encoding of visual information in the human brain. One of the most challenging brain decoding tasks is the accurate reconstruction of the perceived natural images from brain activities measured by functional magnetic resonance imaging (fMRI). In this work, we survey the most recent deep learning methods for natural image reconstruction from fMRI. We examine these methods in terms of architectural design, benchmark datasets, and evaluation metrics and present a fair performance evaluation across standardized evaluation metrics. Finally, we discuss the strengths and limitations of existing studies and present potential future directions.},
  howpublished = {https://arxiv.org/abs/2110.09006v2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UYTIZSCU/Rakhimberdina et al. - 2021 - Natural Image Reconstruction from fMRI using Deep .pdf;/Users/nobr/Zotero/storage/Y3MUENIF/Rakhimberdina et al. - 2021 - Natural Image Reconstruction from fMRI using Deep .pdf}
}

@article{ramesh2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year = {2021},
  month = feb,
  eprint = {2102.12092},
  urldate = {2023-03-22},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/P49IITGE/full-text.pdf}
}

@article{ramicic2023,
  title = {Uncertainty Maximization in Partially Observable Domains: {{A}} Cognitive Perspective},
  shorttitle = {Uncertainty Maximization in Partially Observable Domains},
  author = {Ramicic, Mirza and Bonarini, Andrea},
  year = {2023},
  month = may,
  journal = {Neural Networks},
  volume = {162},
  pages = {456--471},
  issn = {08936080},
  doi = {10.1016/j.neunet.2023.02.044},
  urldate = {2023-11-12},
  abstract = {Faced with an ever-increasing complexity of their domains of application, artificial learning agents are now able to scale up in their ability to process an overwhelming amount of data. However, this comes at the cost of encoding and processing an increasing amount of redundant information. This work exploits the possibility of learning systems, applied in partially observable domains, to selectively focus on the specific type of information that is more likely related to the causal interaction among transitioning states. A temporal difference displacement criterion is defined to implement adaptive masking of the observations. It can enable a significant improvement of convergence of temporal difference algorithms applied to partially observable Markov processes, as shown by experiments performed under a variety of machine learning problems, ranging from highly complex visuals as Atari games to simple textbook control problems such as CartPole. The proposed framework can be added to most RL algorithms since it only affects the observation process, selecting the parts more promising to explain the dynamics of the environment and reducing the dimension of the observation space.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IETVWKLY/Ramicic and Bonarini - 2023 - Uncertainty maximization in partially observable domains A cognitive perspective.pdf}
}

@misc{ramsauer2021,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2021},
  month = apr,
  number = {arXiv:2008.02217},
  eprint = {2008.02217},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-17},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/FBSXRWC7/2008.02217}
}

@article{randell1982,
  title = {From {{Analytical Engine}} to {{Electronic Digital Computer}}: {{The Contributions}} of {{Ludgate}}, {{Torres}}, and {{Bush}}},
  author = {Randell, Brian},
  year = {1982},
  journal = {Bush Annals of the History of Computing},
  volume = {4},
  number = {4},
  urldate = {2023-03-19},
  abstract = {These three inventors, who apparently were unaware of one another's existence, were all directly influenced by knowledge of Charles Babbage's Analytical Engine, and each played a significant role in the history of the development of program-controlled computers.},
  keywords = {A 0 [General]-biographies,Analytical Engine,Experimentation Additional Key Words and Phrases: C Babbage,hardware,L Torres y Quevedo General Terms: Design,L Torres y Quevedo; K2 [History of Computing]-V Bush,P E Ludgate,people,V Bush},
  file = {/Users/nobr/Zotero/storage/H2AQ56MK/full-text.pdf}
}

@misc{rannon2025,
  title = {Leveraging {{Natural Language Processing}} to {{Unravel}} the {{Mystery}} of {{Life}}: {{A Review}} of {{NLP Approaches}} in {{Genomics}}, {{Transcriptomics}}, and {{Proteomics}}},
  shorttitle = {Leveraging {{Natural Language Processing}} to {{Unravel}} the {{Mystery}} of {{Life}}},
  author = {Rannon, Ella and Burstein, David},
  year = {2025},
  month = jun,
  number = {arXiv:2506.02212},
  eprint = {2506.02212},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2506.02212},
  urldate = {2025-06-08},
  abstract = {Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Quantitative Biology - Genomics},
  file = {/Users/nobr/Zotero/storage/FL2FRTZD/Rannon and Burstein - 2025 - Leveraging Natural Language Processing to Unravel the Mystery of Life A Review of NLP Approaches in.pdf}
}

@misc{raposoMixtureofDepthsDynamicallyAllocating2024,
  title = {Mixture-of-{{Depths}}: {{Dynamically}} Allocating Compute in Transformer-Based Language Models},
  shorttitle = {Mixture-of-{{Depths}}},
  author = {Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
  year = {2024},
  month = apr,
  number = {arXiv:2404.02258},
  eprint = {2404.02258},
  primaryclass = {cs},
  urldate = {2024-04-07},
  abstract = {Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (\$k\$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-\$k\$ routing mechanism. Since \$k\$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the \$k\$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50{\textbackslash}\% faster to step during post-training sampling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/HZCFWBRL/Raposo et al. - 2024 - Mixture-of-Depths Dynamically allocating compute in transformer-based language models.pdf}
}

@article{rashid2020,
  title = {Monotonic {{Value Function Factorisation}} for {{Deep Multi-Agent Reinforcement Learning}}},
  author = {Rashid, Tabish and Samvelyan, Mikayel and Witt, C. S. D. and Farquhar, Gregory and Foerster, J. and Whiteson, Shimon},
  year = {2020},
  month = mar,
  journal = {ArXiv},
  urldate = {2024-12-10},
  abstract = {In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.},
  file = {/Users/nobr/Zotero/storage/AJEM5AM7/Rashid et al. - 2020 - Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.pdf}
}

@misc{razaviPreventingPosteriorCollapse2019,
  title = {Preventing {{Posterior Collapse}} with Delta-{{VAEs}}},
  author = {Razavi, Ali and van den Oord, A{\"a}ron and Poole, Ben and Vinyals, Oriol},
  year = {2019},
  month = jan,
  number = {arXiv:1901.03416},
  eprint = {1901.03416},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-11-25},
  abstract = {Due to the phenomenon of "posterior collapse," current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed \${\textbackslash}delta\$-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet \$32{\textbackslash}times 32\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/DV8YPIFY/Razavi et al. - 2019 - Preventing Posterior Collapse with delta-VAEs.pdf;/Users/nobr/Zotero/storage/FY9S9DDA/1901.html}
}

@inproceedings{regli2023,
  title = {On {{Automation}} of {{Flight Training}} of {{AI Pilots}}: {{A Formal Language}} to {{Particularize Flight Instruction Scenarios}}},
  shorttitle = {On {{Automation}} of {{Flight Training}} of {{AI Pilots}}},
  booktitle = {2023 {{IEEE}}/{{AIAA}} 42nd {{Digital Avionics Systems Conference}} ({{DASC}})},
  author = {Regli, Christoph and Annighoefer, Bjoern},
  year = {2023},
  month = oct,
  pages = {1--10},
  publisher = {IEEE},
  address = {Barcelona, Spain},
  doi = {10.1109/DASC58513.2023.10311223},
  urldate = {2024-02-15},
  abstract = {Considering development assurance, predictability, explainability and robustness, the lack of trustworthiness has been identified as the key challenge when it comes to the certification of adaptive flight automation systems. Such systems change their characteristics in response to changes in the mission, environment, threats, or failures, which disallows certification according to the currently applicable set of regulations that stipulate a strictly deterministic behaviour. Automation systems are supposed to maintain the status that has been certified. They are not allowed to evolve, to learn from experience, because the risks associated with a changing behaviour are unforeseeable.},
  isbn = {979-8-3503-3357-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IHIU5UJ5/Regli and Annighoefer - 2023 - On Automation of Flight Training of AI Pilots A Formal Language to Particularize Flight Instruction.pdf}
}

@misc{reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.10084},
  urldate = {2023-06-08},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/R6YIXMTN/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/nobr/Zotero/storage/7L9U4C75/1908.html}
}

@article{rere2015,
  title = {Simulated {{Annealing Algorithm}} for {{Deep Learning}}},
  author = {Rere, L.M. Rasdi and Fanany, Mohamad Ivan and Arymurthy, Aniati Murni},
  year = {2015},
  journal = {Procedia Computer Science},
  volume = {72},
  pages = {137--144},
  issn = {18770509},
  doi = {10.1016/j.procs.2015.12.114},
  urldate = {2023-12-13},
  abstract = {Deep learning (DL) is a new area of research in machine learning, in which the objective is moving us closer to the goal of artificial intelligent. This method can learn many levels of abstraction and representation to create a common sense of data such as text, sound and image. Although DL is useful for a variety of tasks, it's hard to train. Some methods in training deep learning to make it optimal have been proposed, including Stochastic Gradient Descent, Conjugate Gradient, Hessian-free optimization, and Krylov Subspace Descent. In this paper, we proposed Simulated Annealing (SA) to improve the performance of Convolution Neural Network (CNN), as an alternative approach for optimal DL using modern optimization technique, i.e. metaheuristic algorithm. MNIST dataset is used to ensure the accuracy and efficiency of the proposed method. Moreover, we also compare our proposed method with the original of CNN. Although there is an increase in computation time, the experiment results show that the proposed method can improve the performance of original CNN.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7QTWSRK3/Rere et al. - 2015 - Simulated Annealing Algorithm for Deep Learning.pdf}
}

@article{resch2019,
  title = {Human-{{Centric Data Science}} for {{Urban Studies}}},
  author = {Resch, Bernd and Szell, Michael},
  year = {2019},
  month = dec,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {8},
  number = {12},
  pages = {584},
  issn = {2220-9964},
  doi = {10.3390/ijgi8120584},
  urldate = {2023-07-04},
  abstract = {Due to the wide-spread use of disruptive digital technologies like mobile phones, cities have transitioned from data-scarce to data-rich environments. As a result, the field of geoinformatics is being reshaped and challenged to develop adequate data-driven methods. At the same time, the term "smart city" is increasingly being applied in urban planning, reflecting the aims of different stakeholders to create value out of the new data sets. However, many smart city research initiatives are promoting techno-positivistic approaches which do not account enough for the citizens' needs. In this paper, we review the state of quantitative urban studies under this new perspective, and critically discuss the development of smart city programs. We conclude with a call for a new anti-disciplinary, human-centric urban data science, and a well-reflected use of technology and data collection in smart city planning. Finally, we introduce the papers of this special issue which focus on providing a more human-centric view on data-driven urban studies, spanning topics from cycling and wellbeing, to mobility and land use.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DWMGPHES/Resch and Szell - 2019 - Human-Centric Data Science for Urban Studies.pdf}
}

@inproceedings{revay2017,
  title = {{{OODA}} Loop in Command \& Control Systems},
  booktitle = {2017 {{Communication}} and {{Information Technologies}} ({{KIT}})},
  author = {Revay, Martin and Liska, Miroslav},
  year = {2017},
  month = oct,
  pages = {1--4},
  publisher = {IEEE},
  address = {Vysoke Tatry},
  doi = {10.23919/KIT.2017.8109463},
  urldate = {2024-01-14},
  abstract = {This paper describes Network Enabled Capability (NEC) within Command \& Control (C2) systems. In the first part we present the need to transform military operations of Industrial Age to Information Age military operations and the related NCW (Network Centric Warfare). In the next part of the paper we discuss OODA (Observe, Orient, Decide, Act) Loop as a means of optimizing NEC within Command \& Control (C2) systems. The aim of the paper is to introduce the issues of the behavior of systems mutually influencing one another.},
  isbn = {978-80-8040-550-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SPMRQVE2/Revay and Liska - 2017 - OODA loop in command & control systems.pdf}
}

@misc{revisedhamasdocumentofgeneralprinciplesandpolicies2017,
  title = {Chart of {{Allah}}: {{The Platform}} of the {{Islamic Resistance Movement}} ({{Hamas}})},
  shorttitle = {Doctrine of {{Hamas}}},
  author = {{Revised Hamas Document of General Principles and Policies}},
  year = {2017},
  publisher = {Harry Truman Research Jerusalem, Israel},
  urldate = {2024-02-21},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4D3P3BFW/doctrine-hamas.html}
}

@book{rey2023,
  title = {Geographic {{Data Science}} with {{Python}}},
  author = {Rey, Sergio and {Arribas-Bel}, Dani and Wolf, Levi John},
  year = {2023},
  month = may,
  edition = {1},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton},
  doi = {10.1201/9780429292507},
  urldate = {2025-08-10},
  isbn = {978-0-429-29250-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/K5URUXCZ/Rey et al. - 2023 - Geographic Data Science with Python.pdf}
}

@inproceedings{reynolds2023,
  title = {Image-Based Material Analysis of Ancient Historical Documents},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Pattern Recognition Applications}} and {{Methods}}},
  author = {Reynolds, Thomas and Dhali, Maruf A. and Schomaker, Lambert},
  year = {2023},
  eprint = {2203.01042},
  primaryclass = {cs},
  pages = {697--706},
  doi = {10.5220/0011743700003411},
  urldate = {2023-11-13},
  abstract = {Researchers continually perform corroborative tests to classify ancient historical documents based on the physical materials of their writing surfaces. However, these tests, often performed on-site, requires actual access to the manuscript objects. The procedures involve a considerable amount of time and cost, and can damage the manuscripts. Developing a technique to classify such documents using only digital images can be very useful and efficient. In order to tackle this problem, this study uses images of a famous historical collection, the Dead Sea Scrolls, to propose a novel method to classify the materials of the manuscripts. The proposed classifier uses the two-dimensional Fourier Transform to identify patterns within the manuscript surfaces. Combining a binary classification system employing the transform with a majority voting process is shown to be effective for this classification task. This pilot study shows a successful classification percentage of up to 97\% for a confined amount of manuscripts produced from either parchment or papyrus material. Feature vectors based on Fourier-space grid representation outperformed a concentric Fourier-space format.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/3R7X5R62/Reynolds et al. - 2023 - Image-based material analysis of ancient historical documents.pdf;/Users/nobr/Zotero/storage/WFUTWKKK/2203.html}
}

@article{riedl2013,
  title = {Interactive {{Narrative}}: {{An Intelligent Systems Approach}}},
  author = {Riedl, Mark O and Bulitko, Vadim},
  year = {2013},
  issn = {0738-4602},
  abstract = {S torytelling, in oral, visual, or written forms, plays a central role in various types of media, including novels, movies, television, and theater. The prevalence of storytelling in human culture may be explained by the use of narrative as a cognitive tool for situated understanding (Gerrig 1993). This narrative intelligence-the ability to organize experience into narrative form-is central to the cognitive processes employed across a range of experiences, from entertainment to active learning. It follows that computational systems possessing narrative intelligence may be able to interact with human users naturally because they understand collaborative contexts as emerging narrative and are able to express themselves by telling stories. A number of narrative intelligence tasks have been studied from a computational perspective including story understanding , story generation, and commonsense reasoning. One of the most compelling applications of narrative intelligence is the prospect of interactive narrative. Interactive narrative is a form of digital interactive experience in which users create or influence a dramatic storyline through actions, either by assuming the role of a character in a fictional virtual world, issuing commands to computer-controlled characters, or directly manipulating the fictional world state. It is most often considered as a form of interactive entertainment but can also be used for serious applications such as education and training. The most common form of interactive narrative involves the user taking on the role of the protagonist in an unfolding storyline. n Interactive narrative is a form of digital interactive experience in which users create or influence a dramatic storyline through their actions. The goal of an interactive narrative system is to immerse users in a virtual world such that they believe that they are an integral part of an unfolding story and that their actions can significantly alter the direction or outcome of the story. In this article we review the ways in which artificial intelligence can be brought to bear on the creation of interactive narrative systems. We lay out the landscape of about 20 years of interactive narrative research and explore the successes as well as open research questions pertaining to the novel use of computational narrative intelligence in the pursuit of entertainment, education, and training.},
  file = {/Users/nobr/Zotero/storage/647QLKWN/2449-Article Text-4130-1-10-20130311-1.pdf}
}

@article{riskin2003,
  title = {The {{Defecating Duck}}, or, the {{Ambiguous Origins}} of {{Artificial Life}}},
  author = {Riskin, Jessica},
  year = {2003},
  journal = {Source: Critical Inquiry},
  volume = {29},
  number = {4},
  pages = {599--633},
  doi = {10.1086/377722},
  file = {/Users/nobr/Zotero/storage/SYGG7ZY6/full-text.pdf}
}

@misc{rivera2024,
  title = {Escalation {{Risks}} from {{Language Models}} in {{Military}} and {{Diplomatic Decision-Making}}},
  author = {Rivera, Juan-Pablo and Mukobi, Gabriel and Reuel, Anka and Lamparth, Max and Smith, Chandler and Schneider, Jacquelyn},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03408},
  eprint = {2401.03408},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.03408},
  urldate = {2024-02-16},
  abstract = {Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models' reported reasonings for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/FUWU34U8/Rivera et al. - 2024 - Escalation Risks from Language Models in Military and Diplomatic Decision-Making.pdf}
}

@article{robbins1941,
  title = {{{ON THE CLASSIFICATION OF THE MAPPINGS OF A}} 2-{{COMPLEX}}},
  author = {Robbins, Herbert},
  year = {1941},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/Y4HM6VQ3/Robbins - ON THE CLASSIFICATION OF THE MAPPINGS OF A 2-COMPLEX.pdf}
}

@article{robbins1952,
  title = {{{SOME ASPECTS OF THE SEQUENTIAL DESIGN OF EXPERIMENTS}}},
  author = {Robbins, Herbert},
  year = {1952},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UGMW65M7/Robbins - SOME ASPECTS OF THE SEQUENTIAL DESIGN OF EXPERIMENTS.pdf}
}

@article{robertson2014,
  title = {A {{Review}} of {{Real}}-{{Time Strategy Game AI}}},
  author = {Robertson, Glen and Watson, Ian},
  year = {2014},
  month = dec,
  journal = {AI Magazine},
  volume = {35},
  number = {4},
  pages = {75--104},
  issn = {0738-4602, 2371-9621},
  doi = {10.1609/aimag.v35i4.2478},
  urldate = {2024-12-04},
  abstract = {This literature review covers AI techniques used for real-time strategy video games, focusing specifically on StarCraft. It finds that the main areas of current academic research are in tactical and strategic decision making, plan recognition, and learning, and it outlines the research contributions in each of these areas. The paper then contrasts the use of game AI in academe and industry, finding the academic research heavily focused on creating game-winning agents, while the industry aims to maximize player enjoyment. It finds that industry adoption of academic research is low because it is either inapplicable or too time-consuming and risky to implement in a new game, which highlights an area for potential investigation: bridging the gap between academe and industry. Finally, the areas of spatial reasoning, multiscale AI, and cooperation are found to require future work, and standardized evaluation methods are proposed to produce comparable results between studies.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/H2XFWW75/Robertson and Watson - 2014 - A Review of Real‐Time Strategy Game AI.pdf}
}

@inproceedings{robertson2015,
  title = {Building Behavior Trees from Observations in Real-Time Strategy Games},
  booktitle = {2015 International Symposium on Innovations in Intelligent {{SysTems}} and Applications ({{INISTA}})},
  author = {Robertson, Glen and Watson, Ian},
  year = {2015},
  pages = {1--7},
  doi = {10.1109/INISTA.2015.7276774},
  file = {/Users/nobr/Zotero/storage/UVAURIQD/Robertson and Watson - 2015 - Building behavior trees from observations in real-time strategy games.pdf}
}

@inproceedings{robertson2021,
  title = {Wait, {{But Why}}?: {{Assessing Behavior Explanation Strategies}} for {{Real-Time Strategy Games}}},
  shorttitle = {Wait, {{But Why}}?},
  booktitle = {26th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Robertson, Justus and Kokkinakis, Athanasios Vasileios and Hook, Jonathan and Kirman, Ben and Block, Florian and Ursu, Marian F and Patra, Sagarika and Demediuk, Simon and Drachen, Anders and Olarewaju, Oluseyi},
  year = {2021},
  month = apr,
  pages = {32--42},
  publisher = {ACM},
  address = {College Station TX USA},
  doi = {10.1145/3397481.3450699},
  urldate = {2023-11-12},
  isbn = {978-1-4503-8017-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/CTQU6GMI/Robertson et al. - 2021 - Wait, But Why Assessing Behavior Explanation Strategies for Real-Time Strategy Games.pdf}
}

@inproceedings{robinson2016,
  title = {Degree of Automation in Command and Control Decision Support Systems},
  booktitle = {2016 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Robinson, Ryan M. and McCourt, Michael J. and Marathe, Amar R. and Nothwang, William D. and Doucette, Emily A. and Curtis, J. Willard},
  year = {2016},
  month = oct,
  pages = {001184--001190},
  publisher = {IEEE},
  address = {Budapest, Hungary},
  doi = {10.1109/SMC.2016.7844402},
  urldate = {2023-11-19},
  abstract = {This paper investigates the effects of integrating automation into the various stages of information processing in a military command and control scenario. Command and control (C2) is an extreme decision-making paradigm characterized by high uncertainty, high risk, and severe time pressure. We introduce a principled approach to decision support system (DSS) design that specifically addresses these issues. Our approach establishes the principles of communicating confidence in sensor estimates and consequence of actions in an intuitive, timely manner. We hypothesize that automation designed to communicate confidence and/or consequence will improve task performance over systems that neglect these concepts. Toward this end, human-subjects experiments were conducted to compare the effects of displaying confidence/consequence information in a C2 target-tracking and interdiction scenario. Four variations of a decision support interface were designed, each with a distinct ``degree of automation'': (i) an instantaneous sensor measurement visualization (baseline), (ii) a confidence-based visualization, (iii) a confidence- and consequence-based visualization, and (iv) a confidence- and consequence-based visualization with explicit decision recommendations. While increasing automation generally improved results, the inclusion of consequence information did not have a major effect, perhaps because the scenario was overlysimplified.},
  isbn = {978-1-5090-1897-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/Y35KH5NB/Robinson et al. - 2016 - Degree of automation in command and control decision support systems.pdf}
}

@article{rodin1987,
  title = {Artificial Intelligence in Air Combat Games},
  author = {Rodin, E. Y. and Lirov, Y. and Mittnik, S. and McElhaney, B. G. and Wilbur, L.},
  year = {1987},
  month = jan,
  journal = {Computers \& Mathematics with Applications},
  volume = {13},
  number = {1},
  pages = {261--274},
  issn = {0898-1221},
  doi = {10.1016/0898-1221(87)90109-X},
  urldate = {2024-01-15},
  abstract = {A general framework for the utilization of large numbers of optimal pursuit-evasion algorithms, as applied to air combat, is described. The framework is based upon and is driven by artificial intelligence concepts. The method employed involves the valuation of alternative tactical strategies and maneuvers through a goal system and pilot-derived expert data bases. The system is designed to display the most promising strategies to the pilot for a final decision. Two aspects of the concept above are described here: the general framework and a specific implementation for a synthetic method of flight and fire control system optimization. Details of the implementation, based on off-the-shelf hardware and a standard programming lanuage, are also given. Potential utilization of these concepts includes other areas as well: submarine warfare and satellite based weapon systems are two possible additional applications. Nonmilitary applications are air traffic control and optimal scheduling.},
  file = {/Users/nobr/Zotero/storage/A35P45RC/Rodin et al. - 1987 - Artificial intelligence in air combat games.pdf;/Users/nobr/Zotero/storage/S67Q6XUW/089812218790109X.html}
}

@article{rodriguez2020,
  title = {A {{Literary History}} of {{Mental Captivity}} in the {{United States}}. {{{\emph{Blood Meridian}}}}, {{{\emph{Wise Blood}}}}, and {{Contemporary Political Discourse}}},
  author = {Rodriguez, Manuel Broncano},
  year = {2020},
  month = aug,
  journal = {Review of International American Studies},
  volume = {13},
  number = {1},
  pages = {75--97},
  issn = {1991-2773, 1991-2773},
  doi = {10.31261/rias.7623},
  urldate = {2023-09-09},
  abstract = {On July 15, 2018, US President Donald Trump and Russia President~Vladimir Putin held a summit in Helsinki that immediately set off a chain reaction throughout the world. By now, barely two months later, that summit is all but forgotten for the most~part, superseded by the frantic train of events and the subsequent bombardment from the media that have become the ``new normal.'' While the iron secrecy surrounding the conversation between the two dignitaries allowed for all kinds of speculation, the image of president~Trump bowing to his Russian counterpart (indeed a treasure trove for semioticians) became~for many observers in the US and across the world the living proof of Mr. Trump{\textasciiacute}s subservient allegiance to Mr. Putin and his obscure designs. Even some of the most recalcitrant~GOPs vented quite publicly their disgust at the sight of a president paying evident homage~to the archenemy of the United States, as Vercingetorix kneeled down before Julius Cesar~in recognition of the Gaul{\textasciiacute}s surrender to the might of the Roman Empire. For some arcanereason, the whole episode of the Helsinki summit brought to my mind, as in a vivid d{\'e}j{\`a} vu,~Cormac McCarthy{\textasciiacute}s novel Blood Meridian and more specifically, the characters of Judge~Holden and the idiotic freak who becomes Holden{\textasciiacute}s ludicrous disciple in the wastelands~of Arizona. In my presentation, I will provide some possible explanations as to why I came~to blend these two unrelated episodes into a single continuum. In the process, I will briefly~revisit some key texts in the American canon that fully belong in the history of ``mental captivity'' in the United States, yet to be written. Obviously, I am not in hopes of deciphering~the ultimate reasons for current US foreign policy, and the more modest aim of my presentation today is to offer some insights into the general theme of our conference through a novel~and a textual tradition overpopulated with ``captive minds.''},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HEWZC4FD/Rodriguez - 2020 - a Literary History of Mental Captivity in the Unit.pdf}
}

@incollection{rogelj2018mitigation,
  title = {Mitigation Pathways Compatible with 1.5{$^\circ$}{{C}} in the Context of Sustainable Development},
  booktitle = {Global {{Warming}} of 1.5{$^\circ$}{{C}}. {{An IPCC Special Report}} on the Impacts of Global Warming of 1.5{$^\circ$}{{C}} above Pre-Industrial Levels and Related Global Greenhouse Gas Emission Pathways, in the Context of Strengthening the Global Response to the Threat of Climate Change, Sustainable Development, and Efforts to Eradicate Poverty},
  author = {Rogelj, J. and Shindell, D. and Jiang, K. and Fifita, S. and Forster, P. and Ginzburg, V. and Handa, C. and Kheshgi, H. and Kobayashi, S. and Kriegler, E. and Mundaca, L. and S{\'e}f{\'e}rian, R. and Vilari{\~n}o, M. V.},
  editor = {{Masson-Delmotte}, V. and Zhai, P. and P{\"o}rtner, H.-O. and Roberts, D. and Skea, J. and Shukla, P. R. and Pirani, A. and {Moufouma-Okia}, W. and P{\'e}an, C. and Pidcock, R. and Connors, S. and Matthews, J. B. R. and Chen, Y. and Zhou, X. and Gomis, M. I. and Lonnoy, E. and Maycock, T. and Tignor, M. and Waterfield, T.},
  year = {2018},
  pages = {93--174},
  publisher = {IPCC},
  address = {Geneva, Switzerland}
}

@article{rokem2020,
  title = {Fractional Ridge Regression: A Fast, Interpretable Reparameterization of Ridge Regression},
  shorttitle = {Fractional Ridge Regression},
  author = {Rokem, Ariel and Kay, Kendrick},
  year = {2020},
  month = nov,
  journal = {GigaScience},
  volume = {9},
  number = {12},
  pages = {giaa133},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giaa133},
  urldate = {2023-05-11},
  abstract = {Ridge regression is a regularization technique that penalizes the L2-norm of the coefficients in linear regression. One of the challenges of using ridge regression is the need to set a hyperparameter ({$\alpha$}) that controls the amount of regularization. Cross-validation is typically used to select the best {$\alpha$} from a set of candidates. However, efficient and appropriate selection of {$\alpha$} can be challenging. This becomes prohibitive when large amounts of data are analyzed. Because the selected {$\alpha$} depends on the scale of the data and correlations across predictors, it is also not straightforwardly interpretable.The present work addresses these challenges through a novel approach to ridge regression. We propose to reparameterize ridge regression in terms of the ratio {$\gamma$} between the L2-norms of the regularized and unregularized coefficients. We provide an algorithm that efficiently implements this approach, called fractional ridge regression, as well as open-source software implementations in Python and matlab (https://github.com/nrdg/fracridge). We show that the proposed method is fast and scalable for large-scale data problems. In brain imaging data, we demonstrate that this approach delivers results that are straightforward to interpret and compare across models and datasets.Fractional ridge regression has several benefits: the solutions obtained for different {$\gamma$} are guaranteed to vary, guarding against wasted calculations; and automatically span the relevant range of regularization, avoiding the need for arduous manual exploration. These properties make fractional ridge regression particularly suitable for analysis of large complex datasets.},
  file = {/Users/nobr/Zotero/storage/ENRMRZNJ/Rokem and Kay - 2020 - Fractional ridge regression a fast, interpretable reparameterization of ridge regression.pdf}
}

@misc{rosasSoftwareNaturalWorld2024,
  title = {Software in the Natural World: {{A}} Computational Approach to Hierarchical Emergence},
  shorttitle = {Software in the Natural World},
  author = {Rosas, Fernando E. and Geiger, Bernhard C. and Luppi, Andrea I. and Seth, Anil K. and Polani, Daniel and Gastpar, Michael and Mediano, Pedro A. M.},
  year = {2024},
  month = jun,
  number = {arXiv:2402.09090},
  eprint = {2402.09090},
  primaryclass = {nlin},
  doi = {10.48550/arXiv.2402.09090},
  urldate = {2024-06-11},
  abstract = {Understanding the functional architecture of complex systems is crucial to illuminate their inner workings and enable effective methods for their prediction and control. Recent advances have introduced tools to characterise emergent macroscopic levels; however, while these approaches are successful in identifying when emergence takes place, they are limited in the extent they can determine how it does. Here we address this limitation by developing a computational approach to emergence, which characterises macroscopic processes in terms of their computational capabilities. Concretely, we articulate a view on emergence based on how software works, which is rooted on a mathematical formalism that articulates how macroscopic processes can express self-contained informational, interventional, and computational properties. This framework establishes a hierarchy of nested self-contained processes that determines what computations take place at what level, which in turn delineates the functional architecture of a complex system. This approach is illustrated on paradigmatic models from the statistical physics and computational neuroscience literature, which are shown to exhibit macroscopic processes that are akin to software in human-engineered systems. Overall, this framework enables a deeper understanding of the multi-level structure of complex systems, revealing specific ways in which they can be efficiently simulated, predicted, and controlled.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  file = {/Users/nobr/Zotero/storage/QY8AY9EP/Rosas et al. - 2024 - Software in the natural world A computational approach to hierarchical emergence.pdf}
}

@book{rosen2019,
  title = {Discrete Mathematics and Its Applications},
  author = {Rosen, Kenneth H.},
  year = {2019},
  edition = {Eighth edition},
  publisher = {McGraw-Hill},
  address = {New York, NY},
  isbn = {978-1-259-67651-2},
  langid = {english},
  lccn = {QA39.3 .R67 2019},
  keywords = {Computer science,Mathematics},
  file = {/Users/nobr/Zotero/storage/JCGZQBWF/Rosen - 2019 - Discrete mathematics and its applications.pdf}
}

@inbook{rota1997,
  title = {Ten {{Lessons I}} Wish {{I Had Been Taught}}},
  booktitle = {Indiscrete {{Thoughts}}},
  author = {Rota, Gian-Carlo},
  year = {1997},
  pages = {195--203},
  publisher = {Birkh{\"a}user Boston},
  address = {Boston, MA},
  doi = {10.1007/978-0-8176-4781-0_18},
  urldate = {2024-09-05},
  collaborator = {Rota, Gian-Carlo},
  isbn = {978-0-8176-4780-3 978-0-8176-4781-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/EC89XREY/comm-rota.pdf}
}

@misc{rota2030,
  title = {Rota 2030 - Mobility and Logistics Program}
}

@article{roth2022,
  title = {Natural Scene Sampling Reveals Reliable Coarse-Scale Orientation Tuning in Human {{V1}}},
  author = {Roth, Zvi N. and Kay, Kendrick and Merriam, Elisha P.},
  year = {2022},
  month = oct,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {6469},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-34134-7},
  urldate = {2023-05-10},
  abstract = {Orientation selectivity in primate visual cortex is organized into cortical columns. Since cortical columns are at a finer spatial scale than the sampling resolution of standard BOLD fMRI measurements, analysis approaches have been proposed to peer past these spatial resolution limitations. It was recently found that these methods are predominantly sensitive to stimulus vignetting - a form of selectivity arising from an interaction of the oriented stimulus with the aperture edge. Beyond vignetting, it is not clear whether orientation-selective neural responses are detectable in BOLD measurements. Here, we leverage a dataset of visual cortical responses measured using high-field 7T fMRI. Fitting these responses using image-computable models, we compensate for vignetting and nonetheless find reliable tuning for orientation. Results further reveal a coarse-scale map of orientation preference that may constitute the neural basis for known perceptual anisotropies. These findings settle a long-standing debate in human neuroscience, and provide insights into functional organization principles of visual cortex.},
  copyright = {2022 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
  langid = {english},
  keywords = {Neural encoding,Striate cortex},
  file = {/Users/nobr/Zotero/storage/9Q2MEPGH/Roth et al. - 2022 - Natural scene sampling reveals reliable coarse-sca.pdf}
}

@article{rowe2000a,
  title = {{\emph{Jorge }}{{{\emph{Luis Borges}}}}{\emph{: }}{{{\emph{Collected Fictions}}}} . {{Translated}} by {{Andrew Hurley}}. {{Pp}}. Ix+565. {{London}}: {{Allen Lane}}, {{The Penguin Press}}, 1999. {{Hb}}. {\pounds}20.},
  shorttitle = {{\emph{Jorge }}{{{\emph{Luis Borges}}}}},
  author = {Rowe, William},
  year = {2000},
  month = sep,
  journal = {Translation and Literature},
  volume = {9},
  number = {2},
  pages = {272--276},
  issn = {0968-1361, 1750-0214},
  doi = {10.3366/tal.2000.9.2.272},
  urldate = {2024-05-28},
  copyright = {https://www.euppublishing.com/customer-services/librarians/text-and-data-mining-tdm},
  langid = {english}
}

@inproceedings{ruifeng2019,
  title = {Research Progress and Application of Behavior Tree Technology},
  booktitle = {2019 6th International Conference on Behavioral, Economic and Socio-Cultural Computing ({{BESC}})},
  author = {Ruifeng, Liu and Jiasheng, Wang and Haolong, Zhang and Mengfan, Tian},
  year = {2019},
  pages = {1--4},
  doi = {10.1109/BESC48373.2019.8963263}
}

@misc{ruoss2024,
  title = {Grandmaster-{{Level Chess Without Search}}},
  author = {Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Medapati, Sourabh and {Grau-Moya}, Jordi and Wenliang, Li Kevin and Catt, Elliot and Reid, John and Genewein, Tim},
  year = {2024},
  month = feb,
  number = {arXiv:2402.04494},
  eprint = {2402.04494},
  doi = {10.48550/arXiv.2402.04494},
  urldate = {2024-10-18},
  abstract = {The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/WTWSFEBM/Ruoss et al. - 2024 - Grandmaster-Level Chess Without Search.pdf}
}

@book{ruppert2015,
  title = {Statistics and {{Data Analysis}} for {{Financial Engineering}}: With {{R}} Examples},
  shorttitle = {Statistics and {{Data Analysis}} for {{Financial Engineering}}},
  author = {Ruppert, David and Matteson, David S.},
  year = {2015},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4939-2614-5},
  urldate = {2024-01-26},
  isbn = {978-1-4939-2613-8 978-1-4939-2614-5},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4YR73Y7W/Ruppert and Matteson - 2015 - Statistics and Data Analysis for Financial Engineering with R examples.pdf}
}

@techreport{rush2018,
  title = {The {{Annotated Transformer}}},
  author = {Rush, Alexander M},
  year = {2018},
  pages = {52--60},
  abstract = {A major aim of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for replication, it still may be difficult to achieve good results in practice. This paper is an experiment. In it, I consider a worked exercise with the goal of implementing the results of the recent paper. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system. An implicit premise of this exercise is to encourage researchers to consider this method for new results.},
  file = {/Users/nobr/Zotero/storage/F77CB9GE/full-text.pdf}
}

@book{russell2016,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2016},
  series = {Prentice {{Hall}} Series in Artificial Intelligence},
  edition = {Third edition, Global edition},
  publisher = {Pearson},
  address = {Boston Columbus Indianapolis New York San Francisco Upper Saddle River Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo},
  collaborator = {Davis, Ernest and Edwards, Douglas},
  isbn = {978-0-13-604259-4 978-1-292-15396-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/FGRL9GD2/Russell and Norvig - 2016 - Artificial intelligence a modern approach.pdf}
}

@misc{rutherford2023,
  title = {{{JaxMARL}}: {{Multi-Agent RL Environments}} in {{JAX}}},
  shorttitle = {{{JaxMARL}}},
  author = {Rutherford, Alexander and Ellis, Benjamin and Gallici, Matteo and Cook, Jonathan and Lupu, Andrei and Ingvarsson, Gardar and Willi, Timon and Khan, Akbir and {de Witt}, Christian Schroeder and Souly, Alexandra and Bandyopadhyay, Saptarashmi and Samvelyan, Mikayel and Jiang, Minqi and Lange, Robert Tjarko and Whiteson, Shimon and Lacerda, Bruno and Hawes, Nick and Rocktaschel, Tim and Lu, Chris and Foerster, Jakob Nicolaus},
  year = {2023},
  month = dec,
  number = {arXiv:2311.10090},
  eprint = {2311.10090},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.10090},
  urldate = {2024-01-02},
  abstract = {Benchmarks play an important role in the development of machine learning algorithms. For example, research in reinforcement learning (RL) has been heavily influenced by available environments and benchmarks. However, RL environments are traditionally run on the CPU, limiting their scalability with typical academic compute. Recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles, enabling massively parallel RL training pipelines and environments. This is particularly useful for multi-agent reinforcement learning (MARL) research. First of all, multiple agents must be considered at each environment step, adding computational burden, and secondly, the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges. In this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms. When considering wall clock time, our experiments show that per-run our JAX-based training pipeline is up to 12500x faster than existing approaches. This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. We provide code at https://github.com/flairox/jaxmarl.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/5TK2BSKU/Rutherford et al. - 2023 - JaxMARL Multi-Agent RL Environments in JAX.pdf;/Users/nobr/Zotero/storage/F5X597ID/2311.html}
}

@article{ryseff2022,
  title = {Small Is Beautiful},
  author = {Ryseff, James and Bond, Michael},
  year = {2022},
  month = may,
  journal = {The Journal of Defense Modeling and Simulation: Applications, Methodology, Technology},
  pages = {154851292210964},
  issn = {1548-5129, 1557-380X},
  doi = {10.1177/15485129221096478},
  urldate = {2023-11-12},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DBWGANC9/Ryseff and Bond - 2022 - Small is beautiful.pdf}
}

@misc{ryskinaLearningMathematicalProperties2021,
  title = {Learning {{Mathematical Properties}} of {{Integers}}},
  author = {Ryskina, Maria and Knight, Kevin},
  year = {2021},
  month = sep,
  number = {arXiv:2109.07230},
  eprint = {2109.07230},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.07230},
  urldate = {2024-01-22},
  abstract = {Embedding words in high-dimensional vector spaces has proven valuable in many natural language applications. In this work, we investigate whether similarly-trained embeddings of integers can capture concepts that are useful for mathematical applications. We probe the integer embeddings for mathematical knowledge, apply them to a set of numerical reasoning tasks, and show that by learning the representations from mathematical sequence data, we can substantially improve over number embeddings learned from English text corpora.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/I2CXWVGX/Ryskina and Knight - 2021 - Learning Mathematical Properties of Integers.pdf;/Users/nobr/Zotero/storage/TY6EUDI9/2109.html}
}

@inproceedings{saad2021,
  title = {{{SPPL}}: Probabilistic Programming with Fast Exact Symbolic Inference},
  shorttitle = {{{SPPL}}},
  booktitle = {Proceedings of the 42nd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Saad, Feras A. and Rinard, Martin C. and Mansinghka, Vikash K.},
  year = {2021},
  month = jun,
  pages = {804--819},
  publisher = {ACM},
  address = {Virtual Canada},
  doi = {10.1145/3453483.3454078},
  urldate = {2024-10-31},
  isbn = {978-1-4503-8391-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/WADKRISY/Saad et al. - 2021 - SPPL probabilistic programming with fast exact symbolic inference.pdf}
}

@article{sadler2011,
  title = {An Application of the Edge Effect in Measuring Accessibility to Multiple Food Retailer Types in {{Southwestern Ontario}}, {{Canada}}},
  author = {Sadler, Richard C and Gilliland, Jason A and Arku, Godwin},
  year = {2011},
  month = dec,
  journal = {International Journal of Health Geographics},
  volume = {10},
  number = {1},
  pages = {34},
  issn = {1476-072X},
  doi = {10.1186/1476-072X-10-34},
  urldate = {2023-07-06},
  abstract = {Background: Trends in food retailing associated with the consolidation of smaller-format retailers into fewer, larger-format supercentres have left some rural areas with fewer sources of nutritious, affordable food. Access to nutritious, affordable food is essential for good dietary habits and combating health issues such as type-2 diabetes, obesity, and cardiovascular disease. Many studies on food environments use inaccurate or incomplete methods for locating food retailers, which may be responsible for mischaracterising food deserts. This study uses databases of every residence in and every food retailer in and around Middlesex County, Ontario, Canada. Residences were geocoded to their precise address, and network analysis techniques were performed in a geographic information system (GIS) to determine distances between every residence and different types of food retailers (grocery stores, fast food, fruit and vegetable sources, grocery stores plus fruit and vegetable sources, variety stores), both when considering and neglecting facilities outside the area of study, to account for a deficiency in analysis termed the `edge effect'. Results: Analysis of household accessibility to food outlets by neighbourhood socioeconomic distress level indicated that residents in the most distressed neighbourhoods tended to have better accessibility to all types of food retailers. In the most distressed neighbourhoods, 79 percent of residences were within walking distance of a grocery store, compared to only 10 percent in the least distressed neighbourhoods. When the edge effect was neglected, 37 percent of distance estimates proved inaccurate. Average accessibility to all food retailer types improved dramatically when food outlets adjacent to the study area were considered, thereby controlling for the edge effect. Conclusion: By neglecting to consider food retailers just outside study area boundaries, previous studies may significantly over-report the actual distance necessary to travel for food. Research on food access spanning large rural regions requires methods that accurately geocode residents and their food sources. By implementing methods akin to those in this paper, future research will be better able to identify areas with poor food accessibility. Improving identification of food desert communities is a first step in facilitating more effective deployment of food policies and programs in those communities.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/8IY3H4P2/Sadler et al. - 2011 - An application of the edge effect in measuring acc.pdf}
}

@inproceedings{sagredo-olivenza2017,
  title = {Combining Neural Networks for Controlling Non-Player Characters in Games},
  booktitle = {International Work-Conference on Artificial Neural Networks},
  author = {{Sagredo-Olivenza}, Ismael and {G{\'o}mez-Mart{\'i}n}, Pedro Pablo and {G{\'o}mez-Mart{\'i}n}, Marco Antonio and {Gonz{\'a}lez-Calero}, Pedro Antonio},
  year = {2017},
  pages = {694--705},
  publisher = {Springer}
}

@misc{salimansEvolutionStrategiesScalable2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  month = sep,
  number = {arXiv:1703.03864},
  eprint = {1703.03864},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-11-22},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/ZIYKCZ3T/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/JVA4E5J6/1703.html}
}

@article{samcovic2018,
  title = {Serious Games in Military Applications},
  author = {Sam{\v c}ovi{\'c}, Andreja},
  year = {2018},
  journal = {Vojnotehnicki glasnik},
  volume = {66},
  number = {3},
  pages = {597--613},
  issn = {0042-8469, 2217-4753},
  doi = {10.5937/vojtehg66-16367},
  urldate = {2024-09-09},
  abstract = {Serious games as one of the most important trends in e-learning are presented in this paper. An intensive use of information and communication technologies has led to major changes in traditional military education. One of these changes is the use of serious games for simulating the real military environment. This review paper presents several definitions and classifications of serious games, the difference between serious and entertainment games, and considers their military applications in training and simulation systems as well as in education.},
  copyright = {http://creativecommons.org/licenses/BY/4.0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/NFYR4SHH/Samčović - 2018 - Serious games in military applications.pdf}
}

@misc{samvelyan2019,
  title = {The {{StarCraft Multi-Agent Challenge}}},
  author = {Samvelyan, Mikayel and Rashid, Tabish and {de Witt}, Christian Schroeder and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
  year = {2019},
  month = dec,
  number = {arXiv:1902.04043},
  eprint = {1902.04043},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.04043},
  urldate = {2024-01-14},
  abstract = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ\_obZ0.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/JWYP3B5G/Samvelyan et al. - 2019 - The StarCraft Multi-Agent Challenge.pdf;/Users/nobr/Zotero/storage/CMZZBG8K/1902.html}
}

@misc{samvelyan2024,
  title = {Rainbow {{Teaming}}: {{Open-Ended Generation}} of {{Diverse Adversarial Prompts}}},
  shorttitle = {Rainbow {{Teaming}}},
  author = {Samvelyan, Mikayel and Raparthy, Sharath Chandra and Lupu, Andrei and Hambro, Eric and Markosyan, Aram H. and Bhatt, Manish and Mao, Yuning and Jiang, Minqi and {Parker-Holder}, Jack and Foerster, Jakob and Rockt{\"a}schel, Tim and Raileanu, Roberta},
  year = {2024},
  month = dec,
  number = {arXiv:2402.16822},
  eprint = {2402.16822},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16822},
  urldate = {2025-09-10},
  abstract = {As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90\% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/7G5NFPE2/Samvelyan et al. - 2024 - Rainbow Teaming Open-Ended Generation of Diverse Adversarial Prompts.pdf;/Users/nobr/Zotero/storage/WFKPR3PM/2402.html}
}

@misc{sanborn2024,
  title = {Beyond {{Euclid}}: {{An Illustrated Guide}} to {{Modern Machine Learning}} with {{Geometric}}, {{Topological}}, and {{Algebraic Structures}}},
  shorttitle = {Beyond {{Euclid}}},
  author = {Sanborn, Sophia and Mathe, Johan and Papillon, Mathilde and Buracas, Domas and Lillemark, Hansen J. and Shewmake, Christian and Bertics, Abby and Pennec, Xavier and Miolane, Nina},
  year = {2024},
  month = jul,
  number = {arXiv:2407.09468},
  eprint = {2407.09468},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2407.09468},
  urldate = {2024-07-15},
  abstract = {The enduring legacy of Euclidean geometry underpins classical machine learning, which, for decades, has been primarily developed for data lying in Euclidean space. Yet, modern machine learning increasingly encounters richly structured data that is inherently nonEuclidean. This data can exhibit intricate geometric, topological and algebraic structure: from the geometry of the curvature of space-time, to topologically complex interactions between neurons in the brain, to the algebraic transformations describing symmetries of physical systems. Extracting knowledge from such non-Euclidean data necessitates a broader mathematical perspective. Echoing the 19th-century revolutions that gave rise to non-Euclidean geometry, an emerging line of research is redefining modern machine learning with non-Euclidean structures. Its goal: generalizing classical methods to unconventional data types with geometry, topology, and algebra. In this review, we provide an accessible gateway to this fast-growing field and propose a graphical taxonomy that integrates recent advances into an intuitive unified framework. We subsequently extract insights into current challenges and highlight exciting opportunities for future development in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/IILFGCJ3/Sanborn et al. - 2024 - Beyond Euclid An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures.pdf}
}

@article{sanchez-lozano2014,
  title = {Identification and Selection of Potential Sites for Onshore Wind Farms Development in {{Region}} of {{Murcia}}, {{Spain}}},
  author = {{S{\'a}nchez-Lozano}, J.M. and {Garc{\'i}a-Cascales}, M.S. and Lamata, M.T.},
  year = {2014},
  month = aug,
  journal = {Energy},
  volume = {73},
  pages = {311--324},
  issn = {03605442},
  doi = {10.1016/j.energy.2014.06.024},
  urldate = {2023-07-13},
  abstract = {It is often advisable to combine spatial representation tools such as Geographic Information Systems (GIS) with Multi criteria Decision Making Methods (MCDM) when solving location complex problems. The current case refers to the search for and selection of sites for onshore wind farms on the coast of the Region of Murcia, in the southeast of Spain. When resolving the proposed problem, the legal restrictions and the criteria (wind speed, area, slope, etc.) that influence the location will be considered. These will be defined in the form of thematic layers that will be entered into the GIS. Restrictions will be imposed taking into account the legislative framework of the study area so that, through their analysis and editing, it will be possible to reduce the initial area and obtain suitable sites where this type of facilities can be installed. Moreover, as the objective of the study is to select the locations and obtain a ranking two different models will be applied, initially a categorical assessment through a lexicographic order will be performed using the tools available in the GIS and, later it will be applied the ELECTRE-TRI methodology will be applied in order to make a comparison between the methods.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LR9MMT2I/Sánchez-Lozano et al. - 2014 - Identification and selection of potential sites fo.pdf}
}

@book{sanchez-marre2022,
  title = {Intelligent {{Decision Support Systems}}},
  author = {{S{\`a}nchez-Marr{\`e}}, Miquel},
  year = {2022},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87790-3},
  urldate = {2024-01-08},
  isbn = {978-3-030-87789-7 978-3-030-87790-3},
  langid = {english},
  keywords = {Data-Driven Models in IDSS,Intelligent Decision Support Systems,Intelligent Models,Intelligent Techniques in Decision Making,Modeling Decision Processes,Recommender Systems,Uncertainty Models in Artificial Intelligence,Validating IDSS in complex systems},
  file = {/Users/nobr/Zotero/storage/7MGC45KG/Sànchez-Marrè - 2022 - Intelligent Decision Support Systems.pdf}
}

@article{sangwan2020,
  title = {Neuromorphic Nanoelectronic Materials},
  author = {Sangwan, Vinod K. and Hersam, Mark C.},
  year = {2020},
  month = jul,
  journal = {Nature Nanotechnology},
  volume = {15},
  number = {7},
  pages = {517--528},
  publisher = {Nature Publishing Group},
  issn = {1748-3395},
  doi = {10.1038/s41565-020-0647-z},
  urldate = {2024-02-02},
  abstract = {Memristive and nanoionic devices have recently emerged as leading candidates for neuromorphic computing architectures. While top-down fabrication based on conventional bulk materials has enabled many early neuromorphic devices and circuits, bottom-up approaches based on low-dimensional nanomaterials have shown novel device functionality that often better mimics a biological neuron. In addition, the chemical, structural and compositional tunability of low-dimensional nanomaterials coupled with the permutational flexibility enabled by van der Waals heterostructures offers significant opportunities for artificial neural networks. In this Review, we present a critical survey of emerging neuromorphic devices and architectures enabled by quantum dots, metal nanoparticles, polymers, nanotubes, nanowires, two-dimensional layered materials and van der Waals heterojunctions with a particular emphasis on bio-inspired device responses that are uniquely enabled by low-dimensional topology, quantum confinement and interfaces. We also provide a forward-looking perspective on the opportunities and challenges of neuromorphic nanoelectronic materials in comparison with more mature technologies based on traditional bulk electronic materials.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Electronic properties and materials},
  file = {/Users/nobr/Zotero/storage/FZG9VUI9/Sangwan and Hersam - 2020 - Neuromorphic nanoelectronic materials.pdf}
}

@article{santannadesousagomes2019,
  title = {Proposal of a Methodology to Use Offshore Wind Energy on the Southeast Coast of {{Brazil}}},
  author = {Sant'Anna De Sousa Gomes, Mateus and Faulstich De Paiva, Jane Maria and Aparecida Da Silva Moris, Virg{\'i}nia and Nunes, Andr{\'e}a Oliveira},
  year = {2019},
  month = oct,
  journal = {Energy},
  volume = {185},
  pages = {327--336},
  issn = {03605442},
  doi = {10.1016/j.energy.2019.07.057},
  urldate = {2023-07-13},
  abstract = {The electric energy matrix in Brazil mostly comprises energy from hydroelectric power plants, approximately 60.8\%. This large participation presents some vulnerability in periods of drought and often the country's electric energy matrix is complemented by the use of other energy sources, such as thermoelectric. Even though it is currently among the largest onshore wind power producers in the world, Brazil has not yet started developing this energy source in the offshore segment. However, this study shows that offshore wind energy can be a complementary energy source for the country's electric energy matrix, and thus replace other energy sources which have more impact on the environment. Therefore, this study sought to develop a methodology that can be easily replicated using free access tools, aiming to contribute to the development of offshore wind technology and future research in the country. The results show a large production of offshore wind energy in the southeastern region of Brazil. The "Cabo Frio 200 data collection point presented the highest annual production of offshore wind energy. However the study opted for the "Arraial do Cabo-A60600 collection point located in the state of Rio de Janeiro and at ocean depths that can be reached by fixed foundations.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5CMB2SWS/Sant’Anna De Sousa Gomes et al. - 2019 - Proposal of a methodology to use offshore wind ene.pdf}
}

@article{saragih2022,
  title = {Effects of Cognitive Stimulation Therapy for People with Dementia: {{A}} Systematic Review and Meta-Analysis of Randomized Controlled Studies},
  shorttitle = {Effects of Cognitive Stimulation Therapy for People with Dementia},
  author = {Saragih, Ita Daryanti and Tonapa, Santo Imanuel and Saragih, Ice Septriani and Lee, Bih-O},
  year = {2022},
  month = apr,
  journal = {International Journal of Nursing Studies},
  volume = {128},
  pages = {104181},
  issn = {00207489},
  doi = {10.1016/j.ijnurstu.2022.104181},
  urldate = {2023-10-20},
  abstract = {Background: Cognitive stimulation therapy (CST) has been used to improve cognitive function and reduce negative emotions. However, the efficacy of CST among the dementia population remains inconclusive. Aim: To analyze the efficacy of the CST among people with dementia. Design: Systematic review and meta-analysis. Methods: A systematic literature search was performed using the Academic Search Complete, CINAHL, EMBASE, MEDLINE, PubMed, OVID (UpToDate), and Web of Science databases from the inception to October 18, 2021. The revised Cochrane risk of bias tool for randomized trials was used to assess the methodological quality of the included studies. A meta-analysis was performed using a random-effects model to calculate the pooled effects of CST. Stata 16.0 was used for statistical analysis. Results: A total of 26 studies were included. Overall, CST increased cognitive function (standardized mean difference [SMD]: 0.97; 95\% confidence interval [CI]: 0.66 to 1.28) and decreased depression (SMD: -0.18; 95\% CI: -0.33 to -0.04). No significant effects were found for neuropsychiatric symptoms. Conclusions: Cognitive stimulation therapy effectively improves cognitive function and alleviates depression levels among people with mild-to-moderate dementia. Futures studies can consider a protocol combined with a rigorous study design to address the effects of CST.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/I7XVK7CV/Saragih et al. - 2022 - Effects of cognitive stimulation therapy for people with dementia A systematic review and meta-anal.pdf}
}

@article{sarch2023,
  title = {Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models},
  author = {Sarch, Gabriel and Wu, Yue and Tarr, Michael J and Fragkiadaki, Katerina},
  year = {2023},
  journal = {arXiv preprint arXiv:2310.15127},
  eprint = {2310.15127},
  archiveprefix = {arXiv}
}

@inproceedings{sarkar2012,
  title = {Low {{Distortion Delaunay Embedding}} of {{Trees}} in {{Hyperbolic Plane}}},
  booktitle = {Graph {{Drawing}}},
  author = {Sarkar, Rik},
  editor = {{van Kreveld}, Marc and Speckmann, Bettina},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {355--366},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-25878-7_34},
  abstract = {This paper considers the problem of embedding trees into the hyperbolic plane. We show that any tree can be realized as the Delaunay graph of its embedded vertices. Particularly, a weighted tree can be embedded such that the weight on each edge is realized as the hyperbolic distance between its embedded vertices. Thus the embedding preserves the metric information of the tree along with its topology. The distance distortion between non adjacent vertices can be made arbitrarily small -- less than a (1\,+\,{$\varepsilon$}) factor for any given {$\varepsilon$}. Existing results on low distortion of embedding discrete metrics into trees carry over to hyperbolic metric through this result. The Delaunay character implies useful properties such as guaranteed greedy routing and realization as minimum spanning trees.},
  isbn = {978-3-642-25878-7},
  langid = {english},
  keywords = {Hyperbolic Plane,Minimum Span Tree,Voronoi Cell,Voronoi Diagram,Weighted Tree},
  file = {/Users/nobr/Zotero/storage/6ZCVT848/Sarkar - 2012 - Low Distortion Delaunay Embedding of Trees in Hype.pdf}
}

@misc{sarkar2021,
  title = {Procedural {{Content Generation}} Using {{Behavior Trees}} ({{PCGBT}})},
  author = {Sarkar, Anurag and Cooper, Seth},
  year = {2021},
  month = oct,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Behavior trees (BTs) are a popular method for modeling NPC and enemy AI behavior and have been widely used in commercial games. In this work, rather than use BTs to model game playing agents, we use them for modeling game design agents, defining behaviors as content generation tasks rather than in-game actions. Similar to how traditional BTs enable modeling behaviors in a modular and dynamic manner, BTs for PCG enable simple subtrees for generating parts of levels to be combined modularly to form complex trees for generating whole levels as well as generators that can dynamically vary the generated content. We refer to this approach as Procedural Content Generation using Behavior Trees, or PCGBT, and demonstrate it by using BTs to model generators for Super Mario Bros., Mega Man and Metroid levels as well as dungeon layouts and discuss several ways in which this paradigm could be applied and extended in the future.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/DZQBTF59/Sarkar and Cooper - 2021 - Procedural Content Generation using Behavior Trees (PCGBT).pdf}
}

@inproceedings{sarkar2022,
  title = {Is {{Explainable AI}} a {{Race Against Model Complexity}}?},
  booktitle = {Joint {{Proceedings}} of the {{IUI}} 2022 {{Workshops}}: {{APEx-UI}}, {{HAI-GEN}}, {{HEALTHI}}, {{HUMANIZE}}, {{TExSS}}, {{SOCIALIZE}}},
  author = {Sarkar, Advait},
  editor = {{Smith-Renner}, Alison and Amir, Ofra},
  year = {2022},
  month = mar,
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3124},
  pages = {192--199},
  publisher = {CEUR},
  address = {Virtual Event, Helsinki},
  issn = {1613-0073},
  urldate = {2024-02-13},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/EK5HCURV/Sarkar - 2022 - Is Explainable AI a Race Against Model Complexity.pdf}
}

@misc{satoEmbarrassinglySimpleText2023,
  title = {Embarrassingly {{Simple Text Watermarks}}},
  author = {Sato, Ryoma and Takezawa, Yuki and Bao, Han and Niwa, Kenta and Yamada, Makoto},
  year = {2023},
  month = oct,
  number = {arXiv:2310.08920},
  eprint = {2310.08920},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-24},
  abstract = {We propose Easymark, a family of embarrassingly simple yet effective watermarks. Text watermarking is becoming increasingly important with the advent of Large Language Models (LLM). LLMs can generate texts that cannot be distinguished from human-written texts. This is a serious problem for the credibility of the text. Easymark is a simple yet effective solution to this problem. Easymark can inject a watermark without changing the meaning of the text at all while a validator can detect if a text was generated from a system that adopted Easymark or not with high credibility. Easymark is extremely easy to implement so that it only requires a few lines of code. Easymark does not require access to LLMs, so it can be implemented on the user-side when the LLM providers do not offer watermarked LLMs. In spite of its simplicity, it achieves higher detection accuracy and BLEU scores than the state-of-the-art text watermarking methods. We also prove the impossibility theorem of perfect watermarking, which is valuable in its own right. This theorem shows that no matter how sophisticated a watermark is, a malicious user could remove it from the text, which motivate us to use a simple watermark such as Easymark. We carry out experiments with LLM-generated texts and confirm that Easymark can be detected reliably without any degradation of BLEU and perplexity, and outperform state-of-the-art watermarks in terms of both quality and reliability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/99CPAQSV/Sato et al. - 2023 - Embarrassingly Simple Text Watermarks.pdf}
}

@misc{satorras2021,
  title = {Neural {{Enhanced Belief Propagation}} on {{Factor Graphs}}},
  author = {Satorras, Victor Garcia and Welling, Max},
  year = {2021},
  month = mar,
  number = {arXiv:2003.01998},
  eprint = {2003.01998},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-09-11},
  abstract = {A graphical model is a structured representation of locally dependent random variables. A traditional method to reason over these random variables is to perform inference using belief propagation. When provided with the true data generating process, belief propagation can infer the optimal posterior probability estimates in tree structured factor graphs. However, in many cases we may only have access to a poor approximation of the data generating process, or we may face loops in the factor graph, leading to suboptimal estimates. In this work we first extend graph neural networks to factor graphs (FG-GNN). We then propose a new hybrid model that runs conjointly a FG-GNN with belief propagation. The FG-GNN receives as input messages from belief propagation at every inference iteration and outputs a corrected version of them. As a result, we obtain a more accurate algorithm that combines the benefits of both belief propagation and graph neural networks. We apply our ideas to error correction decoding tasks, and we show that our algorithm can outperform belief propagation for LDPC codes on bursty channels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/VJPTYRH7/Satorras and Welling - 2021 - Neural Enhanced Belief Propagation on Factor Graph.pdf;/Users/nobr/Zotero/storage/DJQJDQR5/2003.html}
}

@inproceedings{scaiano2012,
  title = {Getting {{More}} from {{Segmentation Evaluation}}},
  booktitle = {Proceedings of the 2012 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Scaiano, Martin and Inkpen, Diana},
  year = {2012},
  month = jun,
  pages = {362--366},
  publisher = {Association for Computational Linguistics},
  address = {Montr{\'e}al, Canada},
  urldate = {2023-06-11},
  file = {/Users/nobr/Zotero/storage/R8QZKIU3/Scaiano and Inkpen - 2012 - Getting More from Segmentation Evaluation.pdf}
}

@article{schechter2021,
  title = {Wargaming as a {{Methodology}}: {{The International Crisis Wargame}} and {{Experimental Wargaming}}},
  shorttitle = {Wargaming as a {{Methodology}}},
  author = {Schechter, Benjamin and Schneider, Jacquelyn and Shaffer, Rachael},
  year = {2021},
  month = aug,
  journal = {Simulation \& Gaming},
  volume = {52},
  number = {4},
  pages = {513--526},
  publisher = {SAGE Publications Inc},
  issn = {1046-8781},
  doi = {10.1177/1046878120987581},
  urldate = {2024-09-09},
  abstract = {Background.Wargaming has a long history as a tool for understanding the complexity of conflict. Although wargames have shown their relevance across topics and time, the immersive nature of wargames and the guild-like communities that surround them have often resisted the social scientific advances that occurred alongside the evolution of warfare. However, recent work raises new possibilities for integrating wargaming practices and social scientific methods.Purpose.Develop the experimental wargaming method and practice. Prioritizing the focus on iteration, control, and generalizability within experimental design can provide new opportunities for wargames to answer broader questions about decision-making, crisis behaviors, and patterns of outcomes.Method.The International Crisis Wargame developed in 2018 demonstrates the viability of experimental wargaming, and models the process of theorizing, designing, developing, and executing these wargames. It also identifies what makes games more or less experimental and details how experimental design influenced choices in the game.Conclusion.Experimental wargames are a promising new tool for both the social science and the wargaming communities. A proposed new research agenda for experimental design within wargames would support this nascent method},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HUFCAYI7/Schechter et al. - 2021 - Wargaming as a Methodology The International Crisis Wargame and Experimental Wargaming.pdf}
}

@article{schechtman1951,
  title = {The {{Jews}} of {{Aden}}},
  author = {Schechtman, Joseph B.},
  year = {1951},
  journal = {Jewish Social Studies},
  volume = {13},
  number = {2},
  eprint = {4464965},
  eprinttype = {jstor},
  pages = {133--148},
  publisher = {Indiana University Press},
  issn = {0021-6704},
  urldate = {2024-08-27}
}

@article{scheide,
  title = {Learning {{Behavior Trees}} for {{Robotic Task Planning}} by {{Monte Carlo Search}} over a {{Formal Grammar}}},
  author = {Scheide, Emily and Best, Graeme and Hollinger, Geoffrey A},
  year = {2021},
  abstract = {This extended abstract presents a method of learning behavior trees for robotic task and motion planning, which alleviates the need for time-intensive manual design. Our method involves representing a set of behavior trees as a formal grammar and searching over this grammar by means of a generalization of Monte Carlo tree search for directed acyclic graphs. We present promising preliminary results for a marine target search and response scenario, and the learned behavior tree compares well with a manually designed tree. It is notable that the learned tree contains several, but not all, sub-trees that are crucial and present in the manually designed tree. Our results motivate future investigation of ways to learn for our sparse reward functions and to better combine promising sub-trees.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LXZ9EGD6/Scheide et al. - Learning Behavior Trees for Robotic Task Planning by Monte Carlo Search over a Formal Grammar.pdf}
}

@book{schell1982,
  title = {The Fate of the Earth: {{Jonathan Schell}}},
  shorttitle = {The Fate of the Earth},
  author = {Schell, Jonathan},
  year = {1982},
  edition = {3. print},
  publisher = {Knopf},
  address = {New York},
  isbn = {978-0-394-52559-4},
  file = {/Users/nobr/Zotero/storage/RUYFGLQT/Schell - 1982 - The fate of the earth Jonathan Schell.pdf}
}

@book{schilling2017,
  title = {Measures, Integrals and Martingales},
  author = {Schilling, Ren{\'e} L.},
  year = {2017},
  edition = {2nd ed},
  publisher = {Cambridge university press},
  address = {Cambridge},
  isbn = {978-1-316-62024-3},
  langid = {english},
  lccn = {515.42},
  file = {/Users/nobr/Zotero/storage/3UNAJLUI/Schilling - 2017 - Measures, integrals and martingales.pdf}
}

@misc{schmidhuber2003,
  title = {Goedel {{Machines}}: {{Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements}}},
  shorttitle = {Goedel {{Machines}}},
  author = {Schmidhuber, Juergen},
  year = {2003},
  month = sep,
  journal = {arXiv.org},
  urldate = {2023-10-21},
  abstract = {We present the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. Inspired by Kurt Goedel's celebrated self-referential formulas (1931), such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. The searcher systematically and efficiently tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite. We show that such a self-rewrite is globally optimal - no local maxima! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites. Unlike previous non-self-referential methods based on hardwired proof searchers, ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the O()-notation, provided the utility of such speed-ups is provable at all.},
  howpublished = {https://arxiv.org/abs/cs/0309048v5},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UA5LWYHJ/Schmidhuber - 2003 - Goedel Machines Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvement.pdf}
}

@article{schmidhuber2009,
  title = {Driven by Compression Progress: {{A}} Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2009},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {5499 LNAI},
  eprint = {0812.4360},
  pages = {48--76},
  issn = {03029743},
  doi = {10.1007/978-3-642-02565-5_4},
  urldate = {2023-03-15},
  abstract = {I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, non-arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems. {\copyright} 2009 Springer Berlin Heidelberg.},
  archiveprefix = {arXiv},
  isbn = {3642025641},
  file = {/Users/nobr/Zotero/storage/2KR2F5BR/Schmidhuber - 2009 - Driven by compression progress A simple principle explains essential aspects of subjective beauty, .pdf}
}

@article{schmidhuber2009a,
  title = {Ultimate {{Cognition}} {\`a} La {{G{\"o}del}}},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2009},
  month = jun,
  journal = {Cognitive Computation},
  volume = {1},
  number = {2},
  pages = {177--193},
  issn = {1866-9956, 1866-9964},
  doi = {10.1007/s12559-009-9014-y},
  urldate = {2023-10-21},
  abstract = {All life is problem solving,'' said Popper. To deal with arbitrary problems in arbitrary environments, an ultimate cognitive agent should use its limited hardware in the ``best'' and ``most efficient'' possible way. Can we formally nail down this informal statement, and derive a mathematically rigorous blueprint of ultimate cognition? Yes, we can, using Kurt Go{\textasciidieresis}del's celebrated self-reference trick of 1931 in a new way. Go{\textasciidieresis}del exhibited the limits of mathematics and computation by creating a formula that speaks about itself, claiming to be unprovable by an algorithmic theorem prover: either the formula is true but unprovable, or math itself is flawed in an algorithmic sense. Here we describe an agent-controlling program that speaks about itself, ready to rewrite itself in arbitrary fashion once it has found a proof that the rewrite is useful according to a user-defined utility function. Any such a rewrite is necessarily globally optimal---no local maxima!---since this proof necessarily must have demonstrated the uselessness of continuing the proof search for even better rewrites. Our self-referential program will optimally speed up its proof searcher and other program parts, but only if the speed up's utility is indeed provable---even ultimate cognition has limits of the Go{\textasciidieresis}delian kind.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/X3KDQ3M9/Schmidhuber - 2009 - Ultimate Cognition à la Gödel.pdf}
}

@article{schmidhuber2013,
  title = {{{PowerPlay}}: {{Training}} an {{Increasingly General Problem Solver}} by {{Continually Searching}} for the {{Simplest Still Unsolvable Problem}}},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  eprint = {1112.5309},
  publisher = {Frontiers Media SA},
  doi = {10.3389/FPSYG.2013.00313},
  urldate = {2023-03-15},
  abstract = {Most of computer science focuses on automatically solving given computational problems. I focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans, to train a more and more general problem solver from scratch in an unsupervised fashion. Consider the infinite set of all computable descriptions of tasks with possibly computable solutions. The novel algorithmic framework POWERPLAY (2011) continually searches the space of possible pairs of new tasks and modifications of the current problem solver, until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one, while the unmodified predecessor does not. Wow-effects are achieved by continually making previously learned skills more efficient such that they require less time and space. New skills may (partially) re-use previously learned skills. POWERPLAY's search orders candidate pairs of tasks and solver modifications by their conditional computational (time \& space) complexity, given the stored experience so far. The new task and its corresponding task-solving skill are those first found and validated. The computational costs of validating new tasks need not grow with task repertoire size. POWERPLAY's ongoing search for novelty keeps breaking the generalization abilities of its present solver. This is related to Goedel's sequence of increasingly powerful formal theories based on adding formerly unprovable statements to the axioms without affecting previously provable theorems. The continually increasing repertoire of problem solving procedures can be exploited by a parallel search for solutions to additional externally posed tasks. POWERPLAY may be viewed as a greedy but practical implementation of basic principles of creativity. A first experimental analysis can be found in separate papers [53,54].},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/7GADHJJK/Schmidhuber - 2013 - PowerPlay Training an Increasingly General Problem Solver by Continually Searching for the Simplest.pdf}
}

@article{schneider2013,
  title = {Unravelling Daily Human Mobility Motifs},
  author = {Schneider, Christian M. and Belik, Vitaly and Couronn{\'e}, Thomas and Smoreda, Zbigniew and Gonz{\'a}lez, Marta C.},
  year = {2013},
  month = jul,
  journal = {Journal of The Royal Society Interface},
  volume = {10},
  number = {84},
  pages = {20130246},
  issn = {1742-5689, 1742-5662},
  doi = {10.1098/rsif.2013.0246},
  urldate = {2023-07-10},
  abstract = {Human mobility is differentiated by time scales. While the mechanism for long time scales has been studied, the underlying mechanism on the daily scale is still unrevealed. Here, we uncover the mechanism responsible for the daily mobility patterns by analysing the temporal and spatial trajectories of thousands of persons as individual networks. Using the concept of motifs from network theory, we find only 17 unique networks are present in daily mobility and they follow simple rules. These networks, called here motifs, are sufficient to capture up to 90 per cent of the population in surveys and mobile phone datasets for different countries. Each individual exhibits a characteristic motif, which seems to be stable over several months. Consequently, daily human mobility can be reproduced by an analytically tractable framework for Markov chains by modelling periods of high-frequency trips followed by periods of lower activity as the key ingredient.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/95SELYPP/Schneider et al. - 2013 - Unravelling daily human mobility motifs.pdf}
}

@article{schonegger2022,
  title = {What's up with Anti-Natalists? {{An}} Observational Study on the Relationship between Dark Triad Personality Traits and Anti-Natalist Views},
  shorttitle = {What's up with Anti-Natalists?},
  author = {Sch{\"o}negger, Philipp},
  year = {2022},
  month = jan,
  journal = {Philosophical Psychology},
  volume = {35},
  number = {1},
  pages = {66--94},
  publisher = {Routledge},
  issn = {0951-5089},
  doi = {10.1080/09515089.2021.1946026},
  urldate = {2024-07-23},
  abstract = {In the past decade, research on the dark triad of personality (Machiavellianism, narcissism, and psychopathy) has demonstrated a strong relationship to a number of socially aversive moral judgments such as sacrificial utilitarian decisions in moral dilemmas. This study widens the scope of this research program and investigates the association between dark triad personality traits and anti-natalist views, i.e., views holding that procreation is morally wrong. The results of this study indicate that the dark triad personality traits of Machiavellianism and psychopathy are strongly associated with anti-natalist views. Further, depression is found to be both standing independently in a relationship with anti-natalist views as well as functioning as a mediator in the relationships between Machiavellianism/psychopathy and anti-natalist views. This pattern was replicated in a follow-up study. These findings add to the literature on dark triad personality traits and their relationship to moral judgments, suggesting that personality and mood play a substantive part in variation in anti-natalist views in a lay population.},
  file = {/Users/nobr/Zotero/storage/5CP8P3RQ/Schönegger - 2022 - What’s up with anti-natalists An observational study on the relationship between dark triad persona.pdf}
}

@article{schranghamer2020,
  title = {Graphene Memristive Synapses for High Precision Neuromorphic Computing},
  author = {Schranghamer, Thomas F. and Oberoi, Aaryan and Das, Saptarshi},
  year = {2020},
  month = oct,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {5474},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19203-z},
  urldate = {2024-02-02},
  abstract = {Memristive crossbar architectures are evolving as powerful in-memory computing engines for artificial neural networks. However, the limited number of non-volatile conductance states offered by state-of-the-art memristors is a concern for their hardware implementation since trained weights must be rounded to the nearest conductance states, introducing error which can significantly limit inference accuracy. Moreover, the incapability of precise weight updates can lead to convergence problems and slowdown of on-chip training. In this article, we circumvent these challenges by introducing graphene-based multi-level ({$>$}16) and non-volatile memristive synapses with arbitrarily programmable conductance states. We also show desirable retention and programming endurance. Finally, we demonstrate that graphene memristors enable weight assignment based on k-means clustering, which offers greater computing accuracy when compared with uniform weight quantization for vector matrix multiplication, an essential component for any artificial neural network.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Electronic devices,Electronic properties and devices},
  file = {/Users/nobr/Zotero/storage/ZKFSV3IY/Schranghamer et al. - 2020 - Graphene memristive synapses for high precision neuromorphic computing.pdf}
}

@article{schrittwieser2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  eprint = {1911.08265},
  primaryclass = {cs, stat},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  urldate = {2023-08-10},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/P84F8RX8/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2024-01-02},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/NMHN9ZPC/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/Users/nobr/Zotero/storage/U3I9EM2S/1707.html}
}

@article{schuman2017,
  title = {A {{Survey}} of {{Neuromorphic Computing}} and {{Neural Networks}} in {{Hardware}}},
  author = {Schuman, Catherine D. and Potok, T. and Patton, R. and Birdwell, J. and Dean, Mark E. and Rose, G. and Plank, J.},
  year = {2017},
  month = may,
  journal = {ArXiv},
  urldate = {2024-02-02},
  abstract = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
  file = {/Users/nobr/Zotero/storage/HCT8DQ9D/Schuman et al. - 2017 - A Survey of Neuromorphic Computing and Neural Networks in Hardware.pdf}
}

@incollection{schwartz1992,
  title = {Universals in the {{Content}} and {{Structure}} of {{Values}}: {{Theoretical Advances}} and {{Empirical Tests}} in 20 {{Countries}}},
  shorttitle = {Universals in the {{Content}} and {{Structure}} of {{Values}}},
  booktitle = {Advances in {{Experimental Social Psychology}}},
  author = {Schwartz, Shalom H.},
  editor = {Zanna, Mark P.},
  year = {1992},
  month = jan,
  volume = {25},
  pages = {1--65},
  publisher = {Academic Press},
  doi = {10.1016/S0065-2601(08)60281-6},
  urldate = {2025-08-31},
  abstract = {This chapter addresses the universals in the content and structure of values, concentrating on the theoretical advances and empirical tests in 20 countries, and its four basic issues: substantive contents of human values; identification of comprehensive set of values; extent to which the meaning of particular values was equivalent for different groups of people; and how the relations among different values was structured. Substantial progress has been made toward resolving each of these issues. Ten motivationally distinct value types that were likely to be recognized within and across cultures and used to form value priorities were identified. Set of value types that was relatively comprehensive, encompassing virtually all the types of values to which individuals attribute at least moderate importance as criteria of evaluation was demonstrated. The evidence from 20 countries was assembled, showing that the meaning of the value types and most of the single values that constitute them was reasonably equivalent across most groups. Two basic dimensions that organize value systems into an integrated motivational structure with consistent value conflicts and compatibilities were discovered. By identifying universal aspects of value content and structure, the chapter has laid the foundations for investigating culture-specific aspects in the future.},
  file = {/Users/nobr/Zotero/storage/NCALVB6F/S0065260108602816.html}
}

@inproceedings{scialom2022,
  title = {Fine-Tuned Language Models Are Continual Learners},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  year = {2022},
  pages = {6107--6122}
}

@misc{scotti2024,
  title = {{{MindEye2}}: {{Shared-Subject Models Enable fMRI-To-Image With}} 1 {{Hour}} of {{Data}}},
  shorttitle = {{{MindEye2}}},
  author = {Scotti, Paul S. and Tripathy, Mihir and Villanueva, Cesar Kadir Torrico and Kneeland, Reese and Chen, Tong and Narang, Ashutosh and Santhirasegaran, Charan and Xu, Jonathan and Naselaris, Thomas and Norman, Kenneth A. and Abraham, Tanishq Mathew},
  year = {2024},
  month = mar,
  number = {arXiv:2403.11207},
  eprint = {2403.11207},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-03-20},
  abstract = {Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  file = {/Users/nobr/Zotero/storage/JAJBY259/Scotti et al. - 2024 - MindEye2 Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data.pdf}
}

@misc{seba_framework,
  title = {Technology Disruption Framework},
  author = {Seba, Tony}
}

@book{sedgewick2011,
  title = {Algorithms},
  author = {Sedgewick, Robert and Wayne, Kevin Daniel},
  year = {2011},
  edition = {4th ed},
  publisher = {Addison-Wesley},
  address = {Upper Saddle River},
  isbn = {978-0-321-57351-3},
  langid = {english},
  lccn = {518.1},
  file = {/Users/nobr/Zotero/storage/IGCXF4TJ/Sedgewick and Wayne - 2011 - Algorithms.pdf}
}

@misc{seeg_emissions,
  title = {Greenhouse Gas Emission Estimation System ({{SEEG}})}
}

@misc{seeliger2018,
  title = {Generative Adversarial Networks for Reconstructing Natural Images from Brain Activity},
  author = {Seeliger, K. and G{\"u}{\c c}l{\"u}, U. and Ambrogioni, L. and G{\"u}{\c c}l{\"u}t{\"u}rk, Y. and van Gerven, M. A. J.},
  year = {2018},
  month = jun,
  doi = {10.1101/226688},
  urldate = {2023-05-13},
  abstract = {We explore a method for reconstructing visual stimuli from brain activity. Using large databases of natural images we trained a deep convolutional generative adversarial network capable of generating gray scale photos, similar to stimUli presented during two functional magnetic resonance imaging experiments. Using a linear model we learned to predict the generative model's latent space from measured brain activity. The objective was to create an image similar to the presented stimulus image through the previously trained generator. Using this approach we were able to reconstruct structural and some semantic features of a proportion of the natural images sets. A behavioral test showed that subjects were capable of identifying a reconstruction of the original stimuhis in 67.2\% and 66.4\% of the cases in a pairwise comparison for the two natural image datasets respectively. our approach does not require end-to-end training of a large generative model on limited neuroimaging data. Rapid advances in generative modeling promise further improvements in reconstruction performance.},
  copyright = {{\copyright} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BQUHJSSI/Seeliger et al. - 2018 - Generative adversarial networks for reconstructing natural images from brain activity.pdf}
}

@article{senior2019,
  title = {Protein Structure Prediction Using Multiple Deep Neural Networks in the 13th {{Critical Assessment}} of {{Protein Structure Prediction}} ({{CASP13}})},
  author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v Z}{\'i}dek, Augustin and Nelson, Alexander W.R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2019},
  month = dec,
  journal = {Proteins: Structure, Function and Bioinformatics},
  volume = {87},
  number = {12},
  pages = {1141--1148},
  publisher = {{John Wiley and Sons Inc.}},
  issn = {10970134},
  doi = {10.1002/PROT.25834},
  urldate = {2023-03-08},
  abstract = {We describe AlphaFold, the protein structure prediction system that was entered by the group A7D in CASP13. Submissions were made by three free-modeling (FM) methods which combine the predictions of three neural networks. All three systems were guided by predictions of distances between pairs of residues produced by a neural network. Two systems assembled fragments produced by a generative neural network, one using scores from a network trained to regress GDT\_TS. The third system shows that simple gradient descent on a properly constructed potential is able to perform on par with more expensive traditional search techniques and without requiring domain segmentation. In the CASP13 FM assessors' ranking by summed z-scores, this system scored highest with 68.3 vs 48.2 for the next closest group (an average GDT\_TS of 61.4). The system produced high-accuracy structures (with GDT\_TS scores of 70 or higher) for 11 out of 43 FM domains. Despite not explicitly using template information, the results in the template category were comparable to the best performing template-based methods.},
  pmid = {31602685},
  keywords = {CASP,deep learning,machine learning,protein structure prediction},
  file = {/Users/nobr/Zotero/storage/LFNGIJSG/Senior et al. - 2019 - Protein structure prediction using multiple deep neural networks in the 13th Critical Assessment of .pdf}
}

@book{serafini1983,
  title = {Codex {{Seraphinianus}}. 2},
  author = {Serafini, Luigi},
  year = {1983},
  publisher = {Ricci},
  address = {Milano},
  isbn = {978-88-216-0027-2}
}

@article{sevcenko2023,
  title = {Theory-Based Approach for Assessing Cognitive Load during Time-Critical Resource-Managing Human--Computer Interactions: An Eye-Tracking Study},
  shorttitle = {Theory-Based Approach for Assessing Cognitive Load during Time-Critical Resource-Managing Human--Computer Interactions},
  author = {Sevcenko, Natalia and Appel, Tobias and Ninaus, Manuel and Moeller, Korbinian and Gerjets, Peter},
  year = {2023},
  month = mar,
  journal = {Journal on Multimodal User Interfaces},
  volume = {17},
  number = {1},
  pages = {1--19},
  issn = {1783-7677, 1783-8738},
  doi = {10.1007/s12193-022-00398-y},
  urldate = {2024-12-12},
  abstract = {Abstract             Computerized systems are taking on increasingly complex tasks. Consequently, monitoring automated computerized systems is becoming increasingly demanding for human operators, which is particularly relevant in time-critical situations. A possible solution might be adapting human--computer interfaces (HCI) to the operators' cognitive load. Here, we present a novel approach for theory-based measurement of cognitive load based on tracking eye movements of 42 participants while playing a serious game simulating time-critical situations that required resource management at different levels of difficulty. Gaze data was collected within narrow time periods, calculated based on log data interpreted in the light of the time-based resource-sharing model. Our results indicated that eye fixation frequency, saccadic rate, and pupil diameter significantly predicted task difficulty, while performance was best predicted by eye fixation frequency. Subjectively perceived cognitive load was significantly associated with the rate of microsaccades. Moreover our results indicated that more successful players tended to use breaks in gameplay to actively monitor the scene, while players who use these times to rest are more likely to fail the level. The presented approach seems promising for measuring cognitive load in realistic situations, considering adaptation of HCI.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/V42NZ269/Sevcenko et al. - 2023 - Theory-based approach for assessing cognitive load during time-critical resource-managing human–comp.pdf}
}

@misc{shah2024,
  title = {Agents {{Are Not Enough}}},
  author = {Shah, Chirag and White, Ryen W.},
  year = {2024},
  month = dec,
  number = {arXiv:2412.16241},
  eprint = {2412.16241},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.16241},
  urldate = {2025-08-02},
  abstract = {In the midst of the growing integration of Artificial Intelligence (AI) into various aspects of our lives, agents are experiencing a resurgence. These autonomous programs that act on behalf of humans are neither new nor exclusive to the mainstream AI movement. By exploring past incarnations of agents, we can understand what has been done previously, what worked, and more importantly, what did not pan out and why. This understanding lets us to examine what distinguishes the current focus on agents. While generative AI is appealing, this technology alone is insufficient to make new generations of agents more successful. To make the current wave of agents effective and sustainable, we envision an ecosystem that includes not only agents but also Sims, which represent user preferences and behaviors, as well as Assistants, which directly interact with the user and coordinate the execution of user tasks with the help of the agents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/HDEABE9M/Shah and White - 2024 - Agents Are Not Enough.pdf}
}

@techreport{shahsavarani2015,
  title = {Stress: {{Facts}} and {{Theories}} through {{Literature Review}}},
  author = {Shahsavarani, Amir Mohammad and Azad, Esfandiar and Abadi, Marz and Kalkhoran, Maryam Hakimi},
  year = {2015},
  journal = {International Journal of Medical Reviews Systematic Review International Journal of Medical Reviews},
  volume = {2},
  number = {2},
  pages = {230--241},
  file = {/Users/nobr/Zotero/storage/BLYYC6NF/Shahsavarani et al. - 2015 - Stress Facts and Theories through Literature Review.pdf}
}

@inproceedings{shaker2023,
  title = {Application of {{Big Data}} and {{Artificial Intelligence}} in {{Pilot Training}}: {{A Systematic Literature Review}}},
  shorttitle = {Application of {{Big Data}} and {{Artificial Intelligence}} in {{Pilot Training}}},
  booktitle = {2023 {{International Conference On Cyber Management And Engineering}} ({{CyMaEn}})},
  author = {Shaker, Mahmood H. and {Al-Alawi}, Adel Ismail},
  year = {2023},
  month = jan,
  pages = {205--209},
  publisher = {IEEE},
  address = {Bangkok, Thailand},
  doi = {10.1109/CyMaEn57228.2023.10050972},
  urldate = {2024-02-15},
  abstract = {In the aviation industry, large volumes of data are collected daily, varying from engine maintenance to flight monitoring information. The industry also gathers data from each flight that can indicate the level of pilot performance and allow for a thorough analysis to be presented, including tailored training programs to enhance pilot performance. This paper presents a systematic literature review of 10 related works, extracted and analyzed, to describe the current application of Big Data and AI in commercial airline pilots' training, and to identify areas where the research is limited or missing. Based on this review's descriptive qualitative content analysis method, the themes identified ranged from civilian to military training across different stages of the pilot training program. Findings reveal that research gaps exist in the current literature, where there is currently limited knowledge of how Big Data and AI can be systematically and comprehensively applied to pilot training that provides real-time feedback and span across the various stages of training. The review also shows that there is an absence of Micro-Adaptive Learning approaches to training programs that are tailored to individual pilots with varying learning styles and capabilities.},
  isbn = {978-1-6654-9329-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RRRESHTA/Shaker and Al-Alawi - 2023 - Application of Big Data and Artificial Intelligence in Pilot Training A Systematic Literature Revie.pdf}
}

@article{shamma2020,
  title = {Game Theory, Learning, and Control Systems},
  author = {Shamma, Jeff S},
  year = {2020},
  month = jul,
  journal = {National Science Review},
  volume = {7},
  number = {7},
  pages = {1118--1119},
  issn = {2095-5138, 2053-714X},
  doi = {10.1093/nsr/nwz163},
  urldate = {2024-01-13},
  abstract = {Summary             Game theory is the study of interacting decision makers, whereas control systems involve the design of intelligent decision-making devices. When many control systems are interconnected, the result can be viewed through the lens of game theory. This article discusses both long standing connections between these fields as well as new connections stemming from emerging applications.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/DIJRUE2I/Shamma - 2020 - Game theory, learning, and control systems.pdf}
}

@article{shannon1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  year = {1948},
  journal = {Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  publisher = {{American Telephone and Telegraph Company}},
  address = {Oxford, UK},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley 2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  langid = {english}
}

@article{shannon1950,
  type = {Bookitem},
  title = {Programming a {{Computer}} for {{Playing Chess}}},
  author = {Shannon, Claude E.},
  year = {1950},
  journal = {Computer Chess Compendium},
  pages = {2--13},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-1968-0_1},
  isbn = {9781475719703},
  langid = {english},
  keywords = {Chess Piece,Chess Player,General Purpose Computer,Human Player,Legal Move},
  file = {/Users/nobr/Zotero/storage/7ITCWRYU/2-0 and 2-1.Programming_a_computer_for_playing_chess.shannon.062303002.pdf}
}

@article{shannon2001,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  year = {2001},
  month = jan,
  journal = {ACM SIGMOBILE Mobile Computing and Communications Review},
  volume = {5},
  number = {1},
  pages = {3--55},
  issn = {1559-1662, 1931-1222},
  doi = {10.1145/584091.584093},
  urldate = {2023-08-20},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GAWI3HFG/Shannon - A Mathematical Theory of Communication.pdf}
}

@misc{shao2018,
  title = {{{StarCraft Micromanagement}} with {{Reinforcement Learning}} and {{Curriculum Transfer Learning}}},
  author = {Shao, Kun and Zhu, Yuanheng and Zhao, Dongbin},
  year = {2018},
  month = apr,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Real-time strategy games have been an important field of game artificial intelligence in recent years. This paper presents a reinforcement learning and curriculum transfer learning method to control multiple units in StarCraft micromanagement. We define an efficient state representation, which breaks down the complexity caused by the large state space in the game environment. Then a parameter sharing multi-agent gradientdescent Sarsa({$\lambda$}) (PS-MAGDS) algorithm is proposed to train the units. The learning policy is shared among our units to encourage cooperative behaviors. We use a neural network as a function approximator to estimate the action-value function, and propose a reward function to help units balance their move and attack. In addition, a transfer learning method is used to extend our model to more difficult scenarios, which accelerates the training process and improves the learning performance. In small scale scenarios, our units successfully learn to combat and defeat the built-in AI with 100\% win rates. In large scale scenarios, curriculum transfer learning method is used to progressively train a group of units, and shows superior performance over some baseline methods in target scenarios. With reinforcement learning and curriculum transfer learning, our units are able to learn appropriate strategies in StarCraft micromanagement scenarios.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/38PNT3H7/Shao et al. - 2018 - StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning.pdf}
}

@article{shao2021,
  title = {Dynamic {{Oversampling}} for 1-{{Bit ADCs}} in {{Large-Scale Multiple-Antenna Systems}}},
  author = {Shao, Zhichao and Landau, Lukas T. N. and {de Lamare}, Rodrigo C.},
  year = {2021},
  month = may,
  journal = {IEEE Transactions on Communications},
  volume = {69},
  number = {5},
  pages = {3423--3435},
  issn = {1558-0857},
  doi = {10.1109/TCOMM.2021.3059303},
  abstract = {In this work, large-scale multiple-antenna systems are investigated, where the base station employs a large antenna array with low-cost and low-power 1-bit analog-to-digital converters. To compensate for the performance loss caused by the coarse quantization, oversampling is applied at the receiver. Unlike existing works that use uniform oversampling, which samples the signal at a constant rate, a novel dynamic oversampling scheme is proposed. The basic idea is to perform time-varying nonuniform oversampling, which selects samples with nonuniform patterns that vary over time. We consider two system design criteria: a design that maximizes the achievable sum rate and another design that minimizes the mean square error of detected symbols. Dynamic oversampling is carried out using a dimension reduction matrix {$\Delta$}, which can be computed by the generalized eigenvalue decomposition or by novel submatrix-level feature selection algorithms. Moreover, the proposed scheme is analyzed in terms of convergence, computational complexity and power consumption at the receiver. Simulations show that systems with the proposed dynamic oversampling outperform those with uniform oversampling in terms of computational cost, achievable sum rate and symbol error rate performance.},
  keywords = {1-bit ADCs,Computational modeling,dimension reduction,Dimensionality reduction,dynamic oversampling,Large-scale MIMO,Matrix decomposition,MIMO communication,Multi-layer neural network,Power demand,Quantization (signal)},
  file = {/Users/nobr/Zotero/storage/ZQTRYWWW/Shao et al. - 2021 - Dynamic Oversampling for 1-Bit ADCs in Large-Scale.pdf;/Users/nobr/Zotero/storage/8BB9IEQM/9354165.html}
}

@misc{shao2024,
  title = {{{SwarmBrain}}: {{Embodied}} Agent for Real-Time Strategy Game {{StarCraft II}} via Large Language Models},
  shorttitle = {{{SwarmBrain}}},
  author = {Shao, Xiao and Jiang, Weifu and Zuo, Fei and Liu, Mengqing},
  year = {2024},
  month = jan,
  number = {arXiv:2401.17749},
  eprint = {2401.17749},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.17749},
  urldate = {2024-02-16},
  abstract = {Large language models (LLMs) have recently garnered significant accomplishments in various exploratory tasks, even surpassing the performance of traditional reinforcement learning-based methods that have historically dominated the agent-based field. The purpose of this paper is to investigate the efficacy of LLMs in executing real-time strategy war tasks within the StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an embodied agent leveraging LLM for real-time strategy implementation in the StarCraft II game environment. The SwarmBrain comprises two key components: 1) a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed to orchestrate macro-level strategies from a high-level perspective. This matrix emulates the overarching consciousness of the Zerg intelligence brain, synthesizing strategic foresight with the aim of allocating resources, directing expansion, and coordinating multi-pronged assaults. 2) a Swarm ReflexNet, which is agile counterpart to the calculated deliberation of the Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the Swarm ReflexNet employs a condition-response state machine framework, enabling expedited tactical responses for fundamental Zerg unit maneuvers. In the experimental setup, SwarmBrain is in control of the Zerg race in confrontation with an Computer-controlled Terran adversary. Experimental results show the capacity of SwarmBrain to conduct economic augmentation, territorial expansion, and tactical formulation, and it shows the SwarmBrain is capable of achieving victory against Computer players set at different difficulty levels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/TXZ573VD/Shao et al. - 2024 - SwarmBrain Embodied agent for real-time strategy game StarCraft II via large language models.pdf;/Users/nobr/Zotero/storage/4N4QC9V6/2401.html}
}

@techreport{sharma2016,
  title = {Cosmopolitan {{Mumbai}}, {{Orthodox Delhi}}, {{Techcity Bangalore}}: {{Understanding City Specific Societal Sentiment}}},
  author = {Sharma, D S and Sangal, R and Singh Proc, A K and Reganti, Aishwarya N and Maheshwari, Tushar and Kumar, Upendra and Das, Amitava},
  year = {2016},
  journal = {NLP Association of India},
  pages = {167--176},
  institution = {NLPAI},
  abstract = {The paper reports work on investigating societal sentiment using the Schwartz values and ethics model, and applying it to social media text of users from 20 most populous cities of India to represent geo-specific societal sentiment map of In-dia. For the automatic detection of societal sentiment we propose psycholinguistic analysis, that reveals how a user's social media behaviour and language is related to his/her ethical practices. In-dia is a multi-cultural country, values and ethics of each Indian are highly diverse and dependent on the region or society s/he belongs to. Several experiments were carried out incorporating Linguistic Inquiry Word Count analysis, n-grams, topic modeling, psycholinguistic lexica, speech-acts, and non-linguistic features, while experimenting with a range of machine learning algorithms including Support Vector Machines, Logistic Regression, and Random Forests to identify the best linguistic and non-linguistic features for automatic classification of values and ethics.},
  file = {/Users/nobr/Zotero/storage/PAKQ6DNH/Sharma et al. - 2016 - Cosmopolitan Mumbai, Orthodox Delhi, Techcity Bangalore Understanding City Specific Societal Sentim.pdf}
}

@inproceedings{sharma2024,
  title = {Investigating {{Agency}} of {{LLMs}} in {{Human-AI Collaboration Tasks}}},
  booktitle = {Proceedings of the 18th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sharma, Ashish and Rao, Sudha and Brockett, Chris and Malhotra, Akanksha and Jojic, Nebojsa and Dolan, Bill},
  editor = {Graham, Yvette and Purver, Matthew},
  year = {2024},
  month = mar,
  pages = {1968--1987},
  publisher = {Association for Computational Linguistics},
  address = {St. Julian's, Malta},
  urldate = {2024-12-13},
  abstract = {Agency, the capacity to proactively shape events, is central to how humans interact and collaborate. While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration. In this paper, we investigate Agency as a desirable function of LLMs, and how it can be measured and managed. We build on social-cognitive theory to develop a framework of features through which Agency is expressed in dialogue -- indicating what you intend to do (Intentionality), motivating your intentions (Motivation), having self-belief in intentions (Self-Efficacy), and being able to self-adjust (Self-Regulation). We collect a new dataset of 83 human-human collaborative interior design conversations containing 908 conversational snippets annotated for Agency features. Using this dataset, we develop methods for measuring Agency of LLMs. Automatic and human evaluations show that models that manifest features associated with high Intentionality, Motivation, Self-Efficacy, and Self-Regulation are more likely to be perceived as strongly agentive.},
  file = {/Users/nobr/Zotero/storage/B87VAQUM/Sharma et al. - 2024 - Investigating Agency of LLMs in Human-AI Collaboration Tasks.pdf}
}

@article{shen2019,
  title = {Deep Image Reconstruction from Human Brain Activity},
  author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
  year = {2019},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {1},
  pages = {e1006633},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006633},
  urldate = {2023-05-08},
  abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
  langid = {english},
  keywords = {Algorithms,Functional magnetic resonance imaging,Imaging techniques,Luminance,Neural networks,Optimization,Sensory perception,Vision},
  file = {/Users/nobr/Zotero/storage/WUES9AEV/Shen et al. - 2019 - Deep image reconstruction from human brain activit.pdf}
}

@article{shen2019a,
  title = {End-to-{{End Deep Image Reconstruction From Human Brain Activity}}},
  author = {Shen, Guohua and Dwivedi, Kshitij and Majima, Kei and Horikawa, Tomoyasu and Kamitani, Yukiyasu},
  year = {2019},
  journal = {Frontiers in Computational Neuroscience},
  volume = {13},
  issn = {1662-5188},
  urldate = {2023-05-09},
  abstract = {Deep neural networks (DNNs) have recently been applied successfully to brain decoding and image reconstruction from functional magnetic resonance imaging (fMRI) activity. However, direct training of a DNN with fMRI data is often avoided because the size of available data is thought to be insufficient for training a complex network with numerous parameters. Instead, a pre-trained DNN usually serves as a proxy for hierarchical visual representations, and fMRI data are used to decode individual DNN features of a stimulus image using a simple linear model, which are then passed to a reconstruction module. Here, we directly trained a DNN model with fMRI data and the corresponding stimulus images to build an end-to-end reconstruction model. We accomplished this by training a generative adversarial network with an additional loss term that was defined in high-level feature space (feature loss) using up to 6,000 training data samples (natural images and fMRI responses). The above model was tested on independent datasets and directly reconstructed image using an fMRI pattern as the input. Reconstructions obtained from our proposed method resembled the test stimuli (natural and artificial images) and reconstruction accuracy increased as a function of training-data size. Ablation analyses indicated that the feature loss that we employed played a critical role in achieving accurate reconstruction. Our results show that the end-to-end model can learn a direct mapping between brain activity and perception.},
  file = {/Users/nobr/Zotero/storage/ZDHII8FB/Shen et al. - 2019 - End-to-End Deep Image Reconstruction From Human Br.pdf}
}

@article{shorabeh2022,
  title = {The Site Selection of Wind Energy Power Plant Using {{GIS-multi-criteria}} Evaluation from Economic Perspectives},
  author = {Shorabeh, Saman Nadizadeh and Firozjaei, Hamzeh Karimi and Firozjaei, Mohammad Karimi and {Jelokhani-Niaraki}, Mohammadreza and Homaee, Mehdi and Nematollahi, Omid},
  year = {2022},
  month = oct,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {168},
  pages = {112778},
  issn = {13640321},
  doi = {10.1016/j.rser.2022.112778},
  urldate = {2023-07-13},
  abstract = {The use of wind turbines can help progress towards economic and technological development, lower rates of fossil fuel consumption, decreased greenhouse emissions, and reduced side-effects of climate change. A successful mechanism for developing renewable energy worldwide is the guaranteed purchase of electricity generated from renewable energy sources. Accordingly, this study aims to integrate Geographic Information System-based Multicriteria Evaluation (GIS-MCE) models with economic frameworks to estimate the optimal purchasing price for electricity produced by wind turbines. A total of 13 criteria maps were used and integrated using Ordered Weighted Averaging (OWA) as a type of MCE model. The criteria were initially normalized based on the min\- imum, and maximum values and weights were assigned to each criterion, using the Best-Worst method. The OWA model identified optimal site locations at various decision risk levels. The economic efficiency of wind turbines and the potential purchasing price of electricity from turbines were also assessed in terms of Net Present Value (NPV). The results show that Ardabil and Southern Khorasan provinces had the most significant areas in the very-suitable class for wind turbine installation (small/large scale). The purchasing prices for wind-generated electricity ranged from 0.047 to 0.182 US\$ for large wind farms and 0.074 to 0.384 US\$ for small wind plants. The highest electricity produced from large wind farms was found in Maragheh.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/G6BPIXVS/Shorabeh et al. - 2022 - The site selection of wind energy power plant usin.pdf}
}

@techreport{shull1999,
  title = {The {{Battle}} of {{Dien Bien Phu}}: {{Strategic}}, {{Operational}} and {{Tactical Failure}}:},
  shorttitle = {The {{Battle}} of {{Dien Bien Phu}}},
  author = {Shull, Patrick W.},
  year = {1999},
  month = apr,
  address = {Fort Belvoir, VA},
  institution = {Defense Technical Information Center},
  doi = {10.21236/ADA363910},
  urldate = {2025-05-12},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4J3NCWKL/Shull - 1999 - The Battle of Dien Bien Phu Strategic, Operational and Tactical Failure.pdf}
}

@misc{shwartz-ziv2017,
  title = {Opening the {{Black Box}} of {{Deep Neural Networks}} via {{Information}}},
  author = {{Shwartz-Ziv}, Ravid and Tishby, Naftali},
  year = {2017},
  month = apr,
  number = {arXiv:1703.00810},
  eprint = {1703.00810},
  doi = {10.48550/arXiv.1703.00810},
  urldate = {2024-11-20},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the {\textbackslash}textit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \{{\textbackslash}emph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/KNAX2GAX/Shwartz-Ziv and Tishby - 2017 - Opening the Black Box of Deep Neural Networks via Information.pdf}
}

@article{siddiqui2023,
  title = {{{MeshGPT}}: {{Generating Triangle Meshes}} with {{Decoder-Only Transformers}}},
  author = {Siddiqui, Yawar and Alliegro, Antonio and Artemov, Alexey and Tommasi, Tatiana and Sirigatti, Daniele and Rosov, Vladislav and Dai, Angela and Nie{\ss}ner, Matthias},
  year = {2023},
  abstract = {We introduce MeshGPT, a new approach for generating triangle meshes that reflects the compactness typical of artist-created meshes, in contrast to dense triangle meshes extracted by iso-surfacing methods from neural fields. Inspired by recent advances in powerful large language models, we adopt a sequence-based approach to autoregressively generate triangle meshes as sequences of triangles. We first learn a vocabulary of latent quantized embeddings, using graph convolutions, which inform these embeddings of the local mesh geometry and topology. These embeddings are sequenced and decoded into triangles by a decoder, ensuring that they can effectively reconstruct the mesh. A transformer is then trained on this learned vocabulary to predict the index of the next embedding given previous embeddings. Once trained, our model can be autoregressively sampled to generate new triangle meshes, directly generating compact meshes with sharp edges, more closely imitating the efficient triangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable improvement over state of the art mesh generation methods, with a 9\% increase in shape coverage and a 30-point enhancement in FID scores across various categories.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TKMTB7YV/Siddiqui et al. - MeshGPT Generating Triangle Meshes with Decoder-Only Transformers.pdf}
}

@article{silva2013,
  title = {Wind Energy in {{Brazil}}: {{From}} the Power Sector's Expansion Crisis Model to the Favorable Environment},
  shorttitle = {Wind Energy in {{Brazil}}},
  author = {Silva, Neilton Fidelis Da and Rosa, Luiz Pinguelli and Freitas, Marcos Aur{\'e}lio Vasconcelos and Pereira, Marcio Giannini},
  year = {2013},
  month = jun,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {22},
  pages = {686--697},
  issn = {13640321},
  doi = {10.1016/j.rser.2012.12.054},
  urldate = {2023-07-13},
  abstract = {Since the 1970s, demands arising from the impacts of the power sector on the natural environment were added to studies regarding the strategic power sector and its impact on the economic and financial crises. Thus, the development of alternative technologies reflected the new institutional guidelines and overcame the technological paradigms that were based on increasing installed capacities. Consequently, multiple debates that consider the energy use potential of each region and its contributions to sustainable development occurred. This paper presents the information that is necessary for understanding the relationships of the development model that was founded based on waste and the expanding technologies that exploit natural resources. Actions that are aimed at developing renewable energy resources are structured based on the instability of the technological maintenance paradigm and are guaranteed by expanding technologies that were used prior to 1970. In addition, we evaluated the current institutional arrangements that are used to promote wind energy. In this case, greater attention was given to the European experience because Europe provides multiple examples of successful legal frameworks that promote wind energy. In addition, Europe is a benchmark for emerging market countries, such as Brazil.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5U53DUBY/Silva et al. - 2013 - Wind energy in Brazil From the power sector's exp.pdf}
}

@article{silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature 2016 529:7587},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/NATURE16961},
  urldate = {2023-03-22},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence. The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained --- the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99\% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5--0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.},
  isbn = {1051351651952},
  pmid = {26819042},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/nobr/Zotero/storage/DKVHJM6A/full-text.pdf}
}

@article{silver2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2024-01-13},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/nobr/Zotero/storage/WW8ABGUK/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@article{silver2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  month = dec,
  journal = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aar6404},
  urldate = {2024-01-13},
  abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
  file = {/Users/nobr/Zotero/storage/YTYCIVNR/Silver et al. - 2018 - A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.pdf}
}

@article{silver2021,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  year = {2021},
  journal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  doi = {10.1016/j.artint.2021.103535},
  file = {/Users/nobr/Zotero/storage/EDF2HN25/Silver et al. - 2021 - Reward is enough.pdf}
}

@article{simpson2021a,
  title = {Adaptation to a {{Viscous Snowball Earth Ocean}} as a {{Path}} to {{Complex Multicellularity}}},
  author = {Simpson, Carl},
  year = {2021},
  month = nov,
  journal = {The American Naturalist},
  volume = {198},
  number = {5},
  pages = {590--609},
  issn = {0003-0147, 1537-5323},
  doi = {10.1086/716634},
  urldate = {2025-08-02},
  abstract = {Animals, fungi, and algae with complex multicellular bodies all evolved independently from unicellular ancestors. The early history of these major eukaryotic multicellular clades, if not their origins, co-occur with an extreme phase of global glaciations known as the Snowball Earth. Here, I propose that the long-term loss of low-viscosity environments due to several rounds global glaciation drove the multiple origins of complex multicellularity in eukaryotes and the subsequent radiation of complex multicellular groups into previously unoccupied niches. In this scenario, life adapts to Snowball Earth oceans by evolving large size and faster speeds through multicellularity, which acts to compensate for high-viscosity seawater and achieve fluid flow at sufficient levels to satisfy metabolic needs. Warm, low-viscosity seawater returned with the melting of the Snowball glaciers, and with it, by virtue of large and fast multicellular bodies, new ways of life were unveiled.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2UVM62KN/Simpson - 2021 - Adaptation to a Viscous Snowball Earth Ocean as a Path to Complex Multicellularity.pdf}
}

@misc{simpsonAgileAntifragileArtificialIntelligenceEnabled2021,
  title = {Agile, {{Antifragile}}, {{Artificial-Intelligence-Enabled}}, {{Command}} and {{Control}}},
  author = {Simpson, Jacob and Oosthuizen, Rudolph and Sawah, Sondoss El and Abbass, Hussein},
  year = {2021},
  month = sep,
  number = {arXiv:2109.06874},
  eprint = {2109.06874},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.06874},
  urldate = {2023-10-13},
  abstract = {Artificial Intelligence (AI) is rapidly becoming integrated into military Command and Control (C2) systems as a strategic priority for many defence forces. The successful implementation of AI is promising to herald a significant leap in C2 agility through automation. However, realistic expectations need to be set on what AI can achieve in the foreseeable future. This paper will argue that AI could lead to a fragility trap, whereby the delegation of C2 functions to an AI could increase the fragility of C2, resulting in catastrophic strategic failures. This calls for a new framework for AI in C2 to avoid this trap. We will argue that antifragility along with agility should form the core design principles for AI-enabled C2 systems. This duality is termed Agile, Antifragile, AI-Enabled Command and Control (A3IC2). An A3IC2 system continuously improves its capacity to perform in the face of shocks and surprises through overcompensation from feedback during the C2 decision-making cycle. An A3IC2 system will not only be able to survive within a complex operational environment, it will also thrive, benefiting from the inevitable shocks and volatility of war.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/nobr/Zotero/storage/BS6VD9XF/Simpson et al. - 2021 - Agile, Antifragile, Artificial-Intelligence-Enabled, Command and Control.pdf;/Users/nobr/Zotero/storage/PQPIGQWZ/2109.html}
}

@article{singh2023,
  title = {{{ProgPrompt}}: {{Generating Situated Robot Task Plans}} Using {{Large Language Models}}},
  shorttitle = {{{ProgPrompt}}},
  author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  year = {2023},
  month = may,
  journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {11523--11530},
  publisher = {IEEE},
  address = {London, United Kingdom},
  doi = {10.1109/ICRA48891.2023.10161317},
  urldate = {2023-12-30},
  abstract = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io},
  isbn = {9798350323658},
  file = {/Users/nobr/Zotero/storage/YGH3KXM3/Singh et al. - 2023 - ProgPrompt Generating Situated Robot Task Plans using Large Language Models.pdf}
}

@book{sirkis1985silicone,
  title = {{Silicone XXI}},
  author = {Sirkis, Alfredo},
  year = {1985},
  publisher = {Editora Codecri},
  address = {Rio de Janeiro, Brazil},
  langid = {portuguese}
}

@article{sirkis2000,
  title = {Bike {{Networking}} in {{Rio}}: {{The}} Challenges for Non-Motorised Transport in an Automobile-Dominated Government Culture},
  shorttitle = {Bike {{Networking}} in {{Rio}}},
  author = {Sirkis, Alfredo},
  year = {2000},
  month = feb,
  journal = {Local Environment},
  volume = {5},
  number = {1},
  pages = {83--95},
  issn = {1354-9839, 1469-6711},
  doi = {10.1080/135498300113282},
  urldate = {2024-01-07},
  abstract = {Rio de Janeiro has developed, since 1992, an 84 km cycling network, more as the outcome of green and NGO lobbying than of clear cut and continuous government choices. The city transport policies are widely dominated by car and bus oriented priorities with insuf cient investment in rail, ferryboat and other mass transport options. Bicycle use is potentiall y part of a new policy aimed at reducing automobile dependence and its social and environmental consequences. The Rio experience was in uenced by the Dutch example, especially the Amsterdam achievements. The process of building this cycling infrastructur e is also a political and cultural one, sometimes encountering tough resistance from some sectors of the city government and the public, hostile to an investment they consider futile. However, opinion polls conducted in Rio have shown impressive support for bike networking. The main conditions for success are: good integration , maintenance and security; appropriat e support infrastruc ture; and continuity with regular investments and upgrading .},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HARFHC9C/Sirkis - 2000 - Bike Networking in Rio The challenges for non-motorised transport in an automobile-dominated govern.pdf}
}

@article{sirkis2018,
  title = {Pricing Carbon Removal},
  author = {Sirkis, Alfredo},
  year = {2018},
  month = oct,
  journal = {International Economics},
  volume = {155},
  pages = {3--7},
  issn = {21107017},
  doi = {10.1016/j.inteco.2017.11.004},
  urldate = {2023-06-19},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/82S3N3XF/Sirkis - 2018 - Pricing carbon removal.pdf}
}

@misc{sirkis2020,
  title = {Decarbonizer {{English}}.Pdf},
  author = {Sirkis, Alfredo},
  year = {2020},
  file = {/Users/nobr/Zotero/storage/QT4AFNU9/Decarbonizer English.pdf}
}

@book{sirkis2020a,
  title = {{Descarbon{\'a}rio}},
  author = {Sirkis, Alfredo},
  year = {2020},
  month = mar,
  publisher = {Ubook Editora},
  isbn = {978-65-86032-46-8},
  langid = {brazilian},
  keywords = {Meio ambiente; Conservacao e Protecao,meio-ambiente}
}

@book{sivanandam2008,
  title = {Introduction to Genetic Algorithms: With 13 Tables},
  shorttitle = {Introduction to Genetic Algorithms},
  author = {Sivanandam, S. N. and Deepa, S. N.},
  year = {2008},
  publisher = {Springer},
  address = {Berlin Heidelberg},
  isbn = {978-3-540-73189-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GR825U9U/Sivanandam and Deepa - 2008 - Introduction to genetic algorithms with 13 tables.pdf}
}

@misc{sloaneOnLineEncyclopediaInteger2003,
  title = {The {{On-Line Encyclopedia}} of {{Integer Sequences}}},
  author = {Sloane, N. J. A.},
  year = {2003},
  month = dec,
  number = {arXiv:math/0312448},
  eprint = {math/0312448},
  publisher = {arXiv},
  doi = {10.48550/arXiv.math/0312448},
  urldate = {2023-12-15},
  abstract = {This article gives a brief introduction to the On-Line Encyclopedia of Integer Sequences (or OEIS). The OEIS is a database of nearly 90,000 sequences of integers, arranged lexicographically. The entry for a sequence lists the initial terms (50 to 100, if available), a description, formulae, programs to generate the sequence, references, links to relevant web pages, and other information.},
  archiveprefix = {arXiv},
  keywords = {05-00 11-00 40-00,Mathematics - Combinatorics,Mathematics - Number Theory},
  file = {/Users/nobr/Zotero/storage/3DKSXJB5/Sloane - 2003 - The On-Line Encyclopedia of Integer Sequences.pdf;/Users/nobr/Zotero/storage/3HBGSJK7/0312448.html}
}

@book{smart2016,
  title = {Cryptography {{Made Simple}}},
  author = {Smart, Nigel},
  year = {2016},
  series = {Information {{Security}} and {{Cryptography}}},
  edition = {1st ed. 2016},
  publisher = {Springer International Publishing : Imprint: Springer},
  address = {Cham},
  doi = {10.1007/978-3-319-21936-3},
  abstract = {In this introductory textbook the author explains the key topics in cryptography. He takes a modern approach, where defining what is meant by "secure" is as important as creating something that achieves that goal, and security definitions are central to the discussion throughout. The chapters in Part 1 offer a brief introduction to the mathematical foundations: modular arithmetic, groups, finite fields, and probability; primality testing and factoring; discrete logarithms; elliptic curves; and lattices. Part 2 of the book shows how historical ciphers were broken, thus motivating the design of modern cryptosystems since the 1960s; this part also includes a chapter on information-theoretic security. Part 3 covers the core aspects of modern cryptography: the definition of security; modern stream ciphers; block ciphers and modes of operation; hash functions, message authentication codes, and key derivation functions; the "naive" RSA algorithm; public key encryption and signature algorithms; cryptography based on computational complexity; and certificates, key transport and key agreement. Finally, Part 4 addresses advanced prot ocols, where the parties may have different or even conflicting security goals: secret sharing schemes; commitments and oblivious transfer; zero-knowledge proofs; and secure multi-party computation. The author balances a largely non-rigorous style - many proofs are sketched only - with appropriate formality and depth. For example, he uses the terminology of groups and finite fields so that the reader can understand both the latest academic research and "real-world" documents such as application programming interface descriptions and cryptographic standards. The text employs colour to distinguish between public and private information, and all chapters include summaries and suggestions for further reading. This is a suitable textbook for advanced undergraduate and graduate students in computer science, mathematics and engineering, and for self-study by professionals in information security. While the appendix summarizes most of the basic algebra and notation required, it is assumed that the reader has a basic knowledge of discrete mathematics, probability, and elementary calculus},
  isbn = {978-3-319-21936-3},
  langid = {english},
  lccn = {005.73},
  keywords = {Computer science,Data structures (Computer science),Data Structures and Information Theory,Discrete mathematics,Discrete Mathematics,Mathematics,Mathematics of Computing,Security Science and Technology,System safety},
  file = {/Users/nobr/Zotero/storage/NWU67FD9/Smart - 2016 - Cryptography Made Simple.pdf}
}

@article{smith2018,
  title = {{{RETRACTED ARTICLE}}: {{Going}} in {{Through}} the {{Back Door}}: {{Challenging Straight Male Homohysteria}}, {{Transhysteria}}, and {{Transphobia Through Receptive Penetrative Sex Toy Use}}},
  shorttitle = {{{RETRACTED ARTICLE}}},
  author = {Smith, M.},
  year = {2018},
  month = dec,
  journal = {Sexuality \& Culture},
  volume = {22},
  number = {4},
  pages = {1542--1542},
  issn = {1095-5143, 1936-4822},
  doi = {10.1007/s12119-018-9536-0},
  urldate = {2024-01-26},
  abstract = {To date, very little research literature exists concerning receptive penetrative anal eroticism in straight men. Of particular interest are its impacts upon other factors relevant to masculinities, sex roles, and the study of sexualities. Several co-constituted features of masculinity are likely to be relevant to straight-male anal sexuality, including masturbatory play with penetrative sex toys. Specifically, this study seeks to explore, ``Do men who report greater comfort with receptive penetrative anal eroticism also report less transphobia, less obedience to masculine gender norms, greater partner sensitivity, and greater awareness about rape?'' This study uses semistructured interviews with thirteen men to explore this question, analyzed with a naturalist and constructivist grounded theory approach in the context of sexualities research and introduces transhysteria as a parallel concept to Anderson's homohysteria. This analysis recognizes potential socially remedial value for encouraging male anal eroticism with sex toys.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/CKL5FWSZ/Smith - 2018 - RETRACTED ARTICLE Going in Through the Back Door Challenging Straight Male Homohysteria, Transhyst.pdf}
}

@article{smith2022,
  title = {A Step-by-Step Tutorial on Active Inference and Its Application to Empirical Data},
  author = {Smith, Ryan and Friston, Karl J. and Whyte, Christopher J.},
  year = {2022},
  month = apr,
  journal = {Journal of Mathematical Psychology},
  volume = {107},
  pages = {102632},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2021.102632},
  urldate = {2023-12-04},
  abstract = {The active inference framework, and in particular its recent formulation as a partially observable Markov decision process (POMDP), has gained increasing popularity in recent years as a useful approach for modeling neurocognitive processes. This framework is highly general and flexible in its ability to be customized to model any cognitive process, as well as simulate predicted neuronal responses based on its accompanying neural process theory. It also affords both simulation experiments for proof of principle and behavioral modeling for empirical studies. However, there are limited resources that explain how to build and run these models in practice, which limits their widespread use. Most introductions assume a technical background in programming, mathematics, and machine learning. In this paper we offer a step-by-step tutorial on how to build POMDPs, run simulations using standard MATLAB routines, and fit these models to empirical data. We assume a minimal background in programming and mathematics, thoroughly explain all equations, and provide exemplar scripts that can be customized for both theoretical and empirical studies. Our goal is to provide the reader with the requisite background knowledge and practical tools to apply active inference to their own research. We also provide optional technical sections and multiple appendices, which offer the interested reader additional technical details. This tutorial should provide the reader with all the tools necessary to use these models and to follow emerging advances in active inference research.},
  keywords = {Active inference,Bayesian inference,Computational neuroscience,Decision-making,Learning,Machine learning},
  file = {/Users/nobr/Zotero/storage/NLHCMZFL/Smith et al. - 2022 - A step-by-step tutorial on active inference and its application to empirical data.pdf;/Users/nobr/Zotero/storage/YS73D4II/S0022249621000973.html}
}

@incollection{smith2024,
  title = {International Stability and Human Security in 2023},
  booktitle = {{{SIPRI Yearbook}} 2024: {{Armaments}}, {{Disarmament}} and {{International Security}}},
  author = {Smith, Dan},
  year = {2024},
  series = {{{SIPRI Yearbook Series}}},
  publisher = {OUP Oxford},
  urldate = {2024-07-07},
  isbn = {978-0-19-893057-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/WN8FUYWX/YB24 01 Intro.pdf}
}

@article{sogaard2023,
  title = {On the {{Opacity}} of {{Deep Neural Networks}}},
  author = {S{\o}gaard, Anders},
  year = {2023},
  month = apr,
  journal = {Canadian Journal of Philosophy},
  volume = {53},
  number = {3},
  pages = {224--239},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2024.1},
  urldate = {2024-05-27},
  abstract = {Deep neural networks are said to be opaque, impeding the development of safe and trustworthy artificial intelligence, but where this opacity stems from is less clear. What are the sufficient properties for neural network opacity? Here, I discuss five common properties of deep neural networks and two different kinds of opacity. Which of these properties are sufficient for what type of opacity? I show how each kind of opacity stems from only one of these five properties, and then discuss to what extent the two kinds of opacity can be mitigated by explainability methods.},
  copyright = {http://creativecommons.org/licenses/by/4.0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XWRGSMC3/Søgaard - 2023 - On the Opacity of Deep Neural Networks.pdf}
}

@article{sogaard2024,
  title = {Identity {{Theory}} and {{Falsifiability}}},
  author = {S{\o}gaard, Anders},
  year = {2024},
  month = feb,
  journal = {Acta Analytica},
  issn = {1874-6349},
  doi = {10.1007/s12136-024-00587-2},
  urldate = {2024-03-10},
  abstract = {I identify a class of arguments against multiple realization (MR): BookofSand arguments. The arguments are in their general form successful under reasonably uncontroversial assumptions, but this, on the other hand, turns the table on identity theory: If arguments from MR can always be refuted by BookofSand arguments, is identity theory falsifiable? In the absence of operational demarcation criteria, it is not. I suggest a parameterized formal demarcation principle for brain state/process types and show how it can be used to identify previously unconsidered contenders for evidence for MR, e.g., binary classification, division, and sorting. For these to be actual instances of MR, the corresponding psychological kinds must be verifiably, relevantly similar. I also briefly discuss possible linguistic, behavioral, and experimental demarcation criteria for psychological kinds.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/M63L2FGM/Søgaard - 2024 - Identity Theory and Falsifiability.pdf}
}

@misc{sohl-dicksteinBoundaryNeuralNetwork2024,
  title = {The Boundary of Neural Network Trainability Is Fractal},
  author = {{Sohl-Dickstein}, Jascha},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06184},
  eprint = {2402.06184},
  primaryclass = {nlin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.06184},
  urldate = {2024-02-13},
  abstract = {Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Chaotic Dynamics},
  file = {/Users/nobr/Zotero/storage/TYCDTFQ7/Sohl-Dickstein - 2024 - The boundary of neural network trainability is fractal.pdf;/Users/nobr/Zotero/storage/Y28EVYSN/2402.html}
}

@techreport{solar_foundation2016,
  title = {National Solar Jobs Census 2016},
  author = {{The Solar Foundation}},
  year = {2016},
  institution = {The Solar Foundation}
}

@book{solomon2015,
  title = {The Worm at the Core: On the Role of Death in Life},
  shorttitle = {The Worm at the Core},
  author = {Solomon, Sheldon and Greenberg, Jeff and Pyszczynski, Thomas A.},
  year = {2015},
  edition = {First edition},
  publisher = {Random House},
  address = {New York},
  abstract = {Demonstrates how an unconscious fear of death motivates nearly all human goals, behaviors, and cultures, examining the role of mortality awareness in prompting social unrest and war},
  isbn = {978-1-4000-6747-3},
  langid = {english},
  annotation = {OCLC: 890310320},
  file = {/Users/nobr/Zotero/storage/TKICB79X/Solomon et al. - 2015 - The worm at the core on the role of death in life.pdf}
}

@article{song2010,
  title = {Limits of {{Predictability}} in {{Human Mobility}}},
  author = {Song, Chaoming and Qu, Zehui and Blumm, Nicholas and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2010},
  month = feb,
  journal = {Science},
  volume = {327},
  number = {5968},
  pages = {1018--1021},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1177170},
  urldate = {2023-07-10},
  abstract = {Predictable Travel Routines                            While people rarely perceive their actions to be random, current models of human activity are fundamentally stochastic. Processes that rely on human mobility patterns, like the prediction of new epidemics, traffic engineering, or city planning, could benefit from highly accurate predictive models. To investigate the predictability of human dynamics,                                Song                 et al.                              (p.               1018               ) used the recorded trajectories of millions of mobile phone users, collected by mobile phone companies and anonymized for research purposes. They hypothesized that given the wide range of travel patterns that different users follow, there would be significant differences between their predictability as well: Users who travel less should be easier to predict than those who are constantly on the road. Surprisingly, there was 93\% predictability across the whole user base, and individuals' predictability did not in general fall significantly below 80\%.                        ,              Analysis of the trajectories of people carrying cell phones reveals that human mobility patterns are highly predictable.           ,              A range of applications, from predicting the spread of human and electronic viruses to city planning and resource management in mobile communications, depend on our ability to foresee the whereabouts and mobility of individuals, raising a fundamental question: To what degree is human behavior predictable? Here we explore the limits of predictability in human dynamics by studying the mobility patterns of anonymized mobile phone users. By measuring the entropy of each individual's trajectory, we find a 93\% potential predictability in user mobility across the whole user base. Despite the significant differences in the travel patterns, we find a remarkable lack of variability in predictability, which is largely independent of the distance users cover on a regular basis.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/MYQXMYC6/Song et al. - 2010 - Limits of Predictability in Human Mobility.pdf}
}

@misc{sos_mata_atlantica,
  title = {Atlantic Forest Atlas - Deforestation Data 2017-2018}
}

@article{sotiropoulos2019,
  title = {Building Connectomes Using Diffusion {{MRI}}: Why, How and But},
  shorttitle = {Building Connectomes Using Diffusion {{MRI}}},
  author = {Sotiropoulos, Stamatios N. and Zalesky, Andrew},
  year = {2019},
  journal = {NMR in Biomedicine},
  volume = {32},
  number = {4},
  pages = {e3752},
  issn = {1099-1492},
  doi = {10.1002/nbm.3752},
  urldate = {2023-05-13},
  abstract = {Why has diffusion MRI become a principal modality for mapping connectomes in vivo? How do different image acquisition parameters, fiber tracking algorithms and other methodological choices affect connectome estimation? What are the main factors that dictate the success and failure of connectome reconstruction? These are some of the key questions that we aim to address in this review. We provide an overview of the key methods that can be used to estimate the nodes and edges of macroscale connectomes, and we discuss open problems and inherent limitations. We argue that diffusion MRI-based connectome mapping methods are still in their infancy and caution against blind application of deep white matter tractography due to the challenges inherent to connectome reconstruction. We review a number of studies that provide evidence of useful microstructural and network properties that can be extracted in various independent and biologically relevant contexts. Finally, we highlight some of the key deficiencies of current macroscale connectome mapping methodologies and motivate future developments.},
  langid = {english},
  keywords = {brain network,connections,parcellation,tracers,tractography,white matter fibers},
  file = {/Users/nobr/Zotero/storage/YYWIKV5Z/Sotiropoulos and Zalesky - 2019 - Building connectomes using diffusion MRI why, how.pdf;/Users/nobr/Zotero/storage/PNBG28MS/nbm.html}
}

@article{sousa2022,
  title = {Review and Analysis of Research on {{Video Games}} and {{Artificial Intelligence}}: A Look Back and a Step Forward},
  shorttitle = {Review and Analysis of Research on {{Video Games}} and {{Artificial Intelligence}}},
  author = {Sousa, Jo{\~a}o Paulo and Tavares, Rog{\'e}rio and Gomes, Jo{\~a}o Pedro and Mendon{\c c}a, Vitor},
  year = {2022},
  journal = {Procedia Computer Science},
  volume = {204},
  pages = {315--323},
  issn = {18770509},
  doi = {10.1016/j.procs.2022.08.038},
  urldate = {2023-11-12},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UNAUYIQ9/Sousa et al. - 2022 - Review and analysis of research on Video Games and Artificial Intelligence a look back and a step f.pdf}
}

@misc{sovianyCurriculumLearningSurvey2022,
  title = {Curriculum {{Learning}}: {{A Survey}}},
  shorttitle = {Curriculum {{Learning}}},
  author = {Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  year = {2022},
  month = apr,
  number = {arXiv:2101.10382},
  eprint = {2101.10382},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.10382},
  urldate = {2024-01-08},
  abstract = {Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/9NK8YJHG/Soviany et al. - 2022 - Curriculum Learning A Survey.pdf;/Users/nobr/Zotero/storage/ZNW88HCQ/2101.html}
}

@inproceedings{spencer2019,
  title = {Operationalizing Artificial Intelligence for Multi-Domain Operations: A First Look},
  shorttitle = {Operationalizing Artificial Intelligence for Multi-Domain Operations},
  booktitle = {Artificial {{Intelligence}} and {{Machine Learning}} for {{Multi-Domain Operations Applications}}},
  author = {Spencer, David and Duncan, Stephen and Taliaferro, Adam},
  editor = {Pham, Tien},
  year = {2019},
  month = may,
  pages = {1},
  publisher = {SPIE},
  address = {Baltimore, United States},
  doi = {10.1117/12.2524227},
  urldate = {2024-09-09},
  abstract = {Artificial Intelligence / Machine Learning (AI/ML) is a foundational requirement for Multi-Domain Operations (MDO). To solve some of MDO's most critical problems---for example, penetrating and dis-integrating an adversary's antiaccess/area denial (A2/AD) systems---the future force requires the ability to converge capabilities from across multiple domains at speeds and scales beyond human cognitive abilities. This requires robust, interoperable AI/ML that operates across multiple layers: from optimizing technologies and platforms, to fusing data from multiple sources, to transferring knowledge across joint functions to accomplish critical MDO tactical tasks. This paper provides an overview of ongoing work from the Unified Quest Future Study Plan and other events with the Army's Futures and Concepts Center to operationalize AI/ML to address MDO problems with this layered approach. It includes insights and required AI/ML capabilities determined with subject matter experts from various organizations at these learning events over the past two years, as well as vignettes that illustrate how AI/ML can be operationalized to enable successful Multi-Domain Operations against a near peer adversary.},
  isbn = {978-1-5106-2677-5 978-1-5106-2678-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/TEI3SW5Q/Spencer et al. - 2019 - Operationalizing artificial intelligence for multi-domain operations a first look.pdf}
}

@incollection{sporns2016,
  title = {Foreword},
  booktitle = {Fundamentals of {{Brain Network Analysis}}},
  author = {Sporns, Olaf},
  editor = {Fornito, Alex and Zalesky, Andrew and Bullmore, Edward T.},
  year = {2016},
  month = jan,
  pages = {xiii-xv},
  publisher = {Academic Press},
  address = {San Diego},
  doi = {10.1016/B978-0-12-407908-3.09999-4},
  urldate = {2023-05-17},
  isbn = {978-0-12-407908-3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/WGFJCU8B/Sporns - 2016 - Foreword.pdf;/Users/nobr/Zotero/storage/WSAQ8BLL/fundamentals-of-brain-network-analysis.html}
}

@article{stanley2002,
  title = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  year = {2002},
  month = jun,
  journal = {Evolutionary Computation},
  volume = {10},
  number = {2},
  pages = {99--127},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/106365602320169811},
  urldate = {2024-09-05},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/557TL8NF/Stanley and Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topologies.pdf}
}

@book{stanley2015,
  title = {Why Greatness Cannot Be Planned: The Myth of the Objective},
  shorttitle = {Why Greatness Cannot Be Planned},
  author = {Stanley, Kenneth O. and Lehman, Joel},
  year = {2015},
  publisher = {Springer International Publishing},
  address = {Cham Heidelberg New York Dordrecht London},
  isbn = {978-3-319-15523-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/Y5CLYVQL/Stanley and Lehman - 2015 - Why greatness cannot be planned the myth of the objective.pdf}
}

@article{stanley2019,
  title = {Designing Neural Networks through Neuroevolution},
  author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
  year = {2019},
  month = jan,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {1},
  pages = {24--35},
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0006-z},
  urldate = {2024-09-04},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ARTPT7HZ/Stanley et al. - 2019 - Designing neural networks through neuroevolution.pdf}
}

@inproceedings{stapleton2015,
  title = {War {{Gaming Supported}} by {{Cognitive Computing}} and {{Time Manipulation}}},
  author = {Stapleton, Doug},
  year = {2015},
  urldate = {2024-09-09},
  abstract = {Cognitive computing is a disruptive new approach to developing war games and training forces. This approach can help overcome the current difficulty in scaling the translation of force level doctrine into a high level strategy game. Cognitive computing can generate likely scenarios and cascading effects that are based on rules generated from multiple sources of doctrine such as books, journals and after action reports. Adversary behaviour can be modelled from how they adapt to space and time conditions, current events and the positioning of friendly troops and military assets. Historical data can be synthesised into guidelines such as potential adversary range of movement or speed of movement. Players can then be afforded the ability to manipulate the fourth dimension of time. This allows players to optimise strategy collectively in ways that maximise resilience. Time manipulation allows users to explore doctrine and its implications within different scenarios and how those decisions cascade to other players. This paper explores the enhancements possible to traditional war gaming through the use of cognitive computing and time manipulation. 1.0 THE PROBLEM DESCRIPTION A realistic war game is one in which the Opposing Force (OPFOR) behaves consistently with a real world opposing force. How is this achieved? When humans role play the OPFOR and implement their view of the strategy that would be adopted by an OPFOR in the real world, we are inherently limited by our training, experience and perspective. There is a natural tendency to make decisions the way that we would make them based on our culture and doctrine, not on those of a potential adversary. The thinking can effectively become Blue on Blue and degrade the training value of the war game. How can this limitation be overcome. We need to have a deterministic way of getting into the mind of the OPFOR commanders and making the OPFOR behave realistically. This may be done through programming the rules by which the game play is executed for the OPFOR, or by providing strategic guidance to the human OPFOR players. This paper sets out an approach to generating the rules to achieve realistic and consistent OPFOR behaviour. In discussing the representation of adversary forces, the NATO Code of Best Practice (COBP) [15] comments that "Historically, adversary capabilities and behaviours were often fully scripted or heavily constrained. This was more appropriate in Cold War contexts than it is today. However, it was never ideal for Command and Control (C2) analysis because the dynamic interaction among friendly, adversary, and other forces is a critical element of C2 representation. Today, much more robust adversary representation of operational capabilities and choices are employed and indeed are necessary. Analysts must consider not only a range of scenarios, but also the range of possible adversary actions and reactions." Time is an additional training dimension that analysts need to consider. STO-MP-MSG-133 1 1 War Gaming Supported by Cognitive Computing and Time Manipulation In the real world, time is a one way vector. However, there are occasions in strategy games where we would benefit from being able to reset the clock and take a different path forward. This paper also looks at how strategy games can be manipulated in time and the associated learning implications. Indeed the world has become more complex and less certain, as Squadron Leader McPherson [16] puts it "Threats have become less structured, more complex and unpredictable whilst global terrorist organisations such as ISIS in Iraq and Syria, Al Qaeda, the Shabab in Somalia, and Boko Haram in Nigeria, have all emerged to dominate the headlines. Operations in Iraq and Afghanistan reinforced the need for a comprehensive and multi-dimensional solution to future NATO missions. Moreover, the emerging considerations of possible operations within NATO Member countries and the associated sensitivities this creates have only added to the dilemmas and challenges for commanders." Those non-state actors typically do not behave as traditional forces having more random elements to their behaviour profile, increasing the risk when only training with Blue on Blue doctrines. 1.1 Cognitive Computing The term OPFOR in some ways is a diplomatic nicety to avoid the ramifications of naming a real nation as a likely enemy. However, ideally the OPFOR should accurately emulate real-life enemies so that their behaviour in tactics, techniques and procedures (TTPs) in war games and exercises has similar military characteristics to the expected real-world foes. Cognitive computing is a way of mining a body or corpus of knowledge to derive behaviours. We don't want to be guessing at how an enemy commander would behave, we want to understand their military doctrine, their culture, their personality, in an organised fashion. This approach allows us to model the OPFOR on various real world scenarios. What is needed is a body of evidence such as: {$\bullet$} History of the OPFOR nation and the various military conflicts over time {$\bullet$} Their military doctrine as they define it and as we understand it {$\bullet$} After action and military intelligence reports that give factual evidence on how their military has behaved {$\bullet$} Specific information about their commanders, personality, bias for action and other factors that influence their military decision making {$\bullet$} General cultural proclivities of the OPFOR nation and how that might bias certain decision making processes While the OPFOR can be modelled as an amorphous blob, it is more productive to be able to configure each OPFOR instance based on a rich knowledge of potential adversaries, with specific realistic behaviours providing a deeper understanding behind the decision framework/thinking that drives their doctrines. 1.2 Time Manipulation Once a multi-player war game is played past a certain point, there is a cost involved in replaying the game to incorporate learning that would change earlier decisions. Course of Action analysis does not always result in a single outcome that is clearly advantageous. The game may come to a certain point where the players would like to explore a number of options. Having the 'state' of the game saved after each cycle of movement allows the game controllers to reset the play to a certain point in time and then evaluate the various outcomes. Thus time manipulation allows users to explore doctrine and its implications within different gaming scenarios and how those decisions cascade to other players. There is a broad topic of research called War Gaming Supported by Cognitive Computing and Time Manipulation 1 2 STO-MP-MSG-133 'temporal difference', where learning outcomes have to be considered in context to the stream of events in which they occur. Hence time is a fundamental resource for learning. 1.3 Bringing it together In the last hundred years, we have witnessed three distinct evolutions in warfare. {$\bullet$} In World War I, some battle fields were industrialised with light railways [20] to bring in men and materiel [21], with little actual territory changing hands but many thousands of casualties. {$\bullet$} In World War II, the war ending technology of the atomic bomb did not exist in an operational form at the beginning of the conflict. However the pace of the conflict was slow enough that technology continued to develop over the six years of the war. In that sense, WWII started to demonstrate the feedback loop of innovation, as the research and manufacturing necessary to operationalise the new atomic technology was integrated into national war planning. {$\bullet$} In today's world, the length of modern supply chains and the shortening timeframe of modern warfare means that we can no longer research and develop new weapons platforms, nor indeed reorder existing complex weapons platforms for delivery; they must be available in the necessary quantities before conflict arises. In the author's personal opinion, we will stand and fight; win or lose, with the resources at our disposal at the start of the next major conflict against a peer adversary. This change and tightening in our ability to respond makes it all the more important to be confident that we are training for, and able to respond to realistic scenarios. There is a fundamental difference when the game has its scenarios and behaviour modelled accurately on the opposing force doctrine and allowing for the game to be replayed from a certain point. This provides for a failure to be replayed and alternative courses of action experimented with to determine a winning strategy. The key point being that replays or temporal learning moves beyond teaching just the rules of the game and reinforces the strategy and principles to apply. 2.0 COGNITIVE COMPUTING The term cognitive computing is describing a computing platform that is able to form concepts, understand and reason, and learn with the capacity to recognise patterns and use natural language to communicate. That is not to suggest that computers have developed to the point of having human like intelligence characterised by perception, consciousness, self-awareness, and volition. At a practical level, we might recognise intelligence in an OPFOR that generally acts in a recognisable way consistent with the opposing force military doctrine, culture and experience. The following sections provide a background to the theory behind cognitive computing, the range of sources from which OPFOR behaviour can be derived and how that contributes to the modelling of adversary behaviour. 2.1 Cognitive Computing Theory A military researcher might pose the question "What is the maximum speed of the OPFOR Main Battle Tank?" while other more complex concepts such as "Send out a reconnaissance in force" would be interpreted differently based on the applicable military doctrine. War Gaming Supported by Cognitive Computing and Time Manipulation STO-MP-MSG-133 1 3 We need computers to interact and reason over natural-langu},
  file = {/Users/nobr/Zotero/storage/SSEIF4L7/Stapleton - 2015 - War Gaming Supported by Cognitive Computing and Time Manipulation.pdf}
}

@techreport{stern2006economics,
  title = {The Economics of Climate Change: {{The}} Stern Review},
  author = {Stern, Nicholas},
  year = {2006},
  pages = {700},
  address = {Cambridge, United Kingdom},
  institution = {Cambridge University Press / HM Treasury},
  isbn = {0-521-70080-9}
}

@article{stern2023,
  title = {Learning {{Without Neurons}} in {{Physical Systems}}},
  author = {Stern, Menachem and Murugan, Arvind},
  year = {2023},
  month = mar,
  journal = {Annual Review of Condensed Matter Physics},
  volume = {14},
  number = {1},
  pages = {417--441},
  issn = {1947-5454, 1947-5462},
  doi = {10.1146/annurev-conmatphys-040821-113439},
  urldate = {2023-08-28},
  abstract = {Learning is traditionally studied in biological or computational systems. The power of learning frameworks in solving hard inverse problems provides an appealing case for the development of physical learning in which physical systems adopt desirable properties on their own without computational design. It was recently realized that large classes of physical systems can physically learn through local learning rules, autonomously adapting their parameters in response to observed examples of use. We review recent work in the emerging field of physical learning, describing theoretical and experimental advances in areas ranging from molecular self-assembly to flow networks and mechanical materials. Physical learning machines provide multiple practical advantages over computer designed ones, in particular by not requiring an accurate model of the system, and their ability to autonomously adapt to changing needs over time. As theoretical constructs, physical learning machines afford a novel perspective on how physical constraints modify abstract learning theory.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/UGMYYLKL/Stern and Murugan - 2023 - Learning Without Neurons in Physical Systems.pdf}
}

@incollection{steunebrink2011,
  title = {A {{Family}} of {{G{\"o}del Machine Implementations}}},
  booktitle = {Artificial {{General Intelligence}}},
  author = {Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Schmidhuber, J{\"u}rgen and Th{\'o}risson, Kristinn R. and Looks, Moshe},
  year = {2011},
  volume = {6830},
  pages = {275--280},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-22887-2_29},
  urldate = {2023-10-21},
  abstract = {The Go{\textasciidieresis}del Machine is a universal problem solver encoded as a completely self-referential program capable of rewriting any part of itself, provided it can prove that the rewrite is useful according to some utility function, encoded within itself. Based on experience gained by constructing a virtual machine capable of running the first Go{\textasciidieresis}del Machine implementation written in self-referential code, we discuss several important refinements of the original concept. We also show how different approaches to implementing the proof search leads to a family of possible Go{\textasciidieresis}del Machine implementations.},
  isbn = {978-3-642-22886-5 978-3-642-22887-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZWHJMRKG/Steunebrink and Schmidhuber - 2011 - A Family of Gödel Machine Implementations.pdf}
}

@techreport{stocker2013technical,
  title = {Technical Summary},
  author = {Stocker, T. F. and Qin, D. and Plattner, G.-K. and Alexander, L. V. and Allen, S. K. and Bindoff, N. L. and Br{\'e}on, F.-M. and Church, J. A. and Cubasch, U. and Emori, S. and Forster, P. and Friedlingstein, P. and Gillett, N. and Gregory, J. M. and Hartmann, D. L. and Jansen, E. and Kirtman, B. and Knutti, R. and Krishna Kumar, K. and Lemke, P. and Marotzke, J. and {Masson-Delmotte}, V. and Meehl, G. A. and Mokhov, I. I. and Piao, S. and Ramaswamy, V. and Randall, D. and Rhein, M. and Rojas, M. and Sabine, C. and Shindell, D. and Talley, L. D. and Vaughan, D. G. and Xie, S.-P.},
  year = {2013},
  journal = {Climate change 2013: The physical science basis. Contribution of working group I to the fifth assessment report of the intergovernmental panel on climate change},
  pages = {33--115},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  institution = {Cambridge University Press}
}

@misc{stoddartHaltingParadox2019,
  title = {The {{Halting Paradox}}},
  author = {Stoddart, Bill},
  year = {2019},
  month = jun,
  number = {arXiv:1906.05340},
  eprint = {1906.05340},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-16},
  abstract = {The halting problem is considered to be an essential part of the theoretical background to computing. That halting is not in general computable has been ``proved'' in many text books and taught on many computer science courses, and is supposed to illustrate the limits of computation. However, Eric Hehner has a dissenting view, in which the specification of the halting problem is called into question.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Logic in Computer Science},
  file = {/Users/nobr/Zotero/storage/R54ARXEM/Stoddart - 2019 - The Halting Paradox.pdf}
}

@article{storer1982,
  title = {Data Compression via Textual Substitution},
  author = {Storer, James A. and Szymanski, Thomas G.},
  year = {1982},
  month = oct,
  journal = {Journal of the ACM},
  volume = {29},
  number = {4},
  pages = {928--951},
  issn = {0004-5411},
  doi = {10.1145/322344.322346},
  urldate = {2023-09-05},
  file = {/Users/nobr/Zotero/storage/6Z72KFLJ/Storer and Szymanski - 1982 - Data compression via textual substitution.pdf}
}

@misc{su2024,
  title = {Dualformer: {{Controllable Fast}} and {{Slow Thinking}} by {{Learning}} with {{Randomized Reasoning Traces}}},
  shorttitle = {Dualformer},
  author = {Su, DiJia and Sukhbaatar, Sainbayar and Rabbat, Michael and Tian, Yuandong and Zheng, Qinqing},
  year = {2024},
  month = oct,
  number = {arXiv:2410.09918},
  eprint = {2410.09918},
  doi = {10.48550/arXiv.2410.09918},
  urldate = {2024-10-27},
  abstract = {In human cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Recent studies have shown that incorporating System 2 process into Transformers including large language models (LLMs), significantly enhances their reasoning capabilities. Nevertheless, models that purely resemble System 2 thinking require substantially higher computational costs and are much slower to respond. To address this challenge, we present Dualformer, a single Transformer model that seamlessly integrates both the fast and slow reasoning modes. Dualformer is obtained by training on data with randomized reasoning traces, where different parts of the traces are dropped during training. The dropping strategies are specifically tailored according to the trace structure, analogous to analyzing our thinking process and creating shortcuts with patterns. At inference time, our model can be configured to output only the solutions (fast mode) or both the reasoning chain and the final solution (slow mode), or automatically decide which mode to engage (auto mode). In all cases, Dualformer outperforms the corresponding baseline models in both performance and computational efficiency: (1) in slow mode, Dualformer optimally solves unseen 30 x 30 maze navigation tasks 97.6\% of the time, surpassing the Searchformer (trained on data with complete reasoning traces) baseline performance of 93.3\%, while only using 45.5\% fewer reasoning steps; (2) in fast mode, Dualformer completes those tasks with an 80\% optimal rate, significantly outperforming the Solution-Only model (trained on solution-only data), which has an optimal rate of only 30\%. For math problems, our techniques have also achieved improved performance with LLM fine-tuning, showing its generalization beyond task-specific models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/YH7QD29D/Su et al. - 2024 - Dualformer Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces.pdf}
}

@misc{such2018,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms Are}} a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  shorttitle = {Deep {{Neuroevolution}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2018},
  month = apr,
  number = {arXiv:1712.06567},
  eprint = {1712.06567},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-05},
  abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, populationbased genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in {$\sim$}4 hours on one desktop or {$\sim$}1 hour distributed on 720 cores), and enables a stateof-the-art, up to 10,000-fold compact encoding technique.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/N4BTTCVY/Such et al. - 2018 - Deep Neuroevolution Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Netwo.pdf}
}

@misc{suddreyLearningExecutingReusable2021,
  title = {Learning and {{Executing Re-usable Behaviour Trees}} from {{Natural Language Instruction}}},
  author = {Suddrey, Gavin and Talbot, Ben and Maire, Frederic},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01650},
  eprint = {2106.01650},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-08},
  abstract = {Domestic and service robots have the potential to transform industries such as health care and small-scale manufacturing, as well as the homes in which we live. However, due to the overwhelming variety of tasks these robots will be expected to complete, providing generic out-of-the-box solutions that meet the needs of every possible user is clearly intractable. To address this problem, robots must therefore not only be capable of learning how to complete novel tasks at run-time, but the solutions to these tasks must also be informed by the needs of the user. In this paper we demonstrate how behaviour trees, a well established control architecture in the fields of gaming and robotics, can be used in conjunction with natural language instruction to provide a robust and modular control architecture for instructing autonomous agents to learn and perform novel complex tasks. We also show how behaviour trees generated using our approach can be generalised to novel scenarios, and can be re-used in future learning episodes to create increasingly complex behaviours. We validate this work against an existing corpus of natural language instructions, demonstrate the application of our approach on both a simulated robot solving a toy problem, as well as two distinct real-world robot platforms which, respectively, complete a block sorting scenario, and a patrol scenario.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/BV4CYGTH/Suddrey et al. - 2021 - Learning and Executing Re-usable Behaviour Trees from Natural Language Instruction.pdf;/Users/nobr/Zotero/storage/I5I7BTHU/2106.html}
}

@article{sudhakaran2021,
  title = {Growing {{3D Artefacts}} and {{Functional Machines}} with {{Neural Cellular Automata}}},
  author = {Sudhakaran, Shyam and Grbic, Djordje and Li, Siyan and Katona, Adam and Najarro, Elias and Glanois, Claire and Risi, Sebastian},
  year = {2021},
  month = mar,
  eprint = {2103.08737},
  abstract = {Neural Cellular Automata (NCAs) have been proven effective in simulating morphogenetic processes, the continuous construction of complex structures from very few starting cells. Recent developments in NCAs lie in the 2D domain, namely reconstructing target images from a single pixel or infinitely growing 2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D convolutions in the proposed neural network architecture. Minecraft is selected as the environment for our automaton since it allows the generation of both static structures and moving machines. We show that despite their simplicity, NCAs are capable of growing complex entities such as castles, apartment blocks, and trees, some of which are composed of over 3,000 blocks. Additionally, when trained for regeneration, the system is able to regrow parts of simple functional machines, significantly expanding the capabilities of simulated morphogenetic systems. The code for the experiment in this paper can be found at: https://github.com/real-itu/3d-artefacts-nca.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/HL8HK9PM/Sudhakaran et al. - 2021 - Growing 3D Artefacts and Functional Machines with Neural Cellular Automata.pdf}
}

@misc{sugawara2003,
  title = {Destruction of {{Nuclear Bombs Using Ultra-High Energy Neutrino Beam}}},
  author = {Sugawara, Hirotaka and Hagura, Hiroyuki and Sanami, Toshiya},
  year = {2003},
  month = jun,
  number = {arXiv:hep-ph/0305062},
  eprint = {hep-ph/0305062},
  publisher = {arXiv},
  doi = {10.48550/arXiv.hep-ph/0305062},
  urldate = {2025-03-28},
  abstract = {We discuss the possibility of utilizing the ultra-high energy neutrino beam (about 1000 TeV) to detect and destroy the nuclear bombs wherever they are and whoever possess them.},
  archiveprefix = {arXiv},
  keywords = {High Energy Physics - Experiment,High Energy Physics - Phenomenology},
  file = {/Users/nobr/Zotero/storage/LSWM6Z9C/Sugawara et al. - 2003 - Destruction of Nuclear Bombs Using Ultra-High Energy Neutrino Beam.pdf}
}

@misc{sunRetentiveNetworkSuccessor2023,
  title = {Retentive {{Network}}: {{A Successor}} to {{Transformer}} for {{Large Language Models}}},
  shorttitle = {Retentive {{Network}}},
  author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  year = {2023},
  month = aug,
  number = {arXiv:2307.08621},
  eprint = {2307.08621},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-23},
  abstract = {In this work, we propose Retentive Network (RETNET) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RETNET achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RETNET a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/CBDAEI9T/Sun et al. - 2023 - Retentive Network A Successor to Transformer for Large Language Models.pdf}
}

@article{surdu1999,
  title = {{{OPSIM}}: {{A PURPOSE-BUILT DISTRIBUTED SIMULATION FOR THE MISSION OPERATIONAL ENVIRONMENT}}},
  author = {Surdu, John R and Haines, Gary D and Pooch, Udo W},
  year = {1999},
  abstract = {This paper describes a simulation built specifically for use during ongoing operations. This discrete-event simulation, implemented in Java, can be launched by a user (as an applet within a Web browser) or by other simulations. It is designed to run in near real-time so that the progress of the real operation can be compared against the planned operation. Rather than using a proprietary terrain database, OpSim reads terrain directly from VMap-2 files. This simulation also supports the capability for external agents to query the simulation for state information or ask the simulation some level of hypothetical questions while it is running.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/97SYIEM9/Surdu et al. - OPSIM A PURPOSE-BUILT DISTRIBUTED SIMULATION FOR THE MISSION OPERATIONAL ENVIRONMENT.pdf}
}

@misc{suRoFormerEnhancedTransformer2023,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2104.09864},
  eprint = {2104.09864},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-21},
  abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model\_doc/roformer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/GPAZFUTC/Su et al. - 2023 - RoFormer Enhanced Transformer with Rotary Position Embedding.pdf}
}

@book{sutton-smith1997,
  title = {The Ambiguity of Play},
  author = {{Sutton-Smith}, Brian},
  year = {1997},
  publisher = {Harvard University Press},
  address = {Cambridge, Mass},
  isbn = {978-0-674-01733-7},
  langid = {english},
  lccn = {BF717 .S93 1997},
  keywords = {Play,Psychological aspects},
  file = {/Users/nobr/Zotero/storage/3VRFXR33/Sutton-Smith - 1997 - The ambiguity of play.pdf}
}

@book{sutton1998,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {1998},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-19398-6},
  langid = {english},
  lccn = {Q325.6 .S88 1998},
  keywords = {Reinforcement learning},
  file = {/Users/nobr/Zotero/storage/AKZKKX5Y/Sutton and Barto - 1998 - Reinforcement learning an introduction.pdf}
}

@article{suzuki2024,
  title = {Modelling Phenomenological Differences in Aetiologically Distinct Visual Hallucinations Using Deep Neural Networks},
  author = {Suzuki, Keisuke and Seth, Anil K. and Schwartzman, David J.},
  year = {2024},
  journal = {Frontiers in Human Neuroscience},
  volume = {17},
  issn = {1662-5161},
  urldate = {2024-01-07},
  abstract = {Visual hallucinations (VHs) are perceptions of objects or events in the absence of the sensory stimulation that would normally support such perceptions. Although all VHs share this core characteristic, there are substantial phenomenological differences between VHs that have different aetiologies, such as those arising from Neurodegenerative conditions, visual loss, or psychedelic compounds. Here, we examine the potential mechanistic basis of these differences by leveraging recent advances in visualising the learned representations of a coupled classifier and generative deep neural network---an approach we call `computational (neuro)phenomenology'. Examining three aetiologically distinct populations in which VHs occur---Neurodegenerative conditions (Parkinson's Disease and Lewy Body Dementia), visual loss (Charles Bonnet Syndrome, CBS), and psychedelics---we identified three dimensions relevant to distinguishing these classes of VHs: realism (veridicality), dependence on sensory input (spontaneity), and complexity. By selectively tuning the parameters of the visualisation algorithm to reflect influence along each of these phenomenological dimensions we were able to generate `synthetic VHs' that were characteristic of the VHs experienced by each aetiology. We verified the validity of this approach experimentally in two studies that examined the phenomenology of VHs in Neurodegenerative and CBS patients, and in people with recent psychedelic experience. These studies confirmed the existence of phenomenological differences across these three dimensions between groups, and crucially, found that the appropriate synthetic VHs were rated as being representative of each group's hallucinatory phenomenology. Together, our findings highlight the phenomenological diversity of VHs associated with distinct causal factors and demonstrate how a neural network model of visual phenomenology can successfully capture the distinctive visual characteristics of hallucinatory experience.},
  file = {/Users/nobr/Zotero/storage/4XXALYGK/Suzuki et al. - 2024 - Modelling phenomenological differences in aetiologically distinct visual hallucinations using deep n.pdf}
}

@article{swanson2016,
  title = {From {{Cajal}} to {{Connectome}} and {{Beyond}}},
  author = {Swanson, Larry W. and Lichtman, Jeff W.},
  year = {2016},
  journal = {Annual Review of Neuroscience},
  volume = {39},
  number = {1},
  pages = {197--216},
  doi = {10.1146/annurev-neuro-071714-033954},
  urldate = {2023-05-13},
  abstract = {One goal of systems neuroscience is a structure-function model of nervous system organization that would allow mechanistic linking of mind, brain, and behavior. A necessary but not sufficient foundation is a connectome, a complete matrix of structural connections between the nodes of a nervous system. Connections between two nodes can be described at four nested levels of analysis: macroconnections between gray matter regions, mesoconnections between neuron types, microconnections between individual neurons, and nanoconnections at synapses. A long history of attempts to understand how the brain operates as a system began at the macrolevel in the fifth century, was revolutionized at the meso- and microlevels by Cajal and others in the late nineteenth century, and reached the nanolevel in the mid-twentieth century with the advent of electron microscopy. The greatest challenge today is extracting knowledge and understanding of nervous system structure-function architecture from vast amounts of data.},
  pmid = {27442070},
  keywords = {big data,nervous system,network analysis,neural connections},
  file = {/Users/nobr/Zotero/storage/FI2Y93EC/Swanson and Lichtman - 2016 - From Cajal to Connectome and Beyond.pdf}
}

@article{sweller,
  title = {Teaching {{General Problem- Solving Skills Is Not}} a {{Substitute}} for, or a {{Viable Addition}} to, {{Teaching Mathematics}}},
  author = {Sweller, John and Clark, Richard and Kirschner, Paul},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2PY5WYJD/rtx101001303p.pdf}
}

@article{syrkis,
  title = {Mechanistic {{Interpretability}} and {{Implementability}} of {{Irreducible Integer Identifiers}}},
  author = {Syrkis, Noah and S{\o}gaard, Anders},
  abstract = {An attention-based deep learning model {$M$} is trained to solve tasks related to prime numbers. Specifically, {$M$} is trained to predict if a given natural number {$n$} is prime and what, if any, prime numbers it can be factorized by. The model is then reverse engineered to understand the learned algorithms for the tasks for which it generalizes well. Similar to Nanda, N., Chan, L., Lieberum, T., Smith, J., Steinhardt, J. [1], who trained a transformer model to perform modular addition ({$a$} + {$b$} mod 113 for all {$a$}, {$b$} {$<$} 113), focuses on the task ``is {$a$} {$\ast$} {$n$}1 + {$b$} {$\ast$} {$n$}0 prime?'' for all {$a$}, {$b$} {$<$} {$n$}. Setting {$n$} to 113 yields a dataset of size 12,769.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5QL6SP58/Syrkis and Søgaard - Mechanistic Interpretability and Implementability of Irreducible Integer Identifiers.pdf}
}

@book{syrkis2011,
  title = {{Lila}},
  author = {Syrkis, Liliana},
  year = {2011},
  month = oct,
  edition = {First edition},
  publisher = {TIX Edi{\c c}{\~o}es e Arte},
  address = {Rio de Janeiro, Brazil},
  isbn = {978-85-62733-03-1},
  langid = {brazilian},
  keywords = {Biografias}
}

@article{syrkis2024,
  title = {{{MY LOVE IN SUPERPOSIT1ON}}},
  author = {Syrkis, Noah},
  year = {2024},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4GKTMGCQ/Syrkis - MY LOVE IN SUPERPOSIT1ON.pdf}
}

@misc{syrkis2025,
  title = {Parabellum},
  author = {Syrkis, Noah and Anne, Timoth{\'e}e and Risi, Sebastian},
  year = {2025},
  month = jun,
  urldate = {2025-06-30},
  abstract = {A war game}
}

@article{szell2022,
  title = {Growing Urban Bicycle Networks},
  author = {Szell, Michael and Mimar, Sayat and Perlman, Tyler and Ghoshal, Gourab and Sinatra, Roberta},
  year = {2022},
  month = apr,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {6765},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-10783-y},
  urldate = {2023-07-06},
  abstract = {Abstract             Cycling is a promising solution to unsustainable urban transport systems. However, prevailing bicycle network development follows a slow and piecewise process, without taking into account the structural complexity of transportation networks. Here we explore systematically the topological limitations of urban bicycle network development. For 62 cities we study different variations of growing a synthetic bicycle network between an arbitrary set of points routed on the urban street network. We find initially decreasing returns on investment until a critical threshold, posing fundamental consequences to sustainable urban planning: cities must invest into bicycle networks with the right growth strategy, and persistently, to surpass a critical mass. We also find pronounced overlaps of synthetically grown networks in cities with well-developed existing bicycle networks, showing that our model reflects reality. Growing networks from scratch makes our approach a generally applicable starting point for sustainable urban bicycle network planning with minimal data requirements.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5UXMTR9H/Szell et al. - 2022 - Growing urban bicycle networks.pdf}
}

@article{tabbaa2021,
  title = {{{VREED}}: {{Virtual Reality Emotion Recognition Dataset Using Eye Tracking}} \& {{Physiological Measures}}},
  author = {Tabbaa, Luma and Searle, Ryan and Glancy, Maxine and Mirzaee Bafti, Saber and Hossain, Moinul and Intarasisrisawat, Jittrapol and Ang, Chee Siang and Uk, Glancy@bbc Co and Chee, ; and Ang, Siang},
  year = {2021},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol},
  volume = {5},
  number = {4},
  pages = {178},
  doi = {10.1145/3495002},
  urldate = {2023-03-15},
  abstract = {The paper introduces a multimodal affective dataset named VREED (VR Eyes: Emotions Dataset) in which emotions were triggered using immersive 360{$^\circ$} Video-Based Virtual Environments (360-VEs) delivered via Virtual Reality (VR) headset. Behavioural (eye tracking) and physiological signals (Electrocardiogram (ECG) and Galvanic Skin Response (GSR)) were captured, together with self-reported responses, from healthy participants (n=34) experiencing 360-VEs (n=12, 1-3 min each) selected through focus groups and a pilot trial. Statistical analysis confirmed the validity of the selected 360-VEs in eliciting the desired emotions. Preliminary machine learning analysis was carried out, demonstrating state-of-the-art performance reported in affective computing literature using non-immersive modalities. VREED is among the first multimodal VR datasets in emotion recognition using behavioural and physiological signals. VREED is made publicly available on Kaggle 1. We hope that this contribution encourages other researchers to utilise VREED further to understand emotional responses in VR and ultimately enhance VR experiences design in applications where emotional elicitation plays a key role, i.e. healthcare, gaming, education, etc.},
  keywords = {Affective Computing ACM Reference Format:,CCS Concepts:  Human-centered computing  Virtual reality Additional Key Words and Phrases: Dataset,ECG,GSR,Virtual Reality},
  file = {/Users/nobr/Zotero/storage/FIDNJIF2/full-text.pdf}
}

@techreport{takagi2022,
  type = {Preprint},
  title = {High-Resolution Image Reconstruction with Latent Diffusion Models from Human Brain Activity},
  author = {Takagi, Yu and Nishimoto, Shinji},
  year = {2022},
  month = nov,
  institution = {Neuroscience},
  doi = {10.1101/2022.11.18.517004},
  urldate = {2023-06-19},
  abstract = {Reconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straightforward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs. Please check out our webpage at https://sites.google.com/view/stablediffusion-with-brain/},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/YBDRJTQK/Takagi and Nishimoto - 2022 - High-resolution image reconstruction with latent d.pdf}
}

@book{taleb2020,
  title = {Statistical Consequences of Fat Tails: Real {{World}} Preasymptotics, Epistemology, and Applications: Papers and Commentary},
  shorttitle = {Statistical Consequences of Fat Tails},
  author = {Taleb, Nassim Nicholas},
  year = {2020},
  series = {The {{Technical Incerto Collection}}},
  publisher = {STEM Academic Press},
  address = {USA?},
  isbn = {978-1-5445-0805-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/8LRN6GNE/Taleb - 2020 - Statistical consequences of fat tails real World preasymptotics, epistemology, and applications pa.pdf}
}

@inproceedings{tancik2020,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and {Fridovich-Keil}, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
  year = {2020},
  volume = {33},
  pages = {7537--7547},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-12-03},
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP has impractically slow convergence to high frequency signal components. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  file = {/Users/nobr/Zotero/storage/E2KS3ZTI/Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains.pdf}
}

@misc{tang2023,
  title = {Towards {{CausalGPT}}: {{A Multi-Agent Approach}} for {{Faithful Knowledge Reasoning}} via {{Promoting Causal Consistency}} in {{LLMs}}},
  shorttitle = {Towards {{CausalGPT}}},
  author = {Tang, Ziyi and Wang, Ruilin and Chen, Weixing and Wang, Keze and Liu, Yang and Chen, Tianshui and Lin, Liang},
  year = {2023},
  month = sep,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoners and an evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the evaluator agent scrutinizes if a solution is deducible from a non-causal perspective and if it still holds when challenged by a counterfactual candidate. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/JCT6Q6B6/Tang et al. - 2023 - Towards CausalGPT A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Cons.pdf}
}

@misc{tangSensoryNeuronTransformer2021,
  title = {The {{Sensory Neuron}} as a {{Transformer}}: {{Permutation-Invariant Neural Networks}} for {{Reinforcement Learning}}},
  shorttitle = {The {{Sensory Neuron}} as a {{Transformer}}},
  author = {Tang, Yujin and Ha, David},
  year = {2021},
  month = sep,
  number = {arXiv:2109.02869},
  eprint = {2109.02869},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable. Interactive demo and videos of our results: https://attentionneuron.github.io/},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nobr/Zotero/storage/LEAGTGHQ/Tang and Ha - 2021 - The Sensory Neuron as a Transformer Permutation-Invariant Neural Networks for Reinforcement Learnin.pdf}
}

@article{tao2025,
  title = {Machine-{{Assisted Proof}}},
  author = {Tao, Terence Chi-Shen},
  year = {2025},
  month = jan,
  journal = {Notices of the American Mathematical Society},
  volume = {72},
  number = {01},
  pages = {1},
  issn = {0002-9920, 1088-9477},
  doi = {10.1090/noti3041},
  urldate = {2025-08-02},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JWL8A39D/Tao - 2025 - Machine-Assisted Proof.pdf}
}

@misc{tarzanagh2023,
  title = {Transformers as {{Support Vector Machines}}},
  author = {Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  year = {2023},
  month = sep,
  number = {arXiv:2308.16898},
  eprint = {2308.16898},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2023-09-15},
  abstract = {Since its inception in ``Attention Is All You Need'', the transformer architecture has led to revolutionary advancements in natural language processing. The attention layer within the transformer admits a sequence of input tokens X and makes them interact through pairwise similarities computed as softmax(XQK{$\top$} X{$\top$}), where (K, Q) are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism builds on [TLZO23] and allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent, as follows. (1) Optimizing the attention layer, parameterized by (K, Q), with vanishing regularization, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter W := KQ{$\top$}. Instead, directly parameterizing by W minimizes a Frobenius norm SVM objective. We characterize this convergence, highlighting that it can occur in locally-optimal directions rather than global ones. (2) Complementing this, for W-parameterization, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias of 1-layer transformers with nonlinear heads/MLPs. Our findings apply to general datasets, trivially extend to cross-attention layer, and their practical validity is verified via thorough numerical experiments. We also introduce open problems and future research directions. We believe these findings inspire a new perspective, interpreting multilayer transformers as a hierarchy of SVMs that separates and selects optimal tokens.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/nobr/Zotero/storage/GCQWSUX2/Tarzanagh et al. - 2023 - Transformers as Support Vector Machines.pdf}
}

@misc{team2024,
  title = {Large {{Concept Models}}: {{Language Modeling}} in a {{Sentence Representation Space}}},
  shorttitle = {Large {{Concept Models}}},
  author = {{team}, L. C. M. and Barrault, Lo{\"i}c and Duquenne, Paul-Ambroise and Elbayad, Maha and Kozhevnikov, Artyom and Alastruey, Belen and Andrews, Pierre and Coria, Mariano and Couairon, Guillaume and {Costa-juss{\`a}}, Marta R. and Dale, David and Elsahar, Hady and Heffernan, Kevin and Janeiro, Jo{\~a}o Maria and Tran, Tuan and Ropers, Christophe and S{\'a}nchez, Eduardo and Roman, Robin San and Mourachko, Alexandre and Saleem, Safiyyah and Schwenk, Holger},
  year = {2024},
  month = dec,
  number = {arXiv:2412.08821},
  eprint = {2412.08821},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.08821},
  urldate = {2025-08-02},
  abstract = {LLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for many tasks. The current established technology of LLMs is to process input and generate output at the token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well beyond single words, to analyze information and to generate creative content. In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a concept. Concepts are language- and modality-agnostic and represent a higher level idea or action in a flow. Hence, we build a "Large Concept Model". In this study, as proof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence embedding space, SONAR, which supports up to 200 languages in both text and speech modalities. The Large Concept Model is trained to perform autoregressive sentence prediction in an embedding space. We explore multiple approaches, namely MSE regression, variants of diffusion-based generation, and models operating in a quantized SONAR space. These explorations are performed using 1.6B parameter models and training data in the order of 1.3T tokens. We then scale one architecture to a model size of 7B parameters and training data of about 2.7T tokens. We perform an experimental evaluation on several generative tasks, namely summarization and a new task of summary expansion. Finally, we show that our model exhibits impressive zero-shot generalization performance to many languages, outperforming existing LLMs of the same size. The training code of our models is freely available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/2PUREIYI/team et al. - 2024 - Large Concept Models Language Modeling in a Sentence Representation Space.pdf}
}

@misc{team2025,
  title = {Gemma 3 {{Technical Report}}},
  author = {Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and Rouillard, Louis and Mesnard, Thomas and Cideron, Geoffrey and Grill, Jean-bastien and Ramos, Sabela and Yvinec, Edouard and Casbon, Michelle and Pot, Etienne and Penchev, Ivo and Liu, Ga{\"e}l and Visin, Francesco and Kenealy, Kathleen and Beyer, Lucas and Zhai, Xiaohai and Tsitsulin, Anton and {Busa-Fekete}, Robert and Feng, Alex and Sachdeva, Noveen and Coleman, Benjamin and Gao, Yi and Mustafa, Basil and Barr, Iain and Parisotto, Emilio and Tian, David and Eyal, Matan and Cherry, Colin and Peter, Jan-Thorsten and Sinopalnikov, Danila and Bhupatiraju, Surya and Agarwal, Rishabh and Kazemi, Mehran and Malkin, Dan and Kumar, Ravin and Vilar, David and Brusilovsky, Idan and Luo, Jiaming and Steiner, Andreas and Friesen, Abe and Sharma, Abhanshu and Sharma, Abheesht and Gilady, Adi Mayrav and Goedeckemeyer, Adrian and Saade, Alaa and Feng, Alex and Kolesnikov, Alexander and Bendebury, Alexei and Abdagic, Alvin and Vadi, Amit and Gy{\"o}rgy, Andr{\'a}s and Pinto, Andr{\'e} Susano and Das, Anil and Bapna, Ankur and Miech, Antoine and Yang, Antoine and Paterson, Antonia and Shenoy, Ashish and Chakrabarti, Ayan and Piot, Bilal and Wu, Bo and Shahriari, Bobak and Petrini, Bryce and Chen, Charlie and Lan, Charline Le and {Choquette-Choo}, Christopher A. and Carey, C. J. and Brick, Cormac and Deutsch, Daniel and Eisenbud, Danielle and Cattle, Dee and Cheng, Derek and Paparas, Dimitris and Sreepathihalli, Divyashree Shivakumar and Reid, Doug and Tran, Dustin and Zelle, Dustin and Noland, Eric and Huizenga, Erwin and Kharitonov, Eugene and Liu, Frederick and Amirkhanyan, Gagik and Cameron, Glenn and Hashemi, Hadi and {Klimczak-Pluci{\'n}ska}, Hanna and Singh, Harman and Mehta, Harsh and Lehri, Harshal Tushar and Hazimeh, Hussein and Ballantyne, Ian and Szpektor, Idan and Nardini, Ivan and {Pouget-Abadie}, Jean and Chan, Jetha and Stanton, Joe and Wieting, John and Lai, Jonathan and Orbay, Jordi and Fernandez, Joseph and Newlan, Josh and Ji, Ju-yeong and Singh, Jyotinder and Black, Kat and Yu, Kathy and Hui, Kevin and Vodrahalli, Kiran and Greff, Klaus and Qiu, Linhai and Valentine, Marcella and Coelho, Marina and Ritter, Marvin and Hoffman, Matt and Watson, Matthew and Chaturvedi, Mayank and Moynihan, Michael and Ma, Min and Babar, Nabila and Noy, Natasha and Byrd, Nathan and Roy, Nick and Momchev, Nikola and Chauhan, Nilay and Sachdeva, Noveen and Bunyan, Oskar and Botarda, Pankil and Caron, Paul and Rubenstein, Paul Kishan and Culliton, Phil and Schmid, Philipp and Sessa, Pier Giuseppe and Xu, Pingmei and Stanczyk, Piotr and Tafti, Pouya and Shivanna, Rakesh and Wu, Renjie and Pan, Renke and Rokni, Reza and Willoughby, Rob and Vallu, Rohith and Mullins, Ryan and Jerome, Sammy and Smoot, Sara and Girgin, Sertan and Iqbal, Shariq and Reddy, Shashir and Sheth, Shruti and P{\~o}der, Siim and Bhatnagar, Sijal and Panyam, Sindhu Raghuram and Eiger, Sivan and Zhang, Susan and Liu, Tianqi and Yacovone, Trevor and Liechty, Tyler and Kalra, Uday and Evci, Utku and Misra, Vedant and Roseberry, Vincent and Feinberg, Vlad and Kolesnikov, Vlad and Han, Woohyun and Kwon, Woosuk and Chen, Xi and Chow, Yinlam and Zhu, Yuvein and Wei, Zichuan and Egyed, Zoltan and Cotruta, Victor and Giang, Minh and Kirk, Phoebe and Rao, Anand and Black, Kat and Babar, Nabila and Lo, Jessica and Moreira, Erica and Martins, Luiz Gustavo and Sanseviero, Omar and Gonzalez, Lucas and Gleicher, Zach and Warkentin, Tris and Mirrokni, Vahab and Senter, Evan and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and Hadsell, Raia and Matias, Yossi and Sculley, D. and Petrov, Slav and Fiedel, Noah and Shazeer, Noam and Vinyals, Oriol and Dean, Jeff and Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement and Buchatskaya, Elena and Alayrac, Jean-Baptiste and Anil, Rohan and Dmitry and Lepikhin and Borgeaud, Sebastian and Bachem, Olivier and Joulin, Armand and Andreev, Alek and Hardin, Cassidy and Dadashi, Robert and Hussenot, L{\'e}onard},
  year = {2025},
  month = mar,
  number = {arXiv:2503.19786},
  eprint = {2503.19786},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.19786},
  urldate = {2025-06-27},
  abstract = {We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/BHM6KSSH/Team et al. - 2025 - Gemma 3 Technical Report.pdf;/Users/nobr/Zotero/storage/XLG4EXCS/2503.html}
}

@article{tenachi2023,
  title = {Deep Symbolic Regression for Physics Guided by Units Constraints: Toward the Automated Discovery of Physical Laws},
  shorttitle = {Deep Symbolic Regression for Physics Guided by Units Constraints},
  author = {Tenachi, Wassim and Ibata, Rodrigo and Diakogiannis, Foivos I.},
  year = {2023},
  month = dec,
  journal = {The Astrophysical Journal},
  volume = {959},
  number = {2},
  eprint = {2303.03192},
  primaryclass = {astro-ph, physics:physics},
  pages = {99},
  issn = {0004-637X, 1538-4357},
  doi = {10.3847/1538-4357/ad014c},
  urldate = {2024-01-22},
  abstract = {Symbolic Regression is the study of algorithms that automate the search for analytic expressions that fit data. While recent advances in deep learning have generated renewed interest in such approaches, the development of symbolic regression methods has not been focused on physics, where we have important additional constraints due to the units associated with our data. Here we present \${\textbackslash}Phi\$-SO, a Physical Symbolic Optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints. Our system is built, from the ground up, to propose solutions where the physical units are consistent by construction. This is useful not only in eliminating physically impossible solutions, but because the "grammatical" rules of dimensional analysis restrict enormously the freedom of the equation generator, thus vastly improving performance. The algorithm can be used to fit noiseless data, which can be useful for instance when attempting to derive an analytical property of a physical model, and it can also be used to obtain analytical approximations to noisy data. We test our machinery on a standard benchmark of equations from the Feynman Lectures on Physics and other physics textbooks, achieving state-of-the-art performance in the presence of noise (exceeding 0.1\%) and show that it is robust even in the presence of substantial (10\%) noise. We showcase its abilities on a panel of examples from astrophysics.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,Physics - Computational Physics},
  file = {/Users/nobr/Zotero/storage/W93Z5TGF/Tenachi et al. - 2023 - Deep symbolic regression for physics guided by uni.pdf;/Users/nobr/Zotero/storage/H925T5HC/2303.html}
}

@article{tercan2021,
  title = {Land Suitability Assessment for Wind Farms through Best-Worst Method and {{GIS}} in {{Bal{\i}kesir}} Province of {{Turkey}}},
  author = {Tercan, Emre},
  year = {2021},
  month = oct,
  journal = {Sustainable Energy Technologies and Assessments},
  volume = {47},
  pages = {101491},
  issn = {22131388},
  doi = {10.1016/j.seta.2021.101491},
  urldate = {2023-07-13},
  abstract = {Land suitability assessment for wind farms is a considerable step towards sustainable land use planning but also environmental management and protection. The goal of this study was to develop a Geographical Information Systems (GIS)-Best Worst Method (BWM) based ecomentalist land suitability model to identify optimal locations for wind farms, and to evaluate existing planning preferences. Nine assessment criteria and nineteen constraints were used, and BWM was implemented to reckon the criteria weights. The wind velocity was designated to be the most outstanding assessment criterion, followed by land cover/use, proximity to specific conservation zones, proximity to urban settlements, slope, proximity to wetlands and important bird zones, and others. Standardized thematic suitability maps were presented using Weighted Overlay approach in GIS environment. The findings indicate that 40.90\% of the study zone is suitable, and 54.39\% of available wind turbines coincide with the suitable classes proposed in this study. This indicates that 45.61\% of the real planning preferences are made without considering environmental conditions (e.g. visual impact, noise impact, deforestation, habitat damage to flora and fauna, and land fragmentation), land use features, and disaster conditions (e.g. snowslides, landslides, flood zones). This study provides an ecomentalist, conservationist, and sustainable background for siting wind farms.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GJIALP9C/Tercan - 2021 - Land suitability assessment for wind farms through.pdf}
}

@misc{terrabrasillis,
  title = {{{TerraBrasilis}} - Deforestation Dashboard - {{PRODES}}}
}

@article{terry2021pettingzoo,
  title = {Pettingzoo: {{Gym}} for Multi-Agent Reinforcement Learning},
  author = {Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and {Perez-Vicente}, Rodrigo and others},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {15032--15043}
}

@article{tersteege2020,
  title = {Biased-Corrected Richness Estimates for the {{Amazonian}} Tree Flora},
  author = {{ter Steege}, Hans and Prado, Paulo I. and de Lima, Renato A. F. and Pos, Edwin and {de Souza Coelho}, Luiz and {de Andrade Lima Filho}, Diogenes and Salom{\~a}o, Rafael P. and Amaral, I{\^e}da Le{\~a}o and {de Almeida Matos}, Francisca Dion{\'i}zia and Castilho, Carolina V. and Phillips, Oliver L. and Guevara, Juan Ernesto and {de Jesus Veiga Carim}, Marcelo and C{\'a}rdenas L{\'o}pez, Dairon and Magnusson, William E. and Wittmann, Florian and Martins, Maria Pires and Sabatier, Daniel and Irume, Mariana Vict{\'o}ria and {da Silva Guimar{\~a}es}, Jos{\'e} Renan and Molino, Jean-Fran{\c c}ois and B{\'a}nki, Olaf S. and Piedade, Maria Teresa Fernandez and Pitman, Nigel C. A. and Ramos, Jos{\'e} Ferreira and Monteagudo Mendoza, Abel and Venticinque, Eduardo Martins and Luize, Bruno Garcia and N{\'u}{\~n}ez Vargas, Percy and Silva, Thiago Sanna Freire and {de Le{\~a}o Novo}, Evlyn M{\'a}rcia Moraes and Reis, Neidiane Farias Costa and Terborgh, John and Manzatto, Angelo Gilberto and Casula, Katia Regina and Honorio Coronado, Euridice N. and Montero, Juan Carlos and Duque, Alvaro and Costa, Fl{\'a}via R. C. and Casta{\~n}o Arboleda, Nicol{\'a}s and Sch{\"o}ngart, Jochen and Zartman, Charles Eugene and Killeen, Timothy J. and Marimon, Beatriz S. and {Marimon-Junior}, Ben Hur and Vasquez, Rodolfo and Mostacedo, Bonifacio and Demarchi, Layon O. and Feldpausch, Ted R. and Engel, Julien and Petronelli, Pascal and Baraloto, Chris and Assis, Rafael L. and Castellanos, Hern{\'a}n and Simon, Marcelo Fragomeni and {de Medeiros}, Marcelo Brilhante and Quaresma, Adriano and Laurance, Susan G. W. and Rinc{\'o}n, Lorena M. and Andrade, Ana and Sousa, Thaiane R. and Camargo, Jos{\'e} Lu{\'i}s and Schietti, Juliana and Laurance, William F. and {de Queiroz}, Helder Lima and Nascimento, Henrique Eduardo Mendon{\c c}a and Lopes, Maria Aparecida and {de Sousa Farias}, Emanuelle and Magalh{\~a}es, Jos{\'e} Leonardo Lima and Brienen, Roel and Aymard C., Gerardo A. and Revilla, Juan David Cardenas and Vieira, Ima C{\'e}lia Guimar{\~a}es and Cintra, Bruno Bar{\c c}ante Ladvocat and Stevenson, Pablo R. and Feitosa, Yuri Oliveira and Duivenvoorden, Joost F. and Mogoll{\'o}n, Hugo F. and {Araujo-Murakami}, Alejandro and Ferreira, Leandro Valle and Lozada, Jos{\'e} Rafael and Comiskey, James A. and {de Toledo}, Jos{\'e} Julio and Damasco, Gabriel and D{\'a}vila, N{\'a}llarett and Lopes, Aline and {Garc{\'i}a-Villacorta}, Roosevelt and Draper, Freddie and Vicentini, Alberto and Cornejo Valverde, Fernando and Lloyd, Jon and Gomes, Vitor H. F. and Neill, David and Alonso, Alfonso and Dallmeier, Francisco and {de Souza}, Fernanda Coelho and Gribel, Rogerio and Arroyo, Luzmila and Carvalho, Fernanda Antunes and {de Aguiar}, Daniel Praia Portela and {do Amaral}, D{\'a}rio Dantas and Pansonato, Marcelo Petratti and Feeley, Kenneth J. and Berenguer, Erika and Fine, Paul V. A. and Guedes, Marcelino Carneiro and Barlow, Jos and Ferreira, Joice and Villa, Boris and Pe{\~n}uela Mora, Maria Cristina and Jimenez, Eliana M. and Licona, Juan Carlos and Cer{\'o}n, Carlos and Thomas, Raquel and Maas, Paul and Silveira, Marcos and Henkel, Terry W. and Stropp, Juliana and Paredes, Marcos R{\'i}os and Dexter, Kyle G. and Daly, Doug and Baker, Tim R. and {Huamantupa-Chuquimaco}, Isau and Milliken, William and Pennington, Toby and Tello, J. Sebasti{\'a}n and Pena, Jos{\'e} Luis Marcelo and Peres, Carlos A. and Klitgaard, Bente and Fuentes, Alfredo and Silman, Miles R. and Di Fiore, Anthony and {von Hildebrand}, Patricio and Chave, Jerome and {van Andel}, Tinde R. and Hil{\'a}rio, Renato Richard and Phillips, Juan Fernando and {Rivas-Torres}, Gonzalo and Noronha, Jana{\'i}na Costa and Prieto, Adriana and Gonzales, Therany and {de S{\'a} Carpanedo}, Rainiellene and Gonzales, George Pepe Gallardo and G{\'o}mez, Ricardo Z{\'a}rate and {de Jesus Rodrigues}, Domingos and Zent, Egle{\'e} L. and Ruschel, Ademir R. and Vos, Vincent Antoine and Fonty, {\'E}mile and Junqueira, Andr{\'e} Braga and Doza, Hilda Paulette D{\'a}vila and Hoffman, Bruce and Zent, Stanford and Barbosa, Edelcilio Marques and Malhi, Yadvinder and {de Matos Bonates}, Luiz Carlos and {de Andrade Miranda}, Ires Paula and Silva, Natalino and Barbosa, Fl{\'a}via Rodrigues and Vela, C{\'e}sar I. A. and Pinto, Linder Felipe Mozombite and Rudas, Agust{\'i}n and Albuquerque, Bianca Weiss and Uma{\~n}a, Maria Natalia and Carrero M{\'a}rquez, Yrma Andreina and {van der Heijden}, Geertje and Young, Kenneth R. and Tirado, Milton and Correa, Diego F. and Sierra, Rodrigo and Costa, Janaina Barbosa Pedrosa and Rocha, Maira and Vilanova Torre, Emilio and Wang, Ophelia and Oliveira, Alexandre A. and Kalamandeen, Michelle and Vriesendorp, Corine and {Ramirez-Angulo}, Hirma and Holmgren, Milena and Nascimento, Marcelo Trindade and Galbraith, David and Flores, Bernardo Monteiro and Scudeller, Veridiana Vizoni and Cano, Angela and Ahuite Reategui, Manuel Augusto and Mesones, Italo and Baider, Cl{\'a}udia and Mendoza, Casimiro and Zagt, Roderick and Urrego Giraldo, Ligia Estela and Ferreira, Cid and Villarroel, Daniel and {Linares-Palomino}, Reynaldo and {Farfan-Rios}, William and {Farfan-Rios}, William and Casas, Luisa Fernanda and C{\'a}rdenas, Sasha and Balslev, Henrik and {Torres-Lezama}, Armando and Alexiades, Miguel N. and {Garcia-Cabrera}, Karina and Valenzuela Gamarra, Luis and Valderrama Sandoval, Elvis H. and Ramirez Arevalo, Freddy and Hernandez, Lionel and Sampaio, Adeilza Felipe and Pansini, Susamar and Palacios Cuenca, Walter and {de Oliveira}, Edmar Almeida and Pauletto, Daniela and Levesley, Aurora and Melga{\c c}o, Karina and Pickavance, Georgia},
  year = {2020},
  month = jun,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {10130},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-66686-3},
  urldate = {2023-10-16},
  abstract = {Amazonian forests are extraordinarily diverse, but the estimated species richness is very much debated. Here, we apply an ensemble of parametric estimators and a novel technique that includes conspecific spatial aggregation to an extended database of forest plots with up-to-date taxonomy. We show that the species abundance distribution of Amazonia is best approximated by a logseries with aggregated individuals, where aggregation increases with rarity. By averaging several methods to estimate total richness, we confirm that over 15,000 tree species are expected to occur in Amazonia. We also show that using ten times the number of plots would result in an increase to just {\textasciitilde}50\% of those 15,000 estimated species. To get a more complete sample of all tree species, rigorous field campaigns may be needed but the number of trees in Amazonia will remain an estimate for years to come.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Community ecology,Theoretical ecology},
  file = {/Users/nobr/Zotero/storage/9KIRKWIT/ter Steege et al. - 2020 - Biased-corrected richness estimates for the Amazonian tree flora.pdf}
}

@article{tesau1995,
  title = {Temporal Difference Learning and {{TD-Gammon}}},
  author = {Tesau, Covid and Tesau, Gerald},
  year = {1995},
  month = mar,
  journal = {Communications of the ACM},
  volume = {38},
  number = {3},
  pages = {58--68},
  issn = {15577317},
  doi = {10.1145/203330.203343},
  urldate = {2023-03-21},
  abstract = {Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and ...},
  keywords = {/unread},
  annotation = {ACM\\
		PUB27\\
		New York, NY, USA},
  file = {/Users/nobr/Zotero/storage/ZKE3LV9B/full-text.pdf}
}

@article{tesauro1993,
  title = {{{TD-Gammon}}, {{A Self-Teaching Backgammon Program}}, {{Achieves Master-Level Play}}},
  author = {Tesauro, Gerald},
  year = {1993},
  urldate = {2023-03-15},
  abstract = {TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results, based on the TD(A) reinforcement learning algorithm (Sutton, 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e. given only a "raw" description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set of hand-crafted features is added to the network's input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players.},
  file = {/Users/nobr/Zotero/storage/TUDL28MX/Tesauro - 1993 - TD-Gammon, A Self-Teaching Backgammon Program, Achieves Master-Level Play.pdf}
}

@techreport{theresa2014,
  title = {Maria {{Theresa}}, "{{The Turk}}," and {{Habsburg Nostalgia}}},
  author = {Theresa, Maria and Turk, The and Nostalgia, Habsburg and Bridges, Elizabeth},
  year = {2014},
  journal = {Source: Journal of Austrian Studies},
  volume = {47},
  number = {2},
  pages = {17--36},
  file = {/Users/nobr/Zotero/storage/D8CAUP7L/full-text.pdf}
}

@article{thimoteo2022,
  title = {Explainable {{Ensemble Learning}} for {{Alzheimer}}'s {{Disease Diagnosis}}},
  author = {Thimoteo, Lucas Martins},
  year = {2022},
  abstract = {Thimoteo, Lucas Martins; Vellasco, Marley Maria Bernardes Rebuzzi (Advisor); Amaral, Jorge Lu{\'i}s Machado Do (Co-Advisor). Explainable Ensemble Learning for Alzheimer's Disease Diagnosis. Rio de Janeiro, 2022. 119p. Disserta{\c c}{\~a}o de Mestrado -- Departamento de Engenharia El{\'e}trica, Pontif{\'i}cia Universidade Cat{\'o}lica do Rio de Janeiro.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/CLFA5WLJ/Thimoteo - Explainable Ensemble Learning for Alzheimer’s Dise.pdf}
}

@article{thomas2023,
  title = {Benchmarking Explanation Methods for Mental State Decoding with Deep Learning Models},
  author = {Thomas, Armin W. and R{\'e}, Christopher and Poldrack, Russell A.},
  year = {2023},
  month = jun,
  journal = {NeuroImage},
  volume = {273},
  pages = {120109},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2023.120109},
  urldate = {2023-05-10},
  abstract = {Deep learning (DL) models find increasing application in mental state decoding, where researchers seek to understand the mapping between mental states (e.g., experiencing anger or joy) and brain activity by identifying those spatial and temporal features of brain activity that allow to accurately identify (i.e., decode) these states. Once a DL model has been trained to accurately decode a set of mental states, neuroimaging researchers often make use of methods from explainable artificial intelligence research to understand the model's learned mappings between mental states and brain activity. Here, we benchmark prominent explanation methods in a mental state decoding analysis of multiple functional Magnetic Resonance Imaging (fMRI) datasets. Our findings demonstrate a gradient between two key characteristics of an explanation in mental state decoding, namely, its faithfulness and its alignment with other empirical evidence on the mapping between brain activity and decoded mental state: explanation methods with high explanation faithfulness, which capture the model's decision process well, generally provide explanations that align less well with other empirical evidence than the explanations of methods with less faithfulness. Based on our findings, we provide guidance for neuroimaging researchers on how to choose an explanation method to gain insight into the mental state decoding decisions of DL models.},
  langid = {english},
  keywords = {Benchmark,Deep learning,Explainable AI,Mental state decoding,Neuroimaging},
  file = {/Users/nobr/Zotero/storage/NP3XHV8N/Thomas et al. - 2023 - Benchmarking explanation methods for mental state .pdf;/Users/nobr/Zotero/storage/BYUWBFU2/S1053811923002550.html}
}

@article{tiedau2024,
  title = {Laser {{Excitation}} of the {{Th-229 Nucleus}}},
  author = {Tiedau, J. and Okhapkin, M. V. and Zhang, K. and Thielking, J. and Zitzer, G. and Peik, E. and Schaden, F. and Pronebner, T. and Morawetz, I. and De Col, L. Toscani and Schneider, F. and Leitner, A. and Pressler, M. and Kazakov, G. A. and Beeks, K. and Sikorsky, T. and Schumm, T.},
  year = {2024},
  month = apr,
  journal = {Physical Review Letters},
  volume = {132},
  number = {18},
  pages = {182501},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.132.182501},
  urldate = {2024-05-25},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/9UPEEAFX/Tiedau et al. - 2024 - Laser Excitation of the Th-229 Nucleus.pdf}
}

@book{todd2023,
  title = {80,000 {{Hours}}: Find a Fulfilling Career That Does Good},
  shorttitle = {80,000 {{Hours}}},
  author = {Todd, Benjamin},
  year = {2023},
  publisher = {Trojan House},
  address = {Oxford},
  abstract = {You have about 80,000 hours in your career. This means your choice of career is one of the most important decisions you'll ever make. Choose well, and you can help solve the world's most pressing problems, while having a more rewarding, interesting life. This guide will help you do that. It's based on ten years of research alongside academics at the University of Oxford},
  isbn = {978-1-3999-5709-0},
  langid = {english},
  annotation = {OCLC: 1409430663},
  file = {/Users/nobr/Zotero/storage/JX63RPAU/Todd - 2023 - 80,000 Hours find a fulfilling career that does good.pdf}
}

@article{togelius2011,
  title = {Search-Based Procedural Content Generation: {{A}} Taxonomy and Survey},
  author = {Togelius, Julian and Yannakakis, Georgios N. and Stanley, Kenneth O. and Browne, Cameron},
  year = {2011},
  month = sep,
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {3},
  number = {3},
  pages = {172--186},
  issn = {1943068X},
  doi = {10.1109/TCIAIG.2011.2148116},
  urldate = {2023-03-20},
  abstract = {The focus of this survey is on research in applying evolutionary and other metaheuristic search algorithms to automatically generating content for games, both digital and nondigital (such as board games). The term search-based procedural content generation is proposed as the name for this emerging field, which at present is growing quickly. A taxonomy for procedural content generation is devised, centering on what kind of content is generated, how the content is represented and how the quality/fitness of the content is evaluated; search-based procedural content generation in particular is situated within this taxonomy. This article also contains a survey of all published papers known to the authors in which game content is generated through search or optimisation, and ends with an overview of important open research problems. {\copyright} 2011 IEEE.},
  keywords = {Computer graphics,design automation,evolutionary computation,genetic algorithms},
  file = {/Users/nobr/Zotero/storage/43AW4QWW/full_text.pdf}
}

@book{tolstoy2015,
  title = {The {{Death}} of {{Ivan Ilyich}} and {{Other Stories}}},
  author = {Tolstoy, Leo},
  editor = {Kahn, Andrew},
  translator = {Pasternak Slater, Nicolas},
  year = {2015},
  month = jan,
  edition = {1},
  publisher = {Oxford University Press},
  doi = {10.1093/owc/9780199669882.001.0001},
  urldate = {2024-08-11},
  abstract = {`No one pitied him as he would have liked to be pitied.' As Ivan Ilyich lies dying he begins to re-evaluate his life, searching for meaning that will make sense of his sufferings. In `The Death of Ivan Ilyich' and the other works in this volume, Tolstoy conjures characters who, tested to the limit, reveal glorious and unexpected reserves of courage or baseness of a near inhuman kind. Two vivid parables and `The Forged Coupon', a tale of criminality, explore class relations after the emancipation of the serfs in 1861 and the connection between an ethical life and worldly issues. In `Master and Workman' Tolstoy creates one of his most gripping dramas about human relationships put to the test in an extreme situation. `The Death of Ivan Ilyich' is an existential masterpiece, a biting satire that recounts with extraordinary power the final illness and death of a bourgeois lawyer. In his Introduction Andrew Kahn explores Tolstoy's moral concerns and the stylistic features of these late stories, sensitively translated by Nicolas Pasternak Slater.},
  isbn = {978-0-19-966988-2 978-0-19-192365-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LCY8DHZZ/Tolstoy - 2015 - The Death of Ivan Ilyich and Other Stories.pdf}
}

@article{tong2012,
  title = {Decoding {{Patterns}} of {{Human Brain Activity}}},
  author = {Tong, Frank and Pratte, Michael S.},
  year = {2012},
  journal = {Annual review of psychology},
  volume = {63},
  pages = {483--509},
  issn = {0066-4308},
  doi = {10.1146/annurev-psych-120710-100412},
  urldate = {2023-05-13},
  abstract = {Considerable information about mental states can be decoded from non-invasive measures of human brain activity. Analyses of brain activity patterns can reveal what a person is seeing, perceiving, attending to, or remembering. Moreover, multidimensional models can be used to investigate how the brain encodes complex visual scenes or abstract semantic information. Such feats of ``brain reading'' or ``mind reading'', though impressive, raise important conceptual, methodological, and ethical issues. What does successful decoding reveal about the cognitive functions performed by a brain region? How should brain signals be spatially selected and mathematically combined, to ensure that decoding reflects inherent computations of the brain rather than those performed by the decoder? We will highlight recent advances and describe how multivoxel pattern analysis (MVPA) can provide a window into mind-brain relationships with unprecedented specificity, when carefully applied. However, as brain-reading technology advances, issues of neuroethics and mental privacy will be important to consider.},
  pmcid = {PMC7869795},
  pmid = {21943172},
  file = {/Users/nobr/Zotero/storage/YW24RG3C/Tong and Pratte - 2012 - Decoding Patterns of Human Brain Activity.pdf}
}

@article{tong2022,
  title = {Transdiagnostic Connectome Signatures from Resting-State {{fMRI}} Predict Individual-Level Intellectual Capacity},
  author = {Tong, Xiaoyu and Xie, Hua and Carlisle, Nancy and Fonzo, Gregory A. and Oathes, Desmond J. and Jiang, Jing and Zhang, Yu},
  year = {2022},
  month = sep,
  journal = {Translational Psychiatry},
  volume = {12},
  number = {1},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  issn = {2158-3188},
  doi = {10.1038/s41398-022-02134-2},
  urldate = {2023-05-13},
  abstract = {Medication and other therapies for psychiatric disorders show unsatisfying efficacy, in part due to the significant clinical/ biological heterogeneity within each disorder and our over-reliance on categorical clinical diagnoses. Alternatively, dimensional transdiagnostic studies have provided a promising pathway toward realizing personalized medicine and improved treatment outcomes. One factor that may influence response to psychiatric treatments is cognitive function, which is reflected in one's intellectual capacity. Intellectual capacity is also reflected in the organization and structure of intrinsic brain networks. Using a large transdiagnostic cohort (n\,=\,1721), we sought to discover neuroimaging biomarkers by developing a resting-state functional connectome-based prediction model for a key intellectual capacity measure, Full-Scale Intelligence Quotient (FSIQ), across the diagnostic spectrum. Our cross-validated model yielded an excellent prediction accuracy (r\,=\,0.5573, p\,{$<$}\,0.001). The robustness and generalizability of our model was further validated on three independent cohorts (n\,=\,2641). We identified key transdiagnostic connectome signatures underlying FSIQ capacity involving the dorsal-attention, frontoparietal and default-mode networks. Meanwhile, diagnosis groups showed disorder-specific biomarker patterns. Our findings advance the neurobiological understanding of cognitive functioning across traditional diagnostic categories and provide a new avenue for neuropathological classification of psychiatric disorders.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Predictive markers,Psychiatric disorders},
  file = {/Users/nobr/Zotero/storage/PHT2ZAFM/Tong et al. - 2022 - Transdiagnostic connectome signatures from resting.pdf}
}

@misc{torabiBehavioralCloningObservation2018,
  title = {Behavioral {{Cloning}} from {{Observation}}},
  author = {Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  year = {2018},
  month = may,
  number = {arXiv:1805.01954},
  eprint = {1805.01954},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.01954},
  urldate = {2024-02-16},
  abstract = {Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/UDC2EIEW/Torabi et al. - 2018 - Behavioral Cloning from Observation.pdf;/Users/nobr/Zotero/storage/PY44JQ6R/1805.html}
}

@misc{touvronLlama2Open2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2024-01-21},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/NTLJ8I5S/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf;/Users/nobr/Zotero/storage/V9X6XG5M/2307.html}
}

@misc{towers2024,
  title = {Gymnasium: {{A Standard Interface}} for {{Reinforcement Learning Environments}}},
  shorttitle = {Gymnasium},
  author = {Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U. and Cola, Gianluca De and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and {Perez-Vicente}, Rodrigo and Pierr{\'e}, Andrea and Schulhoff, Sander and Tai, Jun Jet and Tan, Hannah and Younis, Omar G.},
  year = {2024},
  month = nov,
  number = {arXiv:2407.17032},
  eprint = {2407.17032},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.17032},
  urldate = {2025-04-17},
  abstract = {Reinforcement Learning (RL) is a continuously growing field that has the potential to revolutionize many areas of artificial intelligence. However, despite its promise, RL research is often hindered by the lack of standardization in environment and algorithm implementations. This makes it difficult for researchers to compare and build upon each other's work, slowing down progress in the field. Gymnasium is an open-source library that provides a standard API for RL environments, aiming to tackle this issue. Gymnasium's main feature is a set of abstractions that allow for wide interoperability between environments and training algorithms, making it easier for researchers to develop and test RL algorithms. In addition, Gymnasium provides a collection of easy-to-use environments, tools for easily customizing environments, and tools to ensure the reproducibility and robustness of RL research. Through this unified framework, Gymnasium significantly streamlines the process of developing and testing RL algorithms, enabling researchers to focus more on innovation and less on implementation details. By providing a standardized platform for RL research, Gymnasium helps to drive forward the field of reinforcement learning and unlock its full potential. Gymnasium is available online at https://github.com/Farama-Foundation/Gymnasium},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/LXFZGR37/Towers et al. - 2024 - Gymnasium A Standard Interface for Reinforcement Learning Environments.pdf}
}

@misc{towers2025,
  title = {Gymnasium: {{A Standard Interface}} for {{Reinforcement Learning Environments}}},
  shorttitle = {Gymnasium},
  author = {Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan K and Balis, John U. and {de Cola}, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and {Perez-Vicente}, Rodrigo and Pierr{\'e}, Andrea and Schulhoff, Sander and Tai, Jun Jet and Tan, Hannah Jin Shen and Younis, Omar G.},
  year = {2025},
  month = mar,
  urldate = {2025-03-19},
  abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
  copyright = {MIT}
}

@article{trinh2024,
  title = {Solving Olympiad Geometry without Human Demonstrations},
  author = {Trinh, Trieu H. and Wu, Yuhuai and Le, Quoc V. and He, He and Luong, Thang},
  year = {2024},
  month = jan,
  journal = {Nature},
  volume = {625},
  number = {7995},
  pages = {476--482},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06747-5},
  urldate = {2024-01-22},
  abstract = {Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1--4, owing to their reputed difficulty among the world's best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a~translated IMO theorem in 2004.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science},
  file = {/Users/nobr/Zotero/storage/YNDG283L/Trinh et al. - 2024 - Solving olympiad geometry without human demonstrations.pdf}
}

@article{Turetsky2019Permafrost,
  title = {Permafrost Collapse Is Accelerating Carbon Release},
  author = {Turetsky, Merritt R. and Abbott, Benjamin W. and Jones, Miriam C. and Anthony, Katey W. Walter and Olefeldt, David and Schuur, Edward A. G. and Koven, Charles and McGuire, A. David and Grosse, Guido and Kuhry, Peter and Hugelius, Gustaf and Lawrence, David M. and Gibson, Corey and Sannel, A. Britta K.},
  year = {2019},
  journal = {Nature},
  volume = {569},
  number = {7754},
  pages = {32--34},
  doi = {10.1038/d41586-019-01313-4}
}

@techreport{turing1950,
  title = {{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {Turing, A M},
  year = {1950},
  journal = {Computing Machinery and Intelligence. Mind},
  volume = {49},
  pages = {433--460},
  file = {/Users/nobr/Zotero/storage/RFBRNMV5/Turing - 1950 - COMPUTING MACHINERY AND INTELLIGENCE.pdf}
}

@incollection{turnitsa2021,
  title = {Front {{Matter}}},
  booktitle = {Simulation and {{Wargaming}}},
  editor = {Turnitsa, Charles and Blais, Curtis and Tolk, Andreas},
  year = {2021},
  month = dec,
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9781119604815.fmatter},
  urldate = {2023-11-12},
  isbn = {978-1-119-60478-5 978-1-119-60481-5},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/876W3U5P/Turnitsa et al. - 2021 - Front Matter.pdf}
}

@article{tversky2015,
  title = {The {{Cognitive Design}} of {{Tools}} of {{Thought}}},
  author = {Tversky, Barbara},
  year = {2015},
  month = mar,
  journal = {Review of Philosophy and Psychology},
  volume = {6},
  number = {1},
  pages = {99--116},
  issn = {1878-5158, 1878-5166},
  doi = {10.1007/s13164-014-0214-3},
  urldate = {2024-07-21},
  abstract = {When thought overwhelms the mind, the mind puts it into the world, notably in diagrams and gestures.Both use space and arrays of elements, depictive and nondepictive, to convey ideas, concrete and abstract,clear and sketchy. The arrays and the non-depictive elements like boxes and arrows serve to showrelationships and organizations, thematic, categorical, and more. on paper, in the air, in the diagrammedworld. Human actions organize space to convey abstractions: spraction.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/7K62ZGMI/Tversky - 2015 - The Cognitive Design of Tools of Thought.pdf}
}

@misc{tyenLLMsCannotFind2023,
  title = {{{LLMs}} Cannot Find Reasoning Errors, but Can Correct Them!},
  author = {Tyen, Gladys and Mansoor, Hassan and Chen, Peter and Mak, Tony and C{\u a}rbune, Victor},
  year = {2023},
  month = nov,
  number = {arXiv:2311.08516},
  eprint = {2311.08516},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.08516},
  urldate = {2023-11-21},
  abstract = {While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70\% accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/SDAYW4EU/Tyen et al. - 2023 - LLMs cannot find reasoning errors, but can correct them!.pdf;/Users/nobr/Zotero/storage/KDIHMFCC/2311.html}
}

@article{tymofiyeva2014,
  title = {Brain without {{Anatomy}}: {{Construction}} and {{Comparison}} of {{Fully Network-Driven Structural MRI Connectomes}}},
  shorttitle = {Brain without {{Anatomy}}},
  author = {Tymofiyeva, Olga and Ziv, Etay and Barkovich, A. James and Hess, Christopher P. and Xu, Duan},
  year = {2014},
  month = may,
  journal = {PLOS ONE},
  volume = {9},
  number = {5},
  pages = {e96196},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0096196},
  urldate = {2023-05-13},
  abstract = {MRI connectomics methods treat the brain as a network and provide new information about its organization, efficiency, and mechanisms of disruption. The most commonly used method of defining network nodes is to register the brain to a standardized anatomical atlas based on the Brodmann areas. This approach is limited by inter-subject variability and can be especially problematic in the context of brain maturation or neuroplasticity (cerebral reorganization after brain damage). In this study, we combined different image processing and network theory methods and created a novel approach that enables atlas-free construction and connection-wise comparison of diffusion MRI-based brain networks. We illustrated the proposed approach in three age groups: neonates, 6-month-old infants, and adults. First, we explored a data-driven method of determining the optimal number of equal-area nodes based on the assumption that all cortical areas of the brain are connected and, thus, no part of the brain is structurally isolated. Second, to enable a connection-wise comparison, alignment to a ``reference brain'' was performed in the network domain within each group using a matrix alignment algorithm with simulated annealing. The correlation coefficients after pair-wise network alignment ranged from 0.6102 to 0.6673. To test the method's reproducibility, one subject from the 6-month-old group and one from the adult group were scanned twice, resulting in correlation coefficients of 0.7443 and 0.7037, respectively. While being less than 1 due to parcellation and noise, statistically, these values were significantly higher than inter-subject values. Rotation of the parcellation largely explained the variability. Through the abstraction from anatomy, the developed framework allows for a fully network-driven analysis of structural MRI connectomes and can be applied to subjects at any stage of development and with substantial differences in cortical anatomy.},
  langid = {english},
  keywords = {Age groups,Brain,Connectomics,Infants,Magnetic resonance imaging,Network analysis,Neural networks,Tractography},
  file = {/Users/nobr/Zotero/storage/LAL4ZWYZ/Tymofiyeva et al. - 2014 - Brain without Anatomy Construction and Comparison.pdf}
}

@article{ulak2019,
  title = {Exploring Alternative Spatial Weights to Detect Crash Hotspots},
  author = {Ulak, Mehmet Baran and Ozguven, Eren Erman and Vanli, O. Arda and Horner, Mark W.},
  year = {2019},
  month = nov,
  journal = {Computers, Environment and Urban Systems},
  volume = {78},
  pages = {101398},
  issn = {01989715},
  doi = {10.1016/j.compenvurbsys.2019.101398},
  urldate = {2023-07-04},
  abstract = {Identifying hotspots or crash clusters is an important problem for detecting high-risk locations at which vehicle accidents frequently occur. Several hotspot identification methods have been developed in the literature; however, there are often large differences between the spatial distributions of hotspots obtained by these methods, and spatial weights such as free flow travel times and congested flow travel times have not been fully examined as alternatives to the weight of distance. This paper compares the following commonly implemented network-based hotspot detection methods: Getis-Ord Gi*, Local Moran's I, KLINCS (K-function local indicators of network-constrained clusters), and KLINCS-IC (Inverse Cost) in order to provide insight into understanding the similarities and differences between selected hotspot detection methods when used with different spatial weights. This assessment is performed through using different spatial weights as part of the statistical analysis and comparing them to a prediction accuracy index. Moreover, a sensitivity analysis was conducted based on alternative spatial weights and different parameters to test the effect of bandwidth on the identified hotspots. The findings on the success of alternative spatial weights has a potential to improve the accuracy of the hotspot detection. Results indicate distinct differences in the spatial distributions of hotspots obtained through the considered methods that are based on alternative spatial weights. An interesting finding is that all alternative approaches (spatial weights and bandwidths) cluster when certain bandwidth values are exceeded. From a practical perspective, the CPAI results show that using network-constrained Local Moran's I statistics with a distance-based spatial weights may provide the most feasible approach while implementing safety-focused efforts.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/B4NGDKJT/Ulak et al. - 2019 - Exploring alternative spatial weights to detect cr.pdf}
}

@techreport{unep2018emissions,
  title = {The Emissions Gap Report 2018},
  author = {{United Nations Environment Programme}},
  year = {2018},
  address = {Nairobi},
  institution = {United Nations Environment Programme},
  isbn = {978-92-807-3726-4}
}

@book{UNEP2019GlobalLinkages,
  title = {Global Linkages: A Graphic Look at the Changing Arctic},
  year = {2019},
  publisher = {{United Nations Environment Programme and GRID-Arendal}},
  address = {Nairobi and Arendal}
}

@misc{vacareanuWordsNumbersYour2024,
  title = {From {{Words}} to {{Numbers}}: {{Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples}}},
  shorttitle = {From {{Words}} to {{Numbers}}},
  author = {Vacareanu, Robert and Negru, Vlad-Andrei and Suciu, Vasile and Surdeanu, Mihai},
  year = {2024},
  month = apr,
  number = {arXiv:2404.07544},
  eprint = {2404.07544},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-22},
  abstract = {We analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates. Our findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman \#2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. We then investigate how well the performance of large language models scales with the number of in-context exemplars. We borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/RH82E8HK/Vacareanu et al. - 2024 - From Words to Numbers Your Large Language Model Is Secretly A Capable Regressor When Given In-Conte.pdf}
}

@article{valdebenito2020,
  title = {Machine Learning Approaches to Study Glioblastoma: {{A}} Review of the Last Decade of Applications},
  author = {Valdebenito, Jessica and Medina, Felipe},
  year = {2020},
  journal = {CANCER REPORTS},
  volume = {2},
  number = {6},
  doi = {10.1002/cnr2.1226},
  file = {/Users/nobr/Zotero/storage/T2GG5Z5F/document.pdf}
}

@misc{valevski2024,
  title = {Diffusion {{Models Are Real-Time Game Engines}}},
  author = {Valevski, Dani and Leviathan, Yaniv and Arar, Moab and Fruchter, Shlomi},
  year = {2024},
  month = aug,
  number = {arXiv:2408.14837},
  eprint = {2408.14837},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.14837},
  urldate = {2024-12-03},
  abstract = {We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/VPYTC39P/Valevski et al. - 2024 - Diffusion Models Are Real-Time Game Engines.pdf;/Users/nobr/Zotero/storage/CU5LJAZY/2408.html}
}

@misc{valevski2024a,
  title = {Diffusion {{Models Are Real-Time Game Engines}}},
  author = {Valevski, Dani and Leviathan, Yaniv and Arar, Moab and Fruchter, Shlomi},
  year = {2024},
  month = aug,
  number = {arXiv:2408.14837},
  eprint = {2408.14837},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-29},
  abstract = {We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/B3DQKCCS/Valevski et al. - 2024 - Diffusion Models Are Real-Time Game Engines.pdf}
}

@article{vanderweel2024,
  title = {Handwriting but Not Typewriting Leads to Widespread Brain Connectivity: A High-Density {{EEG}} Study with Implications for the Classroom},
  shorttitle = {Handwriting but Not Typewriting Leads to Widespread Brain Connectivity},
  author = {Van Der Weel, F. R. (Ruud) and Van Der Meer, Audrey L. H.},
  year = {2024},
  month = jan,
  journal = {Frontiers in Psychology},
  volume = {14},
  pages = {1219945},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1219945},
  urldate = {2024-02-15},
  abstract = {As traditional handwriting is progressively being replaced by digital devices, it is essential to investigate the implications for the human brain. Brain electrical activity was recorded in 36 university students as they were handwriting visually presented words using a digital pen and typewriting the words on a keyboard. Connectivity analyses were performed on EEG data recorded with a 256-channel sensor array. When writing by hand, brain connectivity patterns were far more elaborate than when typewriting on a keyboard, as shown by widespread theta/alpha connectivity coherence patterns between network hubs and nodes in parietal and central brain regions. Existing literature indicates that connectivity patterns in these brain areas and at such frequencies are crucial for memory formation and for encoding new information and, therefore, are beneficial for learning. Our findings suggest that the spatiotemporal pattern from visual and proprioceptive information obtained through the precisely controlled hand movements when using a pen, contribute extensively to the brain's connectivity patterns that promote learning. We urge that children, from an early age, must be exposed to handwriting activities in school to establish the neuronal connectivity patterns that provide the brain with optimal conditions for learning. Although it is vital to maintain handwriting practice at school, it is also important to keep up with continuously developing technological advances. Therefore, both teachers and students should be aware of which practice has the best learning effect in what context, for example when taking lecture notes or when writing an essay.},
  langid = {english}
}

@article{vangelder1995,
  title = {What {{Might Cognition Be}}, {{If Not Computation}}?:},
  shorttitle = {What {{Might Cognition Be}}, {{If Not Computation}}?},
  author = {Van Gelder, Tim and {Journal of Philosophy Inc.}},
  editor = {Smylie, John},
  year = {1995},
  journal = {Journal of Philosophy},
  volume = {92},
  number = {7},
  pages = {345--381},
  issn = {0022-362X},
  doi = {10.2307/2941061},
  urldate = {2023-11-09},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SQJ7U55N/Van Gelder and Journal of Philosophy Inc. - 1995 - What Might Cognition Be, If Not Computation.pdf}
}

@misc{vanhoecke2018,
  title = {Assessment of {{Functional Connectome Construction Strategies}} in {{Neurodegeneration}}},
  author = {Vanhoecke, J. and McColgan, P. and Razi, A. and Gregory, S. and Seunarine, K. and Durr, A. and Roos, R. and Leavitt, B. and Scahill, R. I. and Clark, C. and Tabrizi, S. J. and Rees, G. and Investigators, Track On-HD},
  year = {2018},
  month = aug,
  primaryclass = {New Results},
  pages = {385385},
  publisher = {bioRxiv},
  doi = {10.1101/385385},
  urldate = {2023-05-13},
  abstract = {Connectomics can be used to investigate functional brain networks in neurodegenerative diseases including Huntington's disease (HD). In this developing field, different connectome construction strategies have emerged in parallel. However, there is a need to understand the influences of different strategies on subsequent analyses when constructing a connectome. This study systematically compares connectome construction strategies based on their biological relevance to functional networks in neurodegeneration. We asked which functional connectome construction strategy was best able to discriminate HD gene carriers from healthy controls, and how such a strategy affected modular organization of the network. The major factors compared were principal component-based correction versus wavelet decomposition for physiological noise correction, the type of parcellation atlas (functional, structural and multi-modal), weighted versus binarized networks, and unthresholded versus proportionally thresholded networks. We found that principal component-based correction generated the most discriminatory connectomes, while binarization and proportional thresholding did not increase discrimination between HD gene carriers and healthy controls. When a functional parcellation atlas was used, the highest discrimination rates were obtained. We observed that the group differences in modular organization of the functional connectome were greatly affected by binarization and thresholding, showing no consistent pattern of modularity. This study suggests that functional connectome construction strategies using principal component-based correction and weighted unthresholded connectivity matrices may outperform other strategies.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XWL3A2WJ/Vanhoecke et al. - 2018 - Assessment of Functional Connectome Construction S.pdf}
}

@article{vanlent,
  title = {A {{Tactical}} and {{Strategic AI Interface}} for {{Real-Time Strategy Games}}},
  author = {{van Lent}, Michael and Carpenter, Paul and McAlinden, Ryan and Tan, Poey Guan},
  abstract = {Real Time Strategy (RTS) games present a wide range of AI challenges at the tactical and strategic level. Unfortunately, the lack of flexible ``mod'' interfaces to these games has made it difficult for AI researchers to explore these challenges in the context of RTS games. We are addressing this by building two AI interfaces into Full Spectrum Command, a real time strategy training aid built for the U.S. Army. The tactical AI interface will allow AI systems, such as Soar and Simulation Based Tactics Mining, to control the tactical behavior of platoons and squads within the environment. The strategic AI interface will allow AI planners to generate and adapt higher-level battle plans which can in turn be executed by the tactical AI. This paper describes these two interfaces and our plans for identifying and addressing the research challenges involved in developing and deploying tactical and strategic AI systems.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/H4R27PD3/van Lent et al. - A Tactical and Strategic AI Interface for Real-Time Strategy Games.pdf}
}

@article{vanopheusden2023,
  title = {Expertise Increases Planning Depth in Human Gameplay},
  author = {{van Opheusden}, Bas and Kuperwajs, Ionatan and Galbiati, Gianni and Bnaya, Zahy and Li, Yunqi and Ma, Wei Ji},
  year = {2023},
  month = jun,
  journal = {Nature},
  volume = {618},
  number = {7967},
  pages = {1000--1005},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06124-2},
  urldate = {2024-12-04},
  abstract = {A hallmark of human intelligence is the ability to plan multiple steps into the future1,2. Despite decades of research3--5, it is still debated whether skilled decision-makers plan more steps ahead than novices6--8. Traditionally, the study of expertise in planning has used board games such as chess, but the complexity of these games poses a barrier to quantitative estimates of planning depth. Conversely, common planning tasks in cognitive science often have a lower complexity9,10 and impose a ceiling for the depth to which any player can plan. Here we investigate expertise in a complex board game that offers ample opportunity for skilled players to plan deeply. We use model fitting methods to show that human behaviour can be captured using a computational cognitive model based on heuristic search. To validate this model, we predict human choices, response times and eye movements. We also perform a Turing test and a reconstruction experiment. Using the model, we find robust evidence for increased planning depth with expertise in both laboratory and large-scale mobile data. Experts memorize and reconstruct board features more accurately. Using complex tasks combined with precise behavioural modelling might expand our understanding of human planning and help to bridge the gap with progress in artificial intelligence.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Decision,Human behaviour},
  file = {/Users/nobr/Zotero/storage/5XNT5L6D/van Opheusden et al. - 2023 - Expertise increases planning depth in human gameplay.pdf}
}

@book{vantrees2002,
  title = {Detection, Estimation, and Modulation Theory. 4: {{Optimum}} Array Processing},
  shorttitle = {Detection, Estimation, and Modulation Theory. 4},
  author = {Van Trees, Harry L.},
  year = {2002},
  publisher = {Wiley},
  address = {New York, NY},
  isbn = {978-0-471-09390-9},
  langid = {english}
}

@inproceedings{vasiljevic2018,
  title = {Mental {{War}}: {{An Attention-Based Single}}/{{Multiplayer Brain-Computer Interface Game}}},
  shorttitle = {Mental {{War}}},
  booktitle = {Computational {{Science}} and {{Its Applications}} -- {{ICCSA}} 2018},
  author = {Vasiljevic, Gabriel Alves Mendes and {de Miranda}, Leonardo Cunha and {de Menezes}, Bruna Camila},
  editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Stankova, Elena and Torre, Carmelo M. and Rocha, Ana Maria A.C. and Taniar, David and Apduhan, Bernady O. and Tarantino, Eufemia and Ryu, Yeonseung},
  year = {2018},
  pages = {450--465},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-95162-1_31},
  abstract = {Brain-Computer Interfaces (BCIs) are a novel kind of user interface that allows the recognition of specific user intentions by reading the user's brain activity, translating it into commands and transmitting them to the computer. With increasing advances in the technology behind these interfaces, gaming applications using cerebral input are becoming more common, although the employment of this kind of control for games is still relatively low in comparison to other traditional input modalities. This paper presents Mental War, a brain-controlled multiplayer computer game, in which two or more players are able to compete against each other, or work together in a collaborative mode to achieve a common goal, both using only their mental state alone. Their level of concentration is measured using a single-sensor BCI headset and translated into force to pull a rope in a tug-of-war game. The design and implementation process of the game are detailed and discussed. An evaluation process was performed with a total of 24 participants to acquire qualitative data regarding the interaction with the BCI platform, validating the design and providing insights for developers in future BCI-based research.},
  isbn = {978-3-319-95162-1},
  langid = {english},
  keywords = {BCI,EEG,Evaluation,Games,HCI,NeuroSky MindWave}
}

@misc{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-06-11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/6A7UBTT7/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/nobr/Zotero/storage/JJ65DHJ2/1706.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/DTDPL7K5/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@misc{vatsSurveyHumanAITeaming2024,
  title = {A {{Survey}} on {{Human-AI Teaming}} with {{Large Pre-Trained Models}}},
  author = {Vats, Vanshika and Nizam, Marzia Binta and Liu, Minghao and Wang, Ziyuan and Ho, Richard and Prasad, Mohnish Sai and Titterton, Vincent and Malreddy, Sai Venkat and Aggarwal, Riya and Xu, Yanwen and Ding, Lei and Mehta, Jay and Grinnell, Nathan and Liu, Li and Zhong, Sijia and Gandamani, Devanathan Nallur and Tang, Xinyi and Ghosalkar, Rohan and Shen, Celeste and Shen, Rachel and Hussain, Nafisa and Ravichandran, Kesav and Davis, James},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04931},
  eprint = {2403.04931},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-20},
  abstract = {In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/nobr/Zotero/storage/PB2PFLP4/Vats et al. - 2024 - A Survey on Human-AI Teaming with Large Pre-Trained Models.pdf}
}

@misc{velickovic2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10903},
  urldate = {2024-12-24},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/QJF68Q4H/Veličković et al. - 2018 - Graph Attention Networks.pdf}
}

@book{velleman2019,
  title = {How to {{Prove It}}: {{A Structured Approach}}},
  shorttitle = {How to {{Prove It}}},
  author = {Velleman, Daniel J.},
  year = {2019},
  month = jul,
  edition = {3},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781108539890},
  urldate = {2023-08-17},
  isbn = {978-1-108-53989-0 978-1-108-42418-9 978-1-108-43953-4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/EHVVNSTI/Velleman - 2019 - How to Prove It A Structured Approach.pdf}
}

@book{verny2021,
  title = {The Embodied Mind: Understanding the Mysteries of Cellular Memory, Consciousness, and Our Bodies},
  shorttitle = {The Embodied Mind},
  author = {Verny, Thomas R.},
  year = {2021},
  edition = {First Pegasus Books cloth edition},
  publisher = {Pegasus Books},
  address = {New York, NY},
  abstract = {We understand the workings of the human body as a series of interdependent physiological relationships: muscle interacts with bone as the heart responds to hormones secreted by the brain, all the way down to the inner workings of every cell. To make an organism function, no one component can work alone. In light of this, why is it that the accepted understanding that the physical phenomenon of the mind is attributed only to the brain? In The Embodied Mind, internationally renowned psychiatrist Dr. Thomas R. Verny sets out to redefine our concept of the mind and consciousness. He brilliantly compiles new research that points to the mind's ties to every part of the body. The Embodied Mind collects disparate findings in physiology, genetics, and quantum physics in order to illustrate the mounting evidence that somatic cells, not just neural cells, store memory, inform genetic coding, and adapt to environmental changes-all behaviors that contribute to the mind and consciousness. Cellular memory, Verny shows, is not just an abstraction, but a well-documented scientific fact that will shift our understanding of memory. Verny describes single-celled organisms with no brains demonstrating memory, and points to the remarkable case of a French man who, despite having a brain just a fraction of the typical size, leads a normal life with a family and a job. The Embodied Mind shows how intelligence and consciousness-traits traditionally attributed to the brain alone-also permeate our entire being. Bodily cells and tissues use the same molecular mechanisms for memory as our brain, making our mind more fluid and adaptable than we could have ever imaged.--},
  isbn = {978-1-64313-799-5},
  lccn = {QP411 .V473 2021},
  keywords = {Conscience,Consciousness,Human physiology,Memory,Mind and body,Physiologie humaine},
  annotation = {OCLC: on1236259780},
  file = {/Users/nobr/Zotero/storage/NQR9I3SZ/Verny - 2021 - The embodied mind understanding the mysteries of cellular memory, consciousness, and our bodies.pdf}
}

@article{vestner2019,
  title = {Addressing the {{Use}} of {{Human Shields}}},
  author = {Vestner, Tobias},
  year = {2019},
  journal = {Geneva Centre for Security Policy},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/WPA7VRHY/Vestner - Addressing the Use of Human Shields.pdf}
}

@article{victor2014climate,
  title = {Climate Policy: {{Ditch}} the 2{$^\circ$}{{C}} Warming Goal},
  author = {Victor, David G. and Kennel, Charles F.},
  year = {2014},
  month = oct,
  journal = {Nature},
  volume = {514},
  pages = {30--31}
}

@article{villacreses2017,
  title = {Wind Farms Suitability Location Using Geographical Information System ({{GIS}}), Based on Multi-Criteria Decision Making ({{MCDM}}) Methods: {{The}} Case of Continental {{Ecuador}}},
  shorttitle = {Wind Farms Suitability Location Using Geographical Information System ({{GIS}}), Based on Multi-Criteria Decision Making ({{MCDM}}) Methods},
  author = {Villacreses, Geovanna and Gaona, Gabriel and {Mart{\'i}nez-G{\'o}mez}, Javier and Jij{\'o}n, Diego Juan},
  year = {2017},
  month = aug,
  journal = {Renewable Energy},
  volume = {109},
  pages = {275--286},
  issn = {09601481},
  doi = {10.1016/j.renene.2017.03.041},
  urldate = {2023-07-13},
  abstract = {The aim of this research was to implement a geographical information system with multi-criteria decision making methods, to select the most feasible location for installing wind power plants in continental Ecuador. In addition, a standardization process was performed, which consists of establishing an overall performance index to evaluate the results. Finally, the Pearson correlation coefficient is used to analyze mutual correspondence between multi-criteria decision making methods.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/NZPE4354/Villacreses et al. - 2017 - Wind farms suitability location using geographical.pdf}
}

@misc{villegasPhenakiVariableLength2022,
  title = {Phenaki: {{Variable Length Video Generation From Open Domain Textual Description}}},
  shorttitle = {Phenaki},
  author = {Villegas, Ruben and Babaeizadeh, Mohammad and Kindermans, Pieter-Jan and Moraldo, Hernan and Zhang, Han and Saffar, Mohammad Taghi and Castro, Santiago and Kunze, Julius and Erhan, Dumitru},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02399},
  eprint = {2210.02399},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.02399},
  urldate = {2024-01-07},
  abstract = {We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the per-frame baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/UMJP7EIQ/Villegas et al. - 2022 - Phenaki Variable Length Video Generation From Open Domain Textual Description.pdf;/Users/nobr/Zotero/storage/3ISXQ2ZG/2210.html}
}

@inproceedings{vinyals2015,
  title = {Pointer {{Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-01-02},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that arediscrete tokens corresponding to positions in an input sequence.Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines,because the number of target classes in eachstep of the output depends on the length of the input, which is variable.Problems such as sorting variable sized sequences, and various combinatorialoptimization problems belong to this class.  Our model solvesthe problem of variable size output dictionaries using a recently proposedmechanism of neural attention. It differs from the previous attentionattempts in that, instead of using attention to blend hidden units of anencoder to a context vector at each decoder step, it uses attention asa pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).We show Ptr-Nets can be used to learn approximate solutions to threechallenging geometric problems -- finding planar convex hulls, computingDelaunay triangulations, and the planar Travelling Salesman Problem-- using training examples alone. Ptr-Nets not only improve oversequence-to-sequence with input attention, butalso allow us to generalize to variable size output dictionaries.We show that the learnt models generalize beyond the maximum lengthsthey were trained on. We hope our results on these taskswill encourage a broader exploration of neural learning for discreteproblems.},
  file = {/Users/nobr/Zotero/storage/XU3EYIZA/Vinyals et al. - 2015 - Pointer Networks.pdf}
}

@misc{vinyals2017,
  title = {{{StarCraft II}}: {{A New Challenge}} for {{Reinforcement Learning}}},
  shorttitle = {{{StarCraft II}}},
  author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and {van Hasselt}, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
  year = {2017},
  month = aug,
  number = {arXiv:1708.04782},
  eprint = {1708.04782},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.04782},
  urldate = {2024-01-14},
  abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/MMBS3JDC/Vinyals et al. - 2017 - StarCraft II A New Challenge for Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/QL8QDUWH/1708.html}
}

@article{vinyals2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  urldate = {2023-12-31},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Statistics},
  file = {/Users/nobr/Zotero/storage/YIW93Q4N/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agent reinforcement learning.pdf}
}

@incollection{vitek2020,
  title = {Intelligent Agents in Games: {{Review}} with an Open-Source Tool},
  shorttitle = {Intelligent Agents in Games},
  booktitle = {Advances in {{Computers}}},
  author = {Vitek, Matej and Peer, Peter},
  year = {2020},
  volume = {116},
  pages = {251--303},
  publisher = {Elsevier},
  doi = {10.1016/bs.adcom.2019.07.005},
  urldate = {2023-11-12},
  abstract = {The field of artificial intelligence has come a long way in the last 50 years, and studies of its methods soon expanded to a field in which they are of great practical value---computer games. The concept of intelligent agents provides a much needed theoretical background for the comparison of various different approaches to intelligent, rational behavior of computer-controlled characters in games. By combining rationality with certain limitations to the capabilities of our agents, we can achieve behavior resembling that of a human player, which is also desirable in games. The goal of this article is to introduce various types of agents that are used in games, show how to implement meaningful, reasonable limitations to agent capabilities into the game world, and provide a freely available, open-source application for the comparison of such agents. Additionally, in this article we show that even the simplest agents can succeed in their tasks in certain task environments, whereas more difficult task environments often require a more sophisticated agent architecture. Our application consists of two task environments with nine agents in total but could easily be extended with additional task environments and agent implementations. In the end, we find the addition of goals into the agent architecture has the biggest impact on the agent's behavior and performance, whereas the state-based approach helps keep our implementation simple and compact.},
  isbn = {978-0-12-820196-1},
  langid = {english}
}

@article{vlachopoulos2017,
  title = {The Effect of Games and Simulations on Higher Education: A Systematic Literature Review},
  shorttitle = {The Effect of Games and Simulations on Higher Education},
  author = {Vlachopoulos, Dimitrios and Makri, Agoritsa},
  year = {2017},
  month = jul,
  journal = {International Journal of Educational Technology in Higher Education},
  volume = {14},
  number = {1},
  pages = {22},
  issn = {2365-9440},
  doi = {10.1186/s41239-017-0062-1},
  urldate = {2024-09-09},
  abstract = {The focus of higher education institutions is the preparation of future professionals. To achieve this aim, innovative teaching methods are often deployed, including games and simulations, which form the subject of this paper. As the field of digital games and simulations is ever maturing, this paper attempts to systematically review the literature relevant to games and simulation pedagogy in higher education. Two researchers collaborate to apply a qualitative method, coding and synthesizing the results using multiple criteria. The main objective is to study the impact of games and simulations with regard to achieving specific learning objectives. On balance, results indicate that games and/or simulations have a positive impact on learning goals. The researchers identify three learning outcomes when integrating games into the learning process: cognitive, behavioural, and affective. As a final step, the authors consolidate evidence for the benefit of academics and practitioners in higher education interested in the efficient use of games and simulations for pedagogical purposes. Such evidence also provides potential options and pathways for future research.},
  keywords = {Affective goals,Behavioural goals,Cognitive goals,Digital games,Game-based learning,Higher education,Learning outcomes,Pedagogical use,Simulations},
  file = {/Users/nobr/Zotero/storage/U9DWVRTH/Vlachopoulos and Makri - 2017 - The effect of games and simulations on higher education a systematic literature review.pdf;/Users/nobr/Zotero/storage/HS66MD6W/s41239-017-0062-1.html}
}

@book{vonclausewitz1832,
  title = {On {{War}}},
  author = {{von Clausewitz}, Carl},
  year = {1832},
  urldate = {2025-05-08},
  abstract = {Free kindle book and epub digitized and proofread by volunteers.},
  langid = {american}
}

@article{vonneumann1928,
  title = {{Zur Theorie der Gesellschaftsspiele}},
  author = {{von Neumann}, John},
  year = {1928},
  month = dec,
  journal = {Mathematische Annalen},
  volume = {100},
  number = {1},
  pages = {295--320},
  issn = {1432-1807},
  doi = {10.1007/BF01448847},
  urldate = {2024-01-13},
  langid = {ngerman}
}

@article{voss2021,
  title = {Visualizing {{Weights}}},
  author = {Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Petrov, Michael and Schubert, Ludwig and Egan, Ben and Lim, Swee and Olah, Chris},
  year = {2021},
  month = feb,
  journal = {Distill},
  volume = {6},
  number = {2},
  pages = {10.23915/distill.00024.007},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.007},
  urldate = {2024-06-16}
}

@article{walck2014,
  title = {Learn {{Physics}} by {{Programming}} in {{Haskell}}},
  author = {Walck, Scott N.},
  year = {2014},
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {170},
  pages = {67--77},
  doi = {10.4204/EPTCS.170.5},
  file = {/Users/nobr/Zotero/storage/P3XU4JZM/Walck - 2014 - Learn Physics by Programming in Haskell.pdf}
}

@book{wallace-wells2019uninhabitable,
  title = {The Uninhabitable Earth: {{Life}} after Warming},
  author = {{Wallace-Wells}, David},
  year = {2019},
  pages = {320},
  publisher = {Tim Duggan Books},
  address = {New York},
  isbn = {978-0-525-57670-9}
}

@article{wallace2004,
  title = {A Revised View of Sensory Cortical Parcellation},
  author = {Wallace, Mark T. and Ramachandran, Ramnarayan and Stein, Barry E.},
  year = {2004},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {101},
  number = {7},
  pages = {2167--2172},
  issn = {0027-8424},
  doi = {10.1073/pnas.0305697101},
  urldate = {2023-05-13},
  abstract = {Traditional cortical parcellation schemes have emphasized the presence of sharply defined visual, auditory, and somatosensory domains populated exclusively by modality-specific neurons (i.e., neurons responsive to sensory stimuli from a single sensory modality). However, the modality-exclusivity of this scheme has recently been challenged. Observations in a variety of species suggest that each of these domains is subject to influences from other senses. Using the cerebral cortex of the rat as a model, the present study systematically examined the capability of individual neurons in visual, auditory, and somatosensory cortex to be activated by stimuli from other senses. Within the major modality-specific domains, the incidence of inappropriate (i.e., nonmatching) and/or multisensory neurons was very low. However, at the borders between each of these domains a concentration of multisensory neurons was found whose modality profile matched the representations in neighboring cortices and that were able to integrate their cross-modal inputs to give rise to enhanced and/or depressed responses. The results of these studies are consistent with some features of both the traditional and challenging views of cortical organization, and they suggest a parcellation scheme in which modality-specific cortical domains are separated from one another by transitional multisensory zones.},
  pmcid = {PMC357070},
  pmid = {14766982},
  file = {/Users/nobr/Zotero/storage/ZSBQVG9X/Wallace et al. - 2004 - A revised view of sensory cortical parcellation.pdf}
}

@article{wang2019,
  title = {Dilated {{3D Convolutional Neural Networks}} for {{Brain MRI Data Classification}}},
  author = {Wang, Zijian and Sun, Yaoru and Shen, Qianzi and Cao, Lei},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {134388--134398},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2941912},
  abstract = {Benefiting from the research of machine learning (ML) and deep learning(DL), multivariate methods based on ML and DL have been the mainstream and successful analysis methods in Neural Engineering or Neuroimaging research, for example, assisting diagnosis based on brain Magnetic Resonance Imaging (MRI). However, many existing methods based on traditional ML methods cannot sufficiently extract discriminative features, especially feature patterns across long-distance brain areas, resulting in unsatisfactory classification performance. Designing an effective and robust classifier for different MRI images remains a challenge. In this paper, we introduced dilated 3D CNN method for classifying 3D MRI images combining CNN structure and dilated convolution with a small number of feature maps. We also presented a methodology framework based on dilated 3D CNN method, which can classify both single MRI images and image sequences. Our method and framework were evaluated on the structural MRI images of ADHD-200 dataset and fMRI images of a Schizophrenia dataset, demonstrating better performances than some other state-of-the-art methods.},
  keywords = {Biomedical image processing,Convolution,Feature extraction,Functional magnetic resonance imaging,machine learning,Machine learning algorithms,magnetic resonance imaging,Neuroimaging,Three-dimensional displays},
  file = {/Users/nobr/Zotero/storage/ICWHJKQ9/Wang et al. - 2019 - Dilated 3D Convolutional Neural Networks for Brain.pdf;/Users/nobr/Zotero/storage/9KSLIINN/08840843.html}
}

@article{wang2022,
  title = {A Synchronized Multimodal Neuroimaging Dataset for Studying Brain Language Processing},
  author = {Wang, Shaonan and Zhang, Xiaohan and Zhang, Jiajun and Zong, Chengqing},
  year = {2022},
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  doi = {10.1038/s41597-022-01708-5},
  file = {/Users/nobr/Zotero/storage/CLDIKN6C/Wang et al. - 2022 - A synchronized multimodal neuroimaging dataset for studying brain language processing.pdf}
}

@misc{wang2022a,
  title = {Interpretability in the {{Wild}}: A {{Circuit}} for {{Indirect Object Identification}} in {{GPT-2}} Small},
  shorttitle = {Interpretability in the {{Wild}}},
  author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  year = {2022},
  month = nov,
  number = {arXiv:2211.00593},
  eprint = {2211.00593},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.00593},
  urldate = {2023-06-19},
  abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/AJM3ER6W/Wang et al. - 2022 - Interpretability in the Wild a Circuit for Indirect Object Identification in GPT-2 small.pdf}
}

@article{wang2022b,
  title = {Adversarial {{Policies Beat Superhuman Go AIs}}},
  author = {Wang, Tony Tong and Gleave, Adam and Belrose, Nora and Tseng, Tom and Miller, Joseph and Pelrine, Kellin and Dennis, Michael D and Duan, Yawen and Pogrebniak, Viktor and Levine, Sergey and Russell, Stuart},
  year = {2022},
  month = nov,
  eprint = {2211.00241},
  urldate = {2023-03-22},
  abstract = {We attack the state-of-the-art Go-playing AI system KataGo by training adversarial policies that play against frozen KataGo victims. Our attack achieves a {$>$}99\% win rate when KataGo uses no tree search, and a {$>$}97\% win rate when KataGo uses enough search to be superhuman. We train our adversaries with a modified KataGo implementation, using less than 14\% of the compute used to train the original KataGo. Notably, our adversaries do not win by learning to play Go better than KataGo -- in fact, our adversaries are easily beaten by human amateurs. Instead, our adversaries win by tricking KataGo into making serious blunders. Our attack transfers zero-shot to other superhuman Go-playing AIs, and is interpretable to the extent that human experts can successfully implement it, without algorithmic assistance, to consistently beat superhuman AIs. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes. Example games are available at https://goattack.far.ai/.},
  archiveprefix = {arXiv},
  file = {/Users/nobr/Zotero/storage/QAFD4REC/full-text.pdf}
}

@inproceedings{wang2022c,
  title = {Over-the-{{Horizon Air Combat Environment Modeling}} and {{Deep Reinforcement Learning Application}}},
  booktitle = {2022 4th {{International Conference}} on {{Data-driven Optimization}} of {{Complex Systems}} ({{DOCS}})},
  author = {Wang, Ao and Zhao, Shangwei and Shi, Zhengkang and Wang, Jingcheng},
  year = {2022},
  month = oct,
  pages = {1--6},
  publisher = {IEEE},
  address = {Chengdu, China},
  doi = {10.1109/DOCS55193.2022.9967482},
  urldate = {2024-01-14},
  abstract = {As we all know, over-the-horizon air combat has become one of the important fight forms that determine the trend of modern warfare. The biggest challenge in the confrontation process is how to make aircrafts cooperative-decision to lock, launch and avoid operations. To this end, this paper investigates the deep reinforcement learning application on the over-thehorizon air combat environment to enhance the ability of multiaircraft cooperative decision-making and intelligent optimization. First, a novel over-the-horizon air combat environment is constructed as a training environment for deep reinforcement learning, which could provide an easy-to-calculate simulation environment with higher precision. Then, we propose the proximal policy optimization combined with the long short-term memory network to deal with incomplete information and realize intelligent decision optimization at the same time. Finally, the effectiveness of the proposed algorithm is verified by simulation experiments.},
  isbn = {978-1-6654-5982-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/X2RJUUYD/Wang et al. - 2022 - Over-the-Horizon Air Combat Environment Modeling and Deep Reinforcement Learning Application.pdf}
}

@article{wang2022d,
  title = {{{DeepNet}}: {{Scaling Transformers}} to 1,000 {{Layers}}},
  shorttitle = {{{DeepNet}}},
  author = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2203.00555},
  urldate = {2024-05-25},
  abstract = {In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/nobr/Zotero/storage/HQ39PTJS/Wang et al. - 2022 - DeepNet Scaling Transformers to 1,000 Layers.pdf}
}

@article{wang2023,
  title = {Multimodal Single-Cell and Whole-Genome Sequencing of Small, Frozen Clinical Specimens},
  author = {Wang, Yiping and Fan, Joy Linyue and Melms, Johannes C. and Amin, Amit Dipak and Georgis, Yohanna and Barrera, Irving and Ho, Patricia and Tagore, Somnath and {Abril-Rodr{\'i}guez}, Gabriel and He, Siyu and Jin, Yinuo and Biermann, Jana and Hofree, Matan and Caprio, Lindsay and Berhe, Simon and Khan, Shaheer A. and Henick, Brian S. and Ribas, Antoni and Macosko, Evan Z. and Chen, Fei and Taylor, Alison M. and Schwartz, Gary K. and Carvajal, Richard D. and Azizi, Elham and Izar, Benjamin},
  year = {2023},
  month = jan,
  journal = {Nature Genetics 2023 55:1},
  volume = {55},
  number = {1},
  pages = {19--25},
  publisher = {Nature Publishing Group},
  issn = {1546-1718},
  doi = {10.1038/S41588-022-01268-9},
  urldate = {2023-03-15},
  abstract = {Single-cell genomics enables dissection of tumor heterogeneity and molecular underpinnings of drug response at an unprecedented resolution1--11. However, broad clinical application of these methods remains challenging, due to several practical and preanalytical challenges that are incompatible with typical clinical care workflows, namely the need for relatively large, fresh tissue inputs. In the present study, we show that multimodal, single-nucleus (sn)RNA/T cell receptor (TCR) sequencing, spatial transcriptomics and whole-genome sequencing (WGS) are feasible from small, frozen tissues that approximate routinely collected clinical specimens (for example, core needle biopsies). Compared with data from sample-matched fresh tissue, we find a similar quality in the biological outputs of snRNA/TCR-seq data, while reducing artifactual signals and compositional biases introduced by fresh tissue processing. Profiling sequentially collected melanoma samples from a patient treated in the KEYNOTE-001 trial12, we resolved cellular, genomic, spatial and clonotype dynamics that represent molecular patterns of heterogeneous intralesional evolution during anti-programmed cell death protein 1 therapy. To demonstrate applicability to banked biospecimens of rare diseases13, we generated a single-cell atlas of uveal melanoma liver metastasis with matched WGS data. These results show that single-cell genomics from archival, clinical specimens is feasible and provides a framework for translating these methods more broadly to the clinical arena. High-quality multimodal single-cell, bulk and spatial genomics data are prepared from low-input, frozen needle biopsy specimens collected during routine clinical procedures.},
  pmid = {36624340},
  keywords = {Immunosurveillance,Transcriptomics,Translational research},
  file = {/Users/nobr/Zotero/storage/RHDWD3WJ/full-text.pdf}
}

@article{wang2023a,
  title = {Combining Convolutional Neural Networks and Self-Attention for Fundus Diseases Identification},
  author = {Wang, Keya and Xu, Chuanyun and Li, Gang and Zhang, Yang and Zheng, Yu and Sun, Chengjie},
  year = {2023},
  month = jan,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {76},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-27358-6},
  urldate = {2023-11-16},
  abstract = {Early detection of lesions is of great significance for treating fundus diseases. Fundus photography is an effective and convenient screening technique by which common fundus diseases can be detected. In this study, we use color fundus images to distinguish among multiple fundus diseases. Existing research on fundus disease classification has achieved some success through deep learning techniques, but there is still much room for improvement in model evaluation metrics using only deep convolutional neural network (CNN) architectures with limited global modeling ability; the simultaneous diagnosis of multiple fundus diseases still faces great challenges. Therefore, given that the self-attention (SA) model with a global receptive field may have robust global-level feature modeling ability, we propose a multistage fundus image classification model MBSaNet which combines CNN and SA mechanism. The convolution block extracts the local information of the fundus image, and the SA module further captures the complex relationships between different spatial positions, thereby directly detecting one or more fundus diseases in retinal fundus image. In the initial stage of feature extraction, we propose a multiscale feature fusion stem, which uses convolutional kernels of different scales to extract low-level features of the input image and fuse them to improve recognition accuracy. The training and testing were performed based on the ODIR-5k dataset. The experimental results show that MBSaNet achieves state-of-the-art performance with fewer parameters. The wide range of diseases and different fundus image collection conditions confirmed the applicability of MBSaNet.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Computational neuroscience,Image processing,Machine learning},
  file = {/Users/nobr/Zotero/storage/N3LW6YKP/Wang et al. - 2023 - Combining convolutional neural networks and self-attention for fundus diseases identification.pdf}
}

@article{wang2023b,
  title = {A Multi-Agent Reinforcement Learning Algorithm with the Action Preference Selection Strategy for Massive Target Cooperative Search Mission Planning},
  author = {Wang, Xiaoyan and Fang, Xi},
  year = {2023},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {231},
  pages = {120643},
  issn = {09574174},
  doi = {10.1016/j.eswa.2023.120643},
  urldate = {2023-11-12},
  abstract = {Target search is widely applied in military reconnaissance, geological exploration and personnel search and rescue. Most target search algorithms perform well in single target search but are inefficient or even ineffective in multi-target search. To solve the multi-target search problem in an uncertain environment, this paper constructs a multi-agent massive target cooperative search mission planning model and proposes an improved reinforce\- ment learning algorithm using the action preference selection strategy. Based on the Reinforce algorithm, this algorithm solves the problem of invalid searches within the stochastic strategy by changing the preferred action selection method. The proposed method improves the efficiency of multiple agents in the search for targets without collision using a cooperative mechanism and reward rules based on the odor effect. Simulation exper\- iments are conducted in three aspects to verify the effectiveness and robustness of the improved algorithm and compare it with other reinforcement learning algorithms in the field of multi-agent learning. The results demonstrate that the improved algorithm has obvious advantages in terms of mission success rate, target search rate and average search time, and the movement trajectory of multiple agents is more concise.},
  langid = {english}
}

@article{wang2024b,
  title = {Evaluating {{Causal Reasoning Capabilities}} of {{Large Language Models}}: {{A Systematic Analysis Across Three Scenarios}}},
  shorttitle = {Evaluating {{Causal Reasoning Capabilities}} of {{Large Language Models}}},
  author = {Wang, Lei 1 and Shen, Yiqing 2 1 School of Software Engineering and Guangzhou Intelligence Communications Technology Co., Ltd},
  year = {2024},
  pages = {4584},
  publisher = {MDPI AG},
  doi = {10.3390/electronics13234584},
  urldate = {2025-03-03},
  abstract = {Large language models (LLMs) have shown their capabilities in numerical and logical reasoning, yet their capabilities in higher-order cognitive tasks, particularly causal reasoning, remain less explored. Current research on LLMs in causal reasoning has focused primarily on tasks such as identifying simple cause-effect relationships, answering basic ``what-if'' questions, and generating plausible causal explanations. However, these models often struggle with complex causal structures, confounding variables, and distinguishing correlation from causation. This work addresses these limitations by systematically evaluating LLMs' causal reasoning abilities across three representative scenarios, namely analyzing causation from effects, tracing effects back to causes, and assessing the impact of interventions on causal relationships. These scenarios are designed to challenge LLMs beyond simple associative reasoning and test their ability to handle more nuanced causal problems. For each scenario, we construct four paradigms and employ three types of prompt scheme, namely zero-shot prompting, few-shot prompting, and Chain-of-Thought (CoT) prompting in a set of 36 test cases. Our findings reveal that most LLMs encounter challenges in causal cognition across all prompt schemes, which underscore the need to enhance the cognitive reasoning capabilities of LLMs to better support complex causal reasoning tasks. By identifying these limitations, our study contributes to guiding future research and development efforts in improving LLMs' higher-order reasoning abilities.},
  copyright = {{\copyright} 2024 by the authors.  Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/8W22UYPG/Wang et al. - 2024 - Evaluating Causal Reasoning Capabilities of Large Language Models A Systematic Analysis Across Thre.pdf}
}

@article{wang2024c,
  title = {Exploring {{Equation}} as a {{Better Intermediate Meaning Representation}} for {{Numerical Reasoning}} of {{Large Language Models}}},
  author = {Wang, Dingzirui and Dou, Longxu and Zhang, Wenbin and Zeng, Junyu and Che, Wanxiang},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {17},
  pages = {19116--19125},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i17.29879},
  urldate = {2025-03-03},
  abstract = {Numerical reasoning is a vital capability for natural language processing models to understand and process numerical information in real-world scenarios. Most current methods first generate the Intermediate Meaning Representations (IMRs) of questions and then generate answers. Current SOTA methods generate programs as IMRs with large language models (LLMs). Intuitively, equations have fewer restrictions and closer semantics to the question than programs, leading to higher generation accuracy. However, current LLMs generate equations worse than programs, where we assume that the equation data is rare in pre-training data compared to programs. So in this paper, we try to use equations as IMRs to solve the numerical reasoning task by addressing two problems: (1) Theoretically, how to prove that the equation is an IMR with higher generation accuracy than programs; (2) Empirically, how to improve the generation accuracy of equations with LLMs. For the first problem, we propose and prove a proposition to theoretically compare the generation accuracy of different IMRs. For the second problem, we present a method called Boosting Numerical Reasoning by Decomposing the Generation of Equations (BRIDGE), which can improve the accuracy of LLMs in generating equations as IMRs by reducing the tendency of generating constant expressions and programs. Our method improves the performance by 2.2\%, 0.9\%, and 1.7\% on GSM8K, SVAMP, and Algebra datasets compared to the previous state-of-theart methods under the single reasoning path setting. Our code and prompts are available at https://github.com/ziruiHIT/Bridge for Numerical Reasoning.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/D77U8S48/Wang et al. - 2024 - Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning of Large.pdf}
}

@article{wang2024d,
  title = {{{CreDes}}: {{Causal Reasoning Enhancement}} and {{Dual-End Searching}} for {{Solving Long-Range Reasoning Problems}} Using {{LLMs}}},
  shorttitle = {{{CreDes}}},
  author = {Wang, Kangsheng and Zhang, Xiao and Liu, Hao and Han, Songde and Ma, Huimin and Hu, Tianyu},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2410.01696},
  urldate = {2024-12-03},
  abstract = {Large language models (LLMs) have demonstrated limitations in handling combinatorial optimization problems involving long-range reasoning, partially due to causal hallucinations and huge search space. As for causal hallucinations, i.e., the inconsistency between reasoning and corresponding state transition, this paper introduces the Causal Relationship Enhancement (CRE) mechanism combining cause-effect interventions and the Individual Treatment Effect (ITE) to guarantee the solid causal rightness between each step of reasoning and state transition. As for the long causal range and huge search space limiting the performances of existing models featuring single-direction search, a Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree. By integrating CRE and DES (CreDes), our model has realized simultaneous multi-step reasoning, circumventing the inefficiencies from cascading multiple one-step reasoning like the Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning tasks in terms of both accuracy and time efficiency.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/nobr/Zotero/storage/FF2ZN4HG/Wang et al. - 2024 - CreDes Causal Reasoning Enhancement and Dual-End Searching for Solving Long-Range Reasoning Problem.pdf}
}

@misc{wang2024e,
  title = {1-Bit {{AI Infra}}: {{Part}} 1.1, {{Fast}} and {{Lossless BitNet}} B1.58 {{Inference}} on {{CPUs}}},
  shorttitle = {1-Bit {{AI Infra}}},
  author = {Wang, Jinheng and Zhou, Hansong and Song, Ting and Mao, Shaoguang and Ma, Shuming and Wang, Hongyu and Xia, Yan and Wei, Furu},
  year = {2024},
  month = oct,
  number = {arXiv:2410.16144},
  eprint = {2410.16144},
  doi = {10.48550/arXiv.2410.16144},
  urldate = {2024-11-20},
  abstract = {Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and BitNet b1.58, present a promising approach to enhancing the efficiency of LLMs in terms of speed and energy consumption. These developments also enable local LLM deployment across a broad range of devices. In this work, we introduce bitnet.cpp, a tailored software stack designed to unlock the full potential of 1-bit LLMs. Specifically, we develop a set of kernels to support fast and lossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments demonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x to 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model sizes. The code is available at https://github.com/microsoft/BitNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/ZD3PPQSC/Wang et al. - 2024 - 1-bit AI Infra Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs.pdf}
}

@misc{wangChainofThoughtReasoningPrompting2024,
  title = {Chain-of-{{Thought Reasoning Without Prompting}}},
  author = {Wang, Xuezhi and Zhou, Denny},
  year = {2024},
  month = feb,
  number = {arXiv:2402.10200},
  eprint = {2402.10200},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-20},
  abstract = {In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the {\textbackslash}textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' {\textbackslash}textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/Y5Q6DG5L/Wang and Zhou - 2024 - Chain-of-Thought Reasoning Without Prompting.pdf}
}

@misc{wangDescribeExplainPlan2023,
  title = {Describe, {{Explain}}, {{Plan}} and {{Select}}: {{Interactive Planning}} with {{Large Language Models Enables Open-World Multi-Task Agents}}},
  shorttitle = {Describe, {{Explain}}, {{Plan}} and {{Select}}},
  author = {Wang, Zihao and Cai, Shaofei and Chen, Guanzhou and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  year = {2023},
  month = oct,
  number = {arXiv:2302.01560},
  eprint = {2302.01560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.01560},
  urldate = {2024-02-16},
  abstract = {We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose "\${\textbackslash}underline\{D\}\$escribe, \${\textbackslash}underline\{E\}\$xplain, \${\textbackslash}underline\{P\}\$lan and \${\textbackslash}underline\{S\}\$elect" (\${\textbackslash}textbf\{DEPS\}\$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated \${\textbackslash}textit\{plan\}\$ by integrating \${\textbackslash}textit\{description\}\$ of the plan execution process and providing self-\${\textbackslash}textit\{explanation\}\$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal \${\textbackslash}textit\{selector\}\$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the \${\textbackslash}texttt\{ObtainDiamond\}\$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/6C8VXD7U/Wang et al. - 2023 - Describe, Explain, Plan and Select Interactive Planning with Large Language Models Enables Open-Wor.pdf;/Users/nobr/Zotero/storage/AEJ6X49H/2302.html}
}

@misc{wangGrokkedTransformersAre2024,
  title = {Grokked {{Transformers}} Are {{Implicit Reasoners}}: {{A Mechanistic Journey}} to the {{Edge}} of {{Generalization}}},
  shorttitle = {Grokked {{Transformers}} Are {{Implicit Reasoners}}},
  author = {Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
  year = {2024},
  month = may,
  number = {arXiv:2405.15071},
  eprint = {2405.15071},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.15071},
  urldate = {2024-05-28},
  abstract = {We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/GDAV5WVD/Wang et al. - 2024 - Grokked Transformers are Implicit Reasoners A Mechanistic Journey to the Edge of Generalization.pdf}
}

@misc{wangSHAQIncorporatingShapley2023,
  title = {{{SHAQ}}: {{Incorporating Shapley Value Theory}} into {{Multi-Agent Q-Learning}}},
  shorttitle = {{{SHAQ}}},
  author = {Wang, Jianhong and Zhang, Yuan and Gu, Yunjie and Kim, Tae-Kyun},
  year = {2023},
  month = jan,
  number = {arXiv:2105.15013},
  eprint = {2105.15013},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.15013},
  urldate = {2024-05-30},
  abstract = {Value factorisation is a useful technique for multi-agent reinforcement learning (MARL) in global reward game, however its underlying mechanism is not yet fully understood. This paper studies a theoretical framework for value factorisation with interpretability via Shapley value theory. We generalise Shapley value to Markov convex game called Markov Shapley value (MSV) and apply it as a value factorisation method in global reward game, which is obtained by the equivalence between the two games. Based on the properties of MSV, we derive Shapley-Bellman optimality equation (SBOE) to evaluate the optimal MSV, which corresponds to an optimal joint deterministic policy. Furthermore, we propose Shapley-Bellman operator (SBO) that is proved to solve SBOE. With a stochastic approximation and some transformations, a new MARL algorithm called Shapley Q-learning (SHAQ) is established, the implementation of which is guided by the theoretical results of SBO and MSV. We also discuss the relationship between SHAQ and relevant value factorisation methods. In the experiments, SHAQ exhibits not only superior performances on all tasks but also the interpretability that agrees with the theoretical analysis. The implementation of this paper is on https://github.com/hsvgbkhgbv/shapley-q-learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/W7A5ZUM4/Wang et al. - 2023 - SHAQ Incorporating Shapley Value Theory into Multi-Agent Q-Learning.pdf;/Users/nobr/Zotero/storage/98MRK6M7/2105.html}
}

@misc{wangVoyagerOpenEndedEmbodied2023,
  title = {Voyager: {{An Open-Ended Embodied Agent}} with {{Large Language Models}}},
  shorttitle = {Voyager},
  author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  year = {2023},
  month = oct,
  number = {arXiv:2305.16291},
  eprint = {2305.16291},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16291},
  urldate = {2024-04-09},
  abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/A3DGDGLF/Wang et al. - 2023 - Voyager An Open-Ended Embodied Agent with Large Language Models.pdf;/Users/nobr/Zotero/storage/8VHW2NFW/2305.html}
}

@inproceedings{ward2000,
  title = {Dasher---a Data Entry Interface Using Continuous Gestures and Language Models},
  booktitle = {Proceedings of the 13th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology  - {{UIST}} '00},
  author = {Ward, David J. and Blackwell, Alan F. and MacKay, David J. C.},
  year = {2000},
  pages = {129--137},
  publisher = {ACM Press},
  address = {San Diego, California, United States},
  doi = {10.1145/354401.354427},
  urldate = {2024-02-18},
  isbn = {978-1-58113-212-0},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/E3ESUBYM/Ward et al. - 2000 - Dasher---a data entry interface using continuous gestures and language models.pdf}
}

@misc{wasi2024,
  title = {{{SupplyGraph}}: {{A Benchmark Dataset}} for {{Supply Chain Planning}} Using {{Graph Neural Networks}}},
  shorttitle = {{{SupplyGraph}}},
  author = {Wasi, Azmine Toushik and Islam, MD Shafikul and Akib, Adipto Raihan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.15299},
  eprint = {2401.15299},
  doi = {10.48550/arXiv.2401.15299},
  urldate = {2024-11-13},
  abstract = {Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of factory issues. By utilizing this dataset, researchers can employ GNNs to address numerous supply chain problems, thereby advancing the field of supply chain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,Statistics - Applications},
  file = {/Users/nobr/Zotero/storage/86HZPP4B/Wasi et al. - 2024 - SupplyGraph A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks.pdf}
}

@article{watson2015,
  title = {Regional {{Scale}} Wind Farm and Solar Farm Suitability Assessment Using {{GIS-assisted}} Multi-Criteria Evaluation},
  author = {Watson, Joss J.W. and Hudson, Malcolm D.},
  year = {2015},
  month = jun,
  journal = {Landscape and Urban Planning},
  volume = {138},
  pages = {20--31},
  issn = {01692046},
  doi = {10.1016/j.landurbplan.2015.02.001},
  urldate = {2023-07-13},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/4EAVDUS4/1-s2.0-S0360544219313842-main.pdf;/Users/nobr/Zotero/storage/6RC7X79X/Watson and Hudson - 2015 - Regional Scale wind farm and solar farm suitabilit.pdf;/Users/nobr/Zotero/storage/MSHLTYK4/1-s2.0-S1364032103000686-main.pdf;/Users/nobr/Zotero/storage/SFVYFB9F/zkz001.pdf;/Users/nobr/Zotero/storage/WBWTYTDU/1-s2.0-S1364032113000105-main.pdf;/Users/nobr/Zotero/storage/YXZPGA6E/1-s2.0-S0301421519307517-main.pdf}
}

@article{watson2022,
  title = {Broadly Applicable and Accurate Protein Design by Integrating Structure Prediction Networks and Diffusion Generative Models},
  author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
  year = {2022},
  doi = {10.1101/2022.12.09.519842},
  file = {/Users/nobr/Zotero/storage/NFTMC6EZ/document.pdf}
}

@misc{wbcsd,
  title = {World Business Council for Sustainable Development - Program Areas: {{Climate}} \& Energy}
}

@misc{weather_channel2019,
  title = {2018 Global Disasters Cost \$160 Billion},
  author = {{The Weather Channel}},
  year = {2019},
  month = jan
}

@misc{weber2018,
  title = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author = {Weber, Th{\'e}ophane and Racani{\`e}re, S{\'e}bastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdom{\`e}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  year = {2018},
  month = feb,
  number = {arXiv:1707.06203},
  eprint = {1707.06203},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06203},
  urldate = {2024-09-22},
  abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/IXCZ8LFV/Weber et al. - 2018 - Imagination-Augmented Agents for Deep Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/V2L6HU6Y/1707.html}
}

@article{weed2004,
  title = {A {{Labyrinth}} of {{Symbols}}: {{Exploring}} '{{The Garden}} of {{Forking Paths}}'},
  shorttitle = {A {{Labyrinth}} of {{Symbols}}},
  author = {Weed, Ethan},
  year = {2004},
  journal = {Variaciones Borges},
  number = {18},
  eprint = {24880439},
  eprinttype = {jstor},
  pages = {161--189},
  publisher = {Borges Center, University of Pittsburgh},
  issn = {1396-0482},
  urldate = {2025-05-07},
  file = {/Users/nobr/Zotero/storage/4VW2SADE/Weed - 2004 - A Labyrinth of Symbols Exploring 'The Garden of Forking Paths'.pdf}
}

@article{wei2021,
  title = {Finetuned Language Models Are Zero-Shot Learners},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  year = {2021},
  journal = {arXiv preprint arXiv:2109.01652},
  eprint = {2109.01652},
  archiveprefix = {arXiv}
}

@article{wein2021,
  title = {A Graph Neural Network Framework for Causal Inference in Brain Networks},
  author = {Wein, S. and Malloni, W. M. and Tom{\'e}, A. M. and Frank, S. M. and Henze, G.-I. and W{\"u}st, S. and Greenlee, M. W. and Lang, E. W.},
  year = {2021},
  month = apr,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {8061},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-87411-8},
  urldate = {2023-05-08},
  abstract = {A central question in neuroscience is how self-organizing dynamic interactions in the brain emerge on their relatively static structural backbone. Due to the complexity of spatial and temporal dependencies between different brain areas, fully comprehending the interplay between structure and function is still challenging and an area of intense research. In this paper we present a graph neural network (GNN) framework, to describe functional interactions based on the structural anatomical layout. A GNN allows us to process graph-structured spatio-temporal signals, providing a possibility to combine structural information derived from diffusion tensor imaging (DTI) with temporal neural activity profiles, like that observed in functional magnetic resonance imaging (fMRI). Moreover, dynamic interactions between different brain regions discovered by this data-driven approach can provide a multi-modal measure of causal connectivity strength. We assess the proposed model's accuracy by evaluating its capabilities to replicate empirically observed neural activation profiles, and compare the performance to those of a vector auto regression (VAR), like that typically used in Granger causality. We show that GNNs are able to capture long-term dependencies in data and also computationally scale up to the analysis of large-scale networks. Finally we confirm that features learned by a GNN can generalize across MRI scanner types and acquisition protocols, by demonstrating that the performance on small datasets can be improved by pre-training the GNN on data from an earlier study. We conclude that the proposed multi-modal GNN framework can provide a novel perspective on the structure-function relationship in the brain. Accordingly this approach appears to be promising for the characterization of the information flow in brain networks.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational science,Dynamical systems,Network models},
  file = {/Users/nobr/Zotero/storage/K7WLEYG4/Wein et al. - 2021 - A graph neural network framework for causal infere.pdf}
}

@misc{weissThinkingTransformers2021,
  title = {Thinking {{Like Transformers}}},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  year = {2021},
  month = jul,
  number = {arXiv:2106.06981},
  eprint = {2106.06981},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.06981},
  urldate = {2023-11-23},
  abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/6NJBQF5C/Weiss et al. - 2021 - Thinking Like Transformers.pdf;/Users/nobr/Zotero/storage/IWUGTAAS/2106.html}
}

@book{widrow1985,
  title = {Adaptive Signal Processing},
  author = {Widrow, Bernard and Stearns, Samuel D.},
  year = {1985},
  series = {Prentice-{{Hall}} Signal Processing Series},
  edition = {3. [print.]},
  publisher = {Prentice-Hall},
  address = {Englewood Cliffs, NJ},
  isbn = {978-0-13-004029-9},
  file = {/Users/nobr/Zotero/storage/ZGP9JFSQ/Widrow and Stearns - 1985 - Adaptive signal processing.pdf}
}

@article{wilshaw2024,
  title = {New {{Foundations}} Is Consistent: An Exposition and Formal Verification},
  author = {Wilshaw, Sky},
  year = {2024},
  abstract = {We give a self-contained account of a version of Holmes' proof [6] that Quine's set theory New Foundations [8] is consistent relative to the metatheory ZFC. We have formalised this proof in the Lean interactive theorem prover [11], and this paper is a `deformalisation' of that work. We discuss the challenges of formalising new and untested mathematics in an interactive theorem prover, and how the process of completing the formalisation has influenced our presentation of the proof.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GZQBTR2U/Wilshaw - New Foundations is consistent an exposition and formal verification.pdf}
}

@article{wilson2020,
  title = {Statement of {{Retraction}}: {{Human}} Reactions to Rape Culture and Queer Performativity at Urban Dog Parks in {{Portland}}, {{Oregon}}},
  shorttitle = {Statement of {{Retraction}}},
  author = {Wilson, Helen},
  year = {2020},
  month = feb,
  journal = {Gender, Place \& Culture},
  volume = {27},
  number = {2},
  pages = {(307)-(326)},
  issn = {0966-369X, 1360-0524},
  doi = {10.1080/0966369X.2018.1475346},
  urldate = {2024-01-21},
  abstract = {This article addresses questions in human geography and the geographies of sexuality by drawing upon one year of embedded in situ observations of dogs and their human companions at three public dog parks in Portland, Oregon. The purpose of this research is to uncover emerging themes in human and canine interactive behavioral patterns in urban dog parks to better understand human a-/moral decisionmaking in public spaces and uncover bias and emergent assumptions around gender, race, and sexuality. Specifically, and in order of priority, I examine the following questions: (1) How do human companions manage, contribute, and respond to violence in dogs? (2) What issues surround queer performativity and human reaction to homosexual sex between and among dogs? and (3) Do dogs suffer oppression based upon (perceived) gender? It concludes by applying Black feminist criminology categories through which my observations can be understood and by inferring from lessons relevant to human and dog interactions to suggest practical applications that disrupts hegemonic masculinities and improves access to emancipatory spaces.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/GKY3V2C4/2020 - Statement of Retraction Human reactions to rape culture and queer performativity at urban dog parks.pdf}
}

@misc{wiltzer2024,
  title = {Foundations of {{Multivariate Distributional Reinforcement Learning}}},
  author = {Wiltzer, Harley and Farebrother, Jesse and Gretton, Arthur and Rowland, Mark},
  year = {2024},
  month = aug,
  number = {arXiv:2409.00328},
  eprint = {2409.00328},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.00328},
  urldate = {2025-08-02},
  abstract = {In reinforcement learning (RL), the consideration of multivariate reward signals has led to fundamental advancements in multi-objective decision-making, transfer learning, and representation learning. This work introduces the first oracle-free and computationallytractable algorithms for provably convergent multivariate distributional dynamic programming and temporal difference learning. Our convergence rates match the familiar rates in the scalar reward setting, and additionally provide new insights into the fidelity of approximate return distribution representations as a function of the reward dimension. Surprisingly, when the reward dimension is larger than 1, we show that standard analysis of categorical TD learning fails, which we resolve with a novel projection onto the space of mass-1 signed measures. Finally, with the aid of our technical results and simulations, we identify tradeoffs between distribution representations that influence the performance of multivariate distributional RL in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/2BS62657/Wiltzer et al. - 2024 - Foundations of Multivariate Distributional Reinforcement Learning.pdf}
}

@article{winkler2012,
  title = {Residential Segregation by Age in the {{United States}}},
  author = {Winkler, Richelle and Klaas, Rozalynn},
  year = {2012},
  month = dec,
  journal = {Journal of Maps},
  volume = {8},
  number = {4},
  pages = {374--378},
  issn = {1744-5647},
  doi = {10.1080/17445647.2012.739099},
  urldate = {2023-07-04},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XID5C3RG/Winkler and Klaas - 2012 - Residential segregation by age in the United State.pdf}
}

@book{wolsey2021,
  title = {Integer Programming},
  author = {Wolsey, Laurence A.},
  year = {2021},
  edition = {Second edition},
  publisher = {Wiley},
  address = {Hoboken, NJ},
  abstract = {"An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. Contains a new chapter on Benders' algorithm, as there have been many successful applications of Benders' algorithm since the first edition published. Provides improved presentation of Branch-and-Price Algorithms. Contains an introduction to Branch-Cut and Price. Includes new heuristics within mixed integer programming (MIP) codes and user implemented heuristics using a Modelling Language and a MIP solver . Supplementary material consists of solutions to some exercises, available to instructors on a Wiley Instructor Companion Site"--},
  isbn = {978-1-119-60653-6},
  langid = {english},
  lccn = {T57.74 .W67 2021},
  keywords = {Integer programming},
  file = {/Users/nobr/Zotero/storage/HPESRSYC/Wolsey - 2021 - Integer programming.pdf}
}

@techreport{wong2014coastal,
  title = {Coastal Systems and Low-Lying Areas},
  author = {Wong, P. P. and Losada, I. J. and Gattuso, J.-P. and Hinkel, J. and Khattabi, A. and McInnes, K. L. and Saito, Y. and Sallenger, A.},
  year = {2014},
  journal = {Climate change 2014: Impacts, adaptation, and vulnerability. Part a: Global and sectoral aspects. Contribution of working group II to the fifth assessment report of the intergovernmental panel on climate change},
  pages = {361--409},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  institution = {Cambridge University Press}
}

@article{woo2018,
  title = {{{CBAM}}: {{Convolutional Block Attention Module}}},
  shorttitle = {{{CBAM}}},
  author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  year = {2018},
  month = jul,
  eprint = {1807.06521},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1807.06521},
  urldate = {2023-11-16},
  abstract = {We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS{\textasciitilde}COCO detection, and VOC{\textasciitilde}2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/49RFDW3I/Woo et al. - 2018 - CBAM Convolutional Block Attention Module.pdf;/Users/nobr/Zotero/storage/NYZP8J7G/1807.html}
}

@article{wooldridge1995,
  title = {Intelligent Agents: Theory and Practice},
  shorttitle = {Intelligent Agents},
  author = {Wooldridge, Michael and Jennings, Nicholas R.},
  year = {1995},
  month = jun,
  journal = {The Knowledge Engineering Review},
  volume = {10},
  number = {2},
  pages = {115--152},
  issn = {0269-8889, 1469-8005},
  doi = {10.1017/S0269888900008122},
  urldate = {2025-05-08},
  abstract = {Abstract                            The concept of an               agent               has become important in both artificial intelligence (AT) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary).               Agent theory               is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents.               Agent architectures               can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the properties specified by agent theorists. Finally,               agent languages               are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is               not               intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/85IUEBJB/Wooldridge and Jennings - 1995 - Intelligent agents theory and practice.pdf}
}

@inproceedings{woolley2014,
  title = {A Novel Human-Computer Collaboration: {{Combining}} Novelty Search with Interactive Evolution},
  booktitle = {{{GECCO}} 2014 - {{Proceedings}} of the 2014 {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Woolley, Brian G. and Stanley, Kenneth O.},
  year = {2014},
  pages = {233--240},
  publisher = {Association for Computing Machinery},
  doi = {10.1145/2576768.2598353},
  abstract = {Recent work on novelty and behavioral diversity in evolutionary computation has highlighted the potential disadvantage of driving search purely through objective means. This paper suggests that leveraging human insight during search can complement such novelty- driven approaches. In particular, a new approach called noveltyassisted interactive evolutionary computation (NA-IEC) combines human intuition with novelty search to facilitate the serendipitous discovery of agent behaviors in a deceptive maze. In this approach, the human user directs evolution by selecting what is interesting from the on-screen population of behaviors. However, unlike in typical IEC, the user can now request that the next generation be filled with novel descendants. The experimental results demonstrate that combining human insight with novelty search not only finds solutions significantly faster and at lower genomic complexities than fully-automated processes guided purely by fitness or novelty, but it also finds solutions faster than the traditional IEC approach. Such results add to the evidence that combining human users and automated processes creates a synergistic effect in the search for solutions. {\copyright} 2014 is held by the owner/author(s).},
  isbn = {978-1-4503-2662-9},
  keywords = {Deception,Evolutionary computation,Fitness,Human-led search,Interactive evolutionary computation,Non-objective search,Novelty search,Serendipitous discovery,Stepping stones},
  file = {/Users/nobr/Zotero/storage/T3PXM9LF/Woolley and Stanley - 2014 - A novel human-computer collaboration Combining novelty search with interactive evolution.pdf}
}

@techreport{world_bank_carbon,
  title = {State and Trends of Carbon Pricing},
  institution = {World Bank}
}

@book{worldcommissiononenvironmentanddevelopment1987,
  title = {Our Common Future},
  editor = {{World Commission on Environment {and} Development}},
  year = {1987},
  series = {Oxford Paperbacks},
  publisher = {Oxford University Press},
  address = {Oxford ; New York},
  isbn = {978-0-19-282080-8},
  lccn = {HD75.6 .O97 1987},
  keywords = {Effect of human beings on,Environmental policy,Human ecology,Nature,Sustainable development},
  file = {/Users/nobr/Zotero/storage/UWCKTK45/our_common_futurebrundtlandreport1987.pdf}
}

@misc{wri2015indc,
  title = {Why Are {{INDC}} Studies Reaching Different Temperature Estimates?},
  year = {2015},
  month = nov
}

@article{wu2018,
  title = {Accurate Nonlinear Mapping between {{MNI}} Volumetric and {{FreeSurfer}} Surface Coordinate Systems},
  author = {Wu, Jianxiao and Ngo, Gia H. and Greve, Douglas and Li, Jingwei and He, Tong and Fischl, Bruce and Eickhoff, Simon B. and Yeo, B.T. Thomas},
  year = {2018},
  month = may,
  journal = {Human Brain Mapping},
  volume = {39},
  number = {9},
  pages = {3793--3808},
  issn = {1065-9471},
  doi = {10.1002/hbm.24213},
  urldate = {2024-12-19},
  abstract = {The results of most neuroimaging studies are reported in volumetric (e.g., MNI152) or surface (e.g., fsaverage) coordinate systems. Accurate mappings between volumetric and surface coordinate systems can facilitate many applications, such as projecting fMRI group analyses from MNI152/Colin27 to fsaverage for visualization or projecting resting-state fMRI parcellations from fsaverage to MNI152/Colin27 for volumetric analysis of new data. However, there has been surprisingly little research on this topic. Here, we evaluated three approaches for mapping data between MNI152/Colin27 and fsaverage coordinate systems by simulating the above applications: projection of group-average data from MNI152/Colin27 to fsaverage and projection of fsaverage parcellations to MNI152/Colin27. Two of the approaches are currently widely used. A third approach (registration fusion) was previously proposed, but not widely adopted. Two implementations of the registration fusion (RF) approach were considered, with one implementation utilizing the Advanced Normalization Tools (ANTs). We found that RF-ANTs performed the best for mapping between fsaverage and MNI152/Colin27, even for new subjects registered to MNI152/Colin27 using a different software tool (FSL FNIRT). This suggests that RF-ANTs would be useful even for researchers not using ANTs. Finally, it is worth emphasizing that the most optimal approach for mapping data to a coordinate system (e.g., fsaverage) is to register individual subjects directly to the coordinate system, rather than via another coordinate system. Only in scenarios where the optimal approach is not possible (e.g., mapping previously published results from MNI152 to fsaverage), should the approaches evaluated in this manuscript be considered. In these scenarios, we recommend RF-ANTs (https://github.com/ThomasYeoLab/CBIG/tree/master/stable\_projects/registration/Wu2017\_RegistrationFusion).},
  pmcid = {PMC6239990},
  pmid = {29770530},
  file = {/Users/nobr/Zotero/storage/K4N7AM45/Wu et al. - 2018 - Accurate nonlinear mapping between MNI volumetric and FreeSurfer surface coordinate systems.pdf}
}

@misc{wu2022,
  title = {{{FairPrune}}: {{Achieving Fairness Through Pruning}} for {{Dermatological Disease Diagnosis}}},
  shorttitle = {{{FairPrune}}},
  author = {Wu, Yawen and Zeng, Dewen and Xu, Xiaowei and Shi, Yiyu and Hu, Jingtong},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02110},
  eprint = {2203.02110},
  primaryclass = {cs, eess},
  doi = {10.48550/arXiv.2203.02110},
  urldate = {2023-05-16},
  abstract = {Many works have shown that deep learning-based medical image classification models can exhibit bias toward certain demographic attributes like race, gender, and age. Existing bias mitigation methods primarily focus on learning debiased models, which may not necessarily guarantee all sensitive information can be removed and usually comes with considerable accuracy degradation on both privileged and unprivileged groups. To tackle this issue, we propose a method, FairPrune, that achieves fairness by pruning. Conventionally, pruning is used to reduce the model size for efficient inference. However, we show that pruning can also be a powerful tool to achieve fairness. Our observation is that during pruning, each parameter in the model has different importance for different groups' accuracy. By pruning the parameters based on this importance difference, we can reduce the accuracy difference between the privileged group and the unprivileged group to improve fairness without a large accuracy drop. To this end, we use the second derivative of the parameters of a pre-trained model to quantify the importance of each parameter with respect to the model accuracy for each group. Experiments on two skin lesion diagnosis datasets over multiple sensitive attributes demonstrate that our method can greatly improve fairness while keeping the average accuracy of both groups as high as possible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/nobr/Zotero/storage/DBDAE5FP/Wu et al. - 2022 - FairPrune Achieving Fairness Through Pruning for Dermatological Disease Diagnosis.pdf}
}

@incollection{wu2022a,
  title = {A {{Training Model}} of {{Wargaming Based}} on {{Imitation Learning}} and {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of 2022 {{Chinese Intelligent Systems Conference}}},
  author = {Wu, Kangyu and Liu, Mingyu and Cui, Peng and Zhang, Ya},
  editor = {Jia, Yingmin and Zhang, Weicun and Fu, Yongling and Zhao, Shoujun},
  year = {2022},
  volume = {950},
  pages = {786--795},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-19-6203-5_78},
  urldate = {2024-01-14},
  abstract = {This paper proposes an intelligent game confrontation model for a wargame based on imitation learning and deep reinforcement learning, given light of the supremacy of reinforcement learning for training in complex environments. In the context of simulating the battle between the red team and the blue team, a pre-training model based on expert empirical data is produced using imitation learning, and a TD3 algorithm based on the attention mechanism is further designed to build an experience pool using priority experience replay. Finally, the model is enhanced using the self-play approach to increase its training efficiency. Experiments conducted after training demonstrate that the model has a superior training impact, and the winning rate in simulation training is enhanced by 8\% compared to the original model.},
  isbn = {978-981-19-6202-8 978-981-19-6203-5},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/J6ZGDV8M/Wu et al. - 2022 - A Training Model of Wargaming Based on Imitation Learning and Deep Reinforcement Learning.pdf}
}

@misc{wu2023,
  title = {Plan, Eliminate, and Track -- Language Models Are Good Teachers for Embodied Agents},
  author = {Wu, Yue and Min, So Yeon and Bisk, Yonatan and Salakhutdinov, Ruslan and Azaria, Amos and Li, Yuanzhi and Mitchell, Tom and Prabhumoye, Shrimai},
  year = {2023},
  eprint = {2305.02412},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv}
}

@misc{wu2023a,
  title = {Classification of Integers Based on Residue Classes via Modern Deep Learning Algorithms},
  author = {Wu, Da and Yang, Jingye and Ahsan, Mian Umair and Wang, Kai},
  year = {2023},
  month = apr,
  journal = {arXiv.org},
  doi = {10.1016/j.patter.2023.100860},
  urldate = {2024-02-27},
  abstract = {Judging whether an integer can be divided by prime numbers such as 2 or 3 may appear trivial to human beings, but can be less straightforward for computers. Here, we tested multiple deep learning architectures and feature engineering approaches on classifying integers based on their residues when divided by small prime numbers. We found that the ability of classification critically depends on the feature space. We also evaluated Automated Machine Learning (AutoML) platforms from Amazon, Google and Microsoft, and found that they failed on this task without appropriately engineered features. Furthermore, we introduced a method that utilizes linear regression on Fourier series basis vectors, and demonstrated its effectiveness. Finally, we evaluated Large Language Models (LLMs) such as GPT-4, GPT-J, LLaMA and Falcon, and demonstrated their failures. In conclusion, feature engineering remains an important task to improve performance and increase interpretability of machine-learning models, even in the era of AutoML and LLMs.},
  howpublished = {https://arxiv.org/abs/2304.01333v3},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2FQ4IM8L/Wu et al. - 2023 - Classification of integers based on residue classes via modern deep learning algorithms.pdf}
}

@misc{xi2023,
  title = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}: {{A Survey}}},
  shorttitle = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}},
  author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
  year = {2023},
  month = sep,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and humanagent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/6ZQK6LJC/Xi et al. - 2023 - The Rise and Potential of Large Language Model Based Agents A Survey.pdf}
}

@article{xia2020,
  title = {Temozolomide in {{Combination With NF-$\kappa$B Inhibitor Significantly Disrupts}} the {{Glioblastoma Multiforme Spheroid Formation}}},
  author = {Xia, Hui and Avci, Naze G. and Akay, Yasemin and Esquenazi, Yoshua and Schmitt, Lisa H. and Tandon, Nitin and Zhu, Jay-Jiguang and Akay, Metin},
  year = {2020},
  journal = {IEEE Open Journal of Engineering in Medicine and Biology},
  volume = {1},
  pages = {9--16},
  issn = {2644-1276},
  doi = {10.1109/OJEMB.2019.2962801},
  abstract = {Glioblastoma multiforme (GBM) is the most common malignant primary brain tumor, accounting for 50\% of all cases. GBM patients have a five-year survival rate of merely 5.6\% and a median overall survival of 14.6 months with the ``Stupp'' regimen, 20.9 months with tumor treatment fields (TTF, OptuneR) in patients who participated in clinical trials, and 11 months for all GBM patients prior to TTF use. Objective: Our group recently developed a brain cancer chip which generates tumor spheroids, and provides large-scale assessments on the response of tumor cells to various concentrations and combinations of drugs. This platform could optimize the use of tumor samples derived from GBM patients to provide valuable insight on the tumor growth and responses to drug therapies. To minimize any sample loss in vitro, we improved our brain cancer chip system by adding an additional laminar flow distribution layer, which reduces sample loss during cell seeding and prevents spheroids from escaping from the microwells. Methods: In this study, we cultured 3D spheroids from GBM cell lines and patient-derived GBM cells in vitro, and investigated the effect of the combination of Temozolomide and nuclear factor-{$\kappa$}B inhibitor on tumor growth. Results: Our study revealed that these drugs have synergistic effects in inhibiting spheroid formation when used in combination. Conclusions: These results suggest that the brain cancer chip enables large-scale, inexpensive and sample-effective drug screening to 3D cancer tumors in vitro, and could be applied to related tissue engineering drug screening studies.},
  keywords = {3D cell culture,Cancer,Drug-screening,Drugs,glioblastoma,In vitro,Inhibitors,microfluidic,Noise measurement,PEGDA,Three-dimensional displays,Tumors},
  file = {/Users/nobr/Zotero/storage/KGDW9KPR/Xia et al. - 2020 - Temozolomide in Combination With NF-κB Inhibitor S.pdf}
}

@misc{xiao2023,
  title = {Efficient {{Streaming Language Models}} with {{Attention Sinks}}},
  author = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  year = {2023},
  month = sep,
  number = {arXiv:2309.17453},
  eprint = {2309.17453},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.17453},
  urldate = {2023-10-04},
  abstract = {Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/ECMQQEVI/Xiao et al. - 2023 - Efficient Streaming Language Models with Attention Sinks.pdf}
}

@misc{xie2024,
  title = {{{SANA}}: {{Efficient High-Resolution Image Synthesis}} with {{Linear Diffusion Transformers}}},
  shorttitle = {{{SANA}}},
  author = {Xie, Enze and Chen, Junsong and Chen, Junyu and Cai, Han and Tang, Haotian and Lin, Yujun and Zhang, Zhekai and Li, Muyang and Zhu, Ligeng and Lu, Yao and Han, Song},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10629},
  eprint = {2410.10629},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10629},
  urldate = {2024-10-17},
  abstract = {We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096\${\textbackslash}times\$4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8\${\textbackslash}times\$, we trained an AE that can compress images 32\${\textbackslash}times\$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024\${\textbackslash}times\$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/XRAYPG72/Xie et al. - 2024 - SANA Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers.pdf;/Users/nobr/Zotero/storage/CYERZQYU/2410.html}
}

@misc{xin2024,
  title = {{{DeepSeek-Prover}}: {{Advancing Theorem Proving}} in {{LLMs}} through {{Large-Scale Synthetic Data}}},
  shorttitle = {{{DeepSeek-Prover}}},
  author = {Xin, Huajian and Guo, Daya and Shao, Zhihong and Ren, Zhizhou and Zhu, Qihao and Liu, Bo and Ruan, Chong and Li, Wenda and Liang, Xiaodan},
  year = {2024},
  month = may,
  number = {arXiv:2405.14333},
  eprint = {2405.14333},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-17},
  abstract = {Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data. To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems. This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After finetuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3\% with 64 samples and 52\% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0\% with 64 samples and a tree search reinforcement learning method at 41.0\%. Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any. These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs. Both the synthetic dataset and the model will be made available to facilitate further research in this promising field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/BSDAWTGB/2405.14333}
}

@inproceedings{xing2024,
  title = {Understanding the {{Weakness}} of {{Large Language Model Agents}} within a {{Complex Android Environment}}},
  booktitle = {Proceedings of the 30th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Xing, Mingzhe and Zhang, Rongkai and Xue, Hui and Chen, Qi and Yang, Fan and Xiao, Zhen},
  year = {2024},
  month = aug,
  pages = {6061--6072},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3637528.3671650},
  urldate = {2024-12-10},
  abstract = {Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27\% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, prompt, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.},
  isbn = {979-8-4007-0490-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/AD9KFQK5/Xing et al. - 2024 - Understanding the Weakness of Large Language Model Agents within a Complex Android Environment.pdf}
}

@article{xiong2023,
  title = {Can {{LLMs Express Their Uncertainty}}? {{An Empirical Evaluation}} of {{Confidence Elicitation}} in {{LLMs}}},
  shorttitle = {Can {{LLMs Express Their Uncertainty}}?},
  author = {Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.13063},
  urldate = {2024-02-16},
  abstract = {The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs). This leads to a growing need to explore the untapped area of {\textbackslash}emph\{non-logit-based\} approaches to estimate the uncertainty of LLMs. Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs. Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when verbalizing their confidence; 2) Prompting strategies such as CoT, Top-K and Multi-step confidences improve calibration of verbalized confidence; 3) Consistency-based methods outperform the verbalized confidences in most cases, with particularly notable improvements on the arithmetic reasoning task; 4) Hybrid methods consistently deliver the best performance over their baselines, thereby emerging as a promising state-of-the-art approach; 5) Despite these advancements, all investigated methods continue to struggle with challenging tasks, such as those requiring professional knowledge, leaving significant scope for improvement of confidence elicitation.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/nobr/Zotero/storage/MWC3P3KJ/Xiong et al. - 2023 - Can LLMs Express Their Uncertainty An Empirical Evaluation of Confidence Elicitation in LLMs.pdf}
}

@article{xu2022,
  title = {Meta-Connectomic Analysis Maps Consistent, Reproducible, and Transcriptionally Relevant Functional Connectome Hubs in the Human Brain {\textbar} {{Communications Biology}}},
  author = {Xu, Zhilei and Xia, Mingrui and Wang, Xindi and Liao, Xuhong and Zhao, Tengda and He, Yong},
  year = {2022},
  month = oct,
  journal = {Communications Biology},
  volume = {5},
  number = {1},
  pages = {1056},
  issn = {2399-3642},
  doi = {10.1038/s42003-022-04028-x},
  urldate = {2023-05-13},
  abstract = {Human brain connectomes include sets of densely connected hub regions. However, the consistency and reproducibility of functional connectome hubs have not been established to date and the genetic signatures underlying robust hubs remain unknown. Here, we conduct a worldwide harmonized meta-connectomic analysis by pooling resting-state functional MRI data of 5212 healthy young adults across 61 independent cohorts. We identify highly consistent and reproducible connectome hubs in heteromodal and unimodal regions both across cohorts and across individuals, with the greatest effects observed in lateral parietal cortex. These hubs show heterogeneous connectivity profiles and are critical for both intra- and inter-network communications. Using post-mortem transcriptome datasets, we show that as compared to non-hubs, connectome hubs have a spatiotemporally distinctive transcriptomic pattern dominated by genes involved in the neuropeptide signaling pathway, neurodevelopmental processes, and metabolic processes. These results highlight the robustness of macroscopic connectome hubs and their potential cellular and molecular underpinnings, which markedly furthers our understanding of how connectome hubs emerge in development, support complex cognition in health, and are involved in disease.},
  file = {/Users/nobr/Zotero/storage/7UF4NBWI/Xu et al. - 2022 - Meta-connectomic analysis maps consistent, reproducible, and transcriptionally relevant functional connectome hubs in the human brain  Communications Biology.pdf}
}

@article{xu2022a,
  title = {Tactical {{Maneuver Strategy Learning}} from {{Land Wargame Replay Based}} on {{Convolutional Neural Network}}},
  author = {Xu, Jiale and Zhang, Haidong and Zhao, Donghai and Ni, Wancheng},
  year = {2022},
  volume = {34},
  number = {10},
  abstract = {Aiming at collecting the high valuable knowledge of action decisions in "man-in-the-loop" wargame's replay data, a method of using convolutional neural network to learn the tactical maneuver strategy model from the replay data of wargame is proposed. In this method, the tactical maneuver strategy is modeled as a classification problem of making a good choice from the target candidate locations under the influence of current situation. The key factors affecting commander's decision-making are summarized, and the basic situation features are defined, which are composed of seven attributes such as "maneuverability range and observation range". The feature dataset with positive and negative labels is established. The classifier based on convolutional neural network is designed, which can predict the maneuver terminal position of a single piece by the classification probability. Experimental results show that the prediction accuracy of the tactical maneuver strategy model based on the convolutional neural network is up to 78.96\%, which is improved by at least 4.59\% compared with other models.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/IWRWX5CW/Xu et al. - 2022 - Tactical Maneuver Strategy Learning from Land Wargame Replay Based on Convolutional Neural Network.pdf}
}

@misc{xu2024,
  title = {{{LLaVA-CoT}}: {{Let Vision Language Models Reason Step-by-Step}}},
  shorttitle = {{{LLaVA-CoT}}},
  author = {Xu, Guowei and Jin, Peng and Li, Hao and Song, Yibing and Sun, Lichao and Yuan, Li},
  year = {2024},
  month = nov,
  number = {arXiv:2411.10440},
  eprint = {2411.10440},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.10440},
  urldate = {2024-12-03},
  abstract = {Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-CoT-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-CoT not only outperforms its base model by 8.9\% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/ZWPU9ESS/Xu et al. - 2024 - LLaVA-CoT Let Vision Language Models Reason Step-by-Step.pdf;/Users/nobr/Zotero/storage/EMRHCCFX/2411.html}
}

@misc{xuReWOODecouplingReasoning2023,
  title = {{{ReWOO}}: {{Decoupling Reasoning}} from {{Observations}} for {{Efficient Augmented Language Models}}},
  shorttitle = {{{ReWOO}}},
  author = {Xu, Binfeng and Peng, Zhiyuan and Lei, Bowen and Mukherjee, Subhabrata and Liu, Yuchen and Xu, Dongkuan},
  year = {2023},
  month = may,
  number = {arXiv:2305.18323},
  eprint = {2305.18323},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-19},
  abstract = {Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4\% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/BM76NG86/Xu et al. - 2023 - ReWOO Decoupling Reasoning from Observations for Efficient Augmented Language Models.pdf;/Users/nobr/Zotero/storage/BJIPSDGD/2305.html}
}

@article{yamamoto,
  title = {Self-{{Replicating}} and {{Self-Modifying Programs}} in {{Fraglets}}},
  author = {Yamamoto, Lidia and Schreckling, Daniel and Meyer, Thomas},
  abstract = {The inherently decentralized nature of artificial chemical computing models makes them particularly attractive for building bio-inspired software with self-organizing and emergent properties. Yet it is not straightforward to construct such chemical programs, either manually or automatically.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/8CW27F34/Yamamoto et al. - Self-Replicating and Self-Modifying Programs in Fraglets.pdf}
}

@article{yamin2021,
  title = {Serious Games as a Tool to Model Attack and Defense Scenarios for Cyber-Security Exercises},
  author = {Yamin, Muhammad Mudassar and Katt, Basel and Nowostawski, Mariusz},
  year = {2021},
  month = nov,
  journal = {Computers \& Security},
  volume = {110},
  pages = {102450},
  issn = {01674048},
  doi = {10.1016/j.cose.2021.102450},
  urldate = {2023-11-12},
  langid = {english}
}

@article{yampolskiy,
  title = {How\_to\_{{Escape}}\_{{From}}\_the\_{{Simulation}}},
  author = {Yampolskiy, Roman},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/LZZT5SS8/How_to_Escape_From_the_Simulation.pdf}
}

@article{yan2022,
  title = {Research Trends and Hotspots on Connectomes from 2005 to 2021: {{A}} Bibliometric and Latent {{Dirichlet}} Allocation Application Study},
  shorttitle = {Research Trends and Hotspots on Connectomes from 2005 to 2021},
  author = {Yan, Yangye and Fan, Guoxin and Liao, Xiang and Zhao, Xudong},
  year = {2022},
  journal = {Frontiers in Neuroscience},
  volume = {16},
  issn = {1662-453X},
  urldate = {2023-05-13},
  abstract = {BackgroundThis study aimed to conduct a bibliometric analysis of publications on connectomes and illustrate its trends and hotspots using a machine-learning-based text mining algorithm.MethodsDocuments were retrieved from the Web of Science Core Collection (WoSCC) and Scopus databases and analyzed in Rstudio 1.3.1. Through quantitative and qualitative methods, the most productive and impactful academic journals in the field of connectomes were compared in terms of the total number of publications and h-index over time. Meanwhile, the countries/regions and institutions involved in connectome research were compared, as well as their scientific collaboration. The study analyzed topics and research trends by R package ``bibliometrix.'' The major topics of connectomes were classified by Latent Dirichlet allocation (LDA).ResultsA total of 14,140 publications were included in the study. NEUROIMAGE ranked first in terms of publication volume (1,427 articles) and impact factor (h-index:122) among all the relevant journals. The majority of articles were published by developed countries, with the United States having the most. Harvard Medical School and the University of Pennsylvania were the two most productive institutions. Neuroimaging analysis technology and brain functions and diseases were the two major topics of connectome research. The application of machine learning, deep learning, and graph theory analysis in connectome research has become the current trend, while an increasing number of studies were concentrating on dynamic functional connectivity. Meanwhile, researchers have begun investigating alcohol use disorders and migraine in terms of brain connectivity in the past 2 years.ConclusionThis study illustrates a comprehensive overview of connectome research and provides researchers with critical information for understanding the recent trends and hotspots of connectomes.},
  file = {/Users/nobr/Zotero/storage/B63ECAS3/Yan et al. - 2022 - Research trends and hotspots on connectomes from 2.pdf}
}

@article{yang2018postdrought,
  title = {Post-Drought Decline of the {{Amazon}} Carbon Sink},
  author = {Yang, Yan and Saatchi, Sassan S. and Xu, Liang and Yu, Yifan and Choi, Sungho and Phillips, Nathan and Kennedy, Robert and Keller, Michael and Knyazikhin, Yuri and Myneni, Ranga B.},
  year = {2018},
  journal = {Nature Communications},
  volume = {9},
  number = {3172},
  doi = {10.1038/s41467-018-05668-6}
}

@article{yang2019,
  title = {How to Study the Neural Mechanisms of Multiple Tasks},
  author = {Yang, Guangyu Robert and Cole, Michael W and Rajan, Kanaka},
  year = {2019},
  month = oct,
  journal = {Current opinion in behavioral sciences},
  volume = {29},
  pages = {134--143},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2019.07.001},
  urldate = {2024-11-27},
  abstract = {Most biological and artificial neural systems are capable of completing multiple tasks. However, the neural mechanism by which multiple tasks are accomplished within the same system is largely unclear. We start by discussing how different tasks can be related, and methods to generate large sets of inter-related tasks to study how neural networks and animals perform multiple tasks. We then argue that there are mechanisms that emphasize either specialization or flexibility. We will review two such neural mechanisms underlying multiple tasks at the neuronal level (modularity and mixed selectivity), and discuss how different mechanisms can emerge depending on training methods in neural networks.},
  pmcid = {PMC7266112},
  pmid = {32490053},
  file = {/Users/nobr/Zotero/storage/PPG89L8V/Yang et al. - 2019 - How to study the neural mechanisms of multiple tasks.pdf}
}

@article{yang2020,
  title = {An {{Overview}} of the {{Attention Mechanisms}} in {{Computer Vision}}},
  author = {Yang, Xiao},
  year = {2020},
  month = dec,
  journal = {Journal of Physics: Conference Series},
  volume = {1693},
  number = {1},
  pages = {012173},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1693/1/012173},
  urldate = {2023-11-16},
  abstract = {Deep convolutional neural network (CNN) plays an important role in the field of computer vision and image processing. In order to further improve the performance of CNN, scholars have conducted a series of new explorations, such as the improvement of activation functions, the construction of new loss functions, the regularization of parameters and the development of new network structures. However, every breakthrough of CNN comes from the innovation of network structure, whose design can be inspired by exploring the cognitive process of human brain. As one of the important features of human visual system, visual attention mechanism is essential in image generation, scene classification, target detection and tracking when applied in the field of computer vision. Focusing on the models of attention mechanisms commonly used in computer vision, their categorizations, principles, and outlook are summarized in this overview.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3Y4TKUFV/Yang - 2020 - An Overview of the Attention Mechanisms in Computer Vision.pdf}
}

@article{yang2020a,
  title = {Maneuver {{Decision}} of {{UAV}} in {{Short-Range Air Combat Based}} on {{Deep Reinforcement Learning}}},
  author = {Yang, Qiming and Zhang, Jiandong and Shi, Guoqing and Hu, Jinwen and Wu, Yong},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {363--378},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2961426},
  urldate = {2024-02-15},
  abstract = {With the development of artificial intelligence and integrated sensor technologies, unmanned aerial vehicles (UAVs) are more and more applied in the air combats. A bottleneck that constrains the capability of UAVs against manned vehicles is the autonomous maneuver decision, which is a very challenging problem in the short-range air combat undergoing highly dynamic and uncertain maneuvers of enemies. In this paper, an autonomous maneuver decision model is proposed for the UAV short-range air combat based on reinforcement learning, which mainly includes the aircraft motion model, one-to-one short-range air combat evaluation model and the maneuver decision model based on deep Q network (DQN). However, such model includes a high dimensional state and action space which requires huge computation load for DQN training using traditional methods. Then, a phased training method, called ``basic-confrontation'', which is based on the idea that human beings gradually learn from simple to complex is proposed to help reduce the training time while getting suboptimal but efficient results. Finally, one-to-one short-range air combats are simulated under different target maneuver policies. Simulation results show that the proposed maneuver decision model and training method can help the UAV achieve autonomous decision in the air combats and obtain an effective decision policy to defeat the opponent.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BBE46XZL/Yang et al. - 2020 - Maneuver Decision of UAV in Short-Range Air Combat Based on Deep Reinforcement Learning.pdf}
}

@incollection{yang2021a,
  title = {Artificial {{Intelligence}} in {{Pilot Training}} and {{Education}} -- {{Towards}} a {{Machine Learning Aided Instructor Assistant}} for {{Flight Simulators}}},
  booktitle = {{{HCI International}} 2021 - {{Posters}}},
  author = {Yang, Shuiqiao and Yu, Kun and Lammers, Thorsten and Chen, Fang},
  editor = {Stephanidis, Constantine and Antona, Margherita and Ntoa, Stavroula},
  year = {2021},
  volume = {1420},
  pages = {581--587},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-78642-7_78},
  urldate = {2024-02-15},
  abstract = {The a viation industry was set to see unprecedented growth over the next two decades. Key occupations predicted to be in shortage included not only pilots, but also flight instructors. Undoubtedly, Covid-19 is currently having a huge impact on the industry. Nevertheless, the current environment further strengthens the need for pilots to maintain their training. Consequently, there is pressure to deliver high-quality training outcomes for an increasing number of pilots and trainees with limited resources available. Current simulator-based training schemes are limited by placing a significant reliance on the personal experience of flight instructors to assess pilot performance. Finding ways to increase the quality and efficiency of simulator-based training is therefore of high importance. With recent advances in artificial intelligence, it is possible to use machine learning techniques to extract latent patterns from massive datasets, to analyze pilot trainees' activities, and to provide feedback on their performance by processing hundreds of different parameters available on flight simulators. An ML-aided pilot training and education framework is needed that exploits the power of the ML techniques for more objective performance evaluation. In this paper, we describe a conceptual framework for such a system and outline steps toward the development of a full digital instructor system with the potential to overcome current limitations and enabling comprehensive and meaningful feedback that is tailored to the individual need of the trainee.},
  isbn = {978-3-030-78641-0 978-3-030-78642-7},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/RVW2XQ6B/Yang et al. - 2021 - Artificial Intelligence in Pilot Training and Education – Towards a Machine Learning Aided Instructo.pdf}
}

@article{yang2023,
  title = {Pre-Train {{Graph Neural Networks}} for {{Brain Network Analysis}}},
  author = {Yang, Yi and Cui, Hejie and Yang, Carl},
  year = {2023},
  month = feb,
  urldate = {2023-05-16},
  abstract = {Human brains, controlling behaviors and cognition, are at the center of complex neurobiological systems. Recent studies in neuroscience and neuroimaging analysis have reached a consensus that interactions among brain regions of interest (ROIs) are driving factors for neural development and disorders. Graph neural networks as a powerful tool for analyzing graph-structured data are naturally applied to the analysis of brain networks. However, training of deep learning models including GNNs often requires a significant amount of labeled data. Due to the complicated data acquisition process and restrictions on data sharing, brain network datasets are still small compared to other domains (e.g., molecules, proteins). Moreover, real clinical tasks (e.g., mental disorder analysis) are often conducted on local datasets with even smaller scales and larger noises. To this end, we propose to leverage pre-training to capture the intrinsic brain network structures regardless of specific clinical outcomes. Specifically, we characterize the contributions in this work from two perspectives: (1) We design brain-network-oriented unsupervised pre-training techniques to utilize large-scale brain imaging studies without highly relevant task labels. (2) To facilitate effective knowledge transfer across studies with different ROI systems, we propose to develop a data-driven parcellation atlas mapping pipeline. The proposed pre-training techniques are validated with various GNN models. Extensive experiments demonstrate consistent improvement in performance as well as robustness.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/HD3PS3DT/Yang et al. - 2023 - Pre-train Graph Neural Networks for Brain Network .pdf}
}

@article{yang2023a,
  title = {{{AlphaFold2}} and Its Applications in the Fields of Biology and Medicine},
  author = {Yang, Zhenyu and Zeng, Xiaoxi and Zhao, Yi and Chen, Runsheng},
  year = {2023},
  month = mar,
  journal = {Signal Transduction and Targeted Therapy},
  volume = {8},
  number = {1},
  pages = {1--14},
  publisher = {Nature Publishing Group},
  issn = {2059-3635},
  doi = {10.1038/s41392-023-01381-z},
  urldate = {2024-11-23},
  abstract = {AlphaFold2 (AF2) is an artificial intelligence (AI) system developed by DeepMind that can predict three-dimensional (3D) structures of proteins from amino acid sequences with atomic-level accuracy. Protein structure prediction is one of the most challenging problems in computational biology and chemistry, and has puzzled scientists for 50 years. The advent of AF2 presents an unprecedented progress in protein structure prediction and has attracted much attention. Subsequent release of structures of more than 200 million proteins predicted by AF2 further aroused great enthusiasm in the science community, especially in the fields of biology and medicine. AF2 is thought to have a significant impact on structural biology and research areas that need protein structure information, such as drug discovery, protein design, prediction of protein function, et al. Though the time is not long since AF2 was developed, there are already quite a few application studies of AF2 in the fields of biology and medicine, with many of them having preliminarily proved the potential of AF2. To better understand AF2 and promote its applications, we will in this article summarize the principle and system architecture of AF2 as well as the recipe of its success, and particularly focus on reviewing its applications in the fields of biology and medicine. Limitations of current AF2 prediction will also be discussed.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Structural biology},
  file = {/Users/nobr/Zotero/storage/6QFPD4UF/Yang et al. - 2023 - AlphaFold2 and its applications in the fields of biology and medicine.pdf}
}

@misc{yangOverviewMultiAgentReinforcement2021,
  title = {An {{Overview}} of {{Multi-Agent Reinforcement Learning}} from {{Game Theoretical Perspective}}},
  author = {Yang, Yaodong and Wang, Jun},
  year = {2021},
  month = mar,
  number = {arXiv:2011.00583},
  eprint = {2011.00583},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-13},
  abstract = {Following the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier. The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/WBFCILG2/Yang and Wang - 2021 - An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective.pdf;/Users/nobr/Zotero/storage/RPNT8U3Z/2011.html}
}

@article{ybaena2022,
  title = {Hypes and {{Hopes}} of {{Stem Cell Therapies}} in {{Dentistry}}: A {{Review}}},
  shorttitle = {Hypes and {{Hopes}} of {{Stem Cell Therapies}} in {{Dentistry}}},
  author = {{y Baena}, Alessandra Rodriguez and Casasco, Andrea and Monti, Manuela},
  year = {2022},
  journal = {Stem Cell Reviews and Reports},
  volume = {18},
  number = {4},
  pages = {1294--1308},
  issn = {2629-3269},
  doi = {10.1007/s12015-021-10326-4},
  urldate = {2023-08-17},
  abstract = {One of the most exciting advances in life science research is the development of 3D cell culture systems to obtain complex structures called organoids and spheroids. These 3D cultures closely mimic in vivo conditions, where cells can grow and interact with their surroundings. This allows us to better study the spatio-temporal dynamics of organogenesis and organ function. Furthermore, physiologically relevant organoids cultures can be used for basic research, medical research, and drug discovery. Although most of the research thus far focuses on the development of heart, liver, kidney, and brain organoids, to name a few, most recently, these structures were obtained using dental stem cells to study in vitro tooth regeneration. This review aims to present the most up-to-date research showing how dental stem cells can be grown on specific biomaterials to induce their differentiation in 3D. The possibility of combining engineering and biology principles to replicate and/or increase tissue function has been an emerging and exciting field in medicine. The use of this methodology in dentistry has already yielded many interesting results paving the way for the improvement of dental care and successful therapies.},
  pmcid = {PMC8748526},
  pmid = {35015212},
  file = {/Users/nobr/Zotero/storage/VEFI8AZ9/y Baena et al. - 2022 - Hypes and Hopes of Stem Cell Therapies in Dentistr.pdf}
}

@misc{ye2024,
  title = {Differential {{Transformer}}},
  author = {Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
  year = {2024},
  month = oct,
  number = {arXiv:2410.05258},
  eprint = {2410.05258},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-17},
  abstract = {Transformer tends to overallocate attention to irrelevant context. In this work, we introduce DIFF Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that DIFF Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, DIFF Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, DIFF Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position DIFF Transformer as a highly effective and promising architecture to advance large language models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/VF3QES7Q/2410.05258v1.pdf}
}

@misc{ye2025,
  title = {{{LIMO}}: {{Less}} Is {{More}} for {{Reasoning}}},
  shorttitle = {{{LIMO}}},
  author = {Ye, Yixin and Huang, Zhen and Xiao, Yang and Chern, Ethan and Xia, Shijie and Liu, Pengfei},
  year = {2025},
  month = feb,
  number = {arXiv:2502.03387},
  eprint = {2502.03387},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2502.03387},
  urldate = {2025-02-10},
  abstract = {We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data ({$>$}100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1\% accuracy on AIME and 94.8\% on MATH, improving from previous SFT-based models' 6.5\% and 59.2\% respectively, while only using 1\% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5\% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as "cognitive templates" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/MAR5AXHK/Ye et al. - 2025 - LIMO Less is More for Reasoning.pdf}
}

@misc{yim2024,
  title = {Evaluating and {{Enhancing LLMs Agent}} Based on {{Theory}} of {{Mind}} in {{Guandan}}: {{A Multi-Player Cooperative Game}} under {{Imperfect Information}}},
  shorttitle = {Evaluating and {{Enhancing LLMs Agent}} Based on {{Theory}} of {{Mind}} in {{Guandan}}},
  author = {Yim, Yauwai and Chan, Chunkit and Shi, Tianyu and Deng, Zheye and Fan, Wei and Zheng, Tianshi and Song, Yangqiu},
  year = {2024},
  month = aug,
  number = {arXiv:2408.02559},
  eprint = {2408.02559},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.02559},
  urldate = {2024-12-03},
  abstract = {Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/PZU48PGN/Yim et al. - 2024 - Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan A Multi-Player Cooperative G.pdf;/Users/nobr/Zotero/storage/4HS4I9H6/2408.html}
}

@misc{yoshidaTextMotionGrounding2023,
  title = {From {{Text}} to {{Motion}}: {{Grounding GPT-4}} in a {{Humanoid Robot}} "{{Alter3}}"},
  shorttitle = {From {{Text}} to {{Motion}}},
  author = {Yoshida, Takahide and Masumori, Atsushi and Ikegami, Takashi},
  year = {2023},
  month = dec,
  number = {arXiv:2312.06571},
  eprint = {2312.06571},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-29},
  abstract = {We report the development of Alter3, a humanoid robot capable of generating spontaneous motion using a Large Language Model (LLM), specifically GPT-4. This achievement was realized by integrating GPT-4 into our proprietary android, Alter3, thereby effectively grounding the LLM with Alter's bodily movement. Typically, low-level robot control is hardware-dependent and falls outside the scope of LLM corpora, presenting challenges for direct LLM-based robot control. However, in the case of humanoid robots like Alter3, direct control is feasible by mapping the linguistic expressions of human actions onto the robot's body through program code. Remarkably, this approach enables Alter3 to adopt various poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate sequences of actions over time without explicit programming for each body part. This demonstrates the robot's zero-shot learning capabilities. Additionally, verbal feedback can adjust poses, obviating the need for fine-tuning. A video of Alter3's generated motions is available at this URL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/TTU66ATZ/Yoshida et al. - 2023 - From Text to Motion Grounding GPT-4 in a Humanoid Robot Alter3.pdf}
}

@misc{youPacketRoutingFullydistributed2019,
  title = {Toward {{Packet Routing}} with {{Fully-distributed Multi-agent Deep Reinforcement Learning}}},
  author = {You, Xinyu and Li, Xuanjie and Xu, Yuedong and Feng, Hui and Zhao, Jin and Yan, Huaicheng},
  year = {2019},
  month = nov,
  number = {arXiv:1905.03494},
  eprint = {1905.03494},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-05},
  abstract = {Packet routing is one of the fundamental problems in computer networks in which a router determines the next-hop of each packet in the queue to get it as quickly as possible to its destination. Reinforcement learning (RL) has been introduced to design autonomous packet routing policies with local information of stochastic packet arrival and service. However, the curse of dimensionality of RL prohibits the more comprehensive representation of dynamic network states, thus limiting its potential benefit. In this paper, we propose a novel packet routing framework based on {\textbackslash}emph\{multi-agent\} deep reinforcement learning (DRL) in which each router possess an {\textbackslash}emph\{independent\} LSTM recurrent neural network for training and decision making in a {\textbackslash}emph\{fully distributed\} environment. The LSTM recurrent neural network extracts routing features from rich information regarding backlogged packets and past actions, and effectively approximates the value function of Q-learning. We further allow each route to communicate periodically with direct neighbors so that a broader view of network state can be incorporated. Experimental results manifest that our multi-agent DRL policy can strike the delicate balance between congestion-aware and shortest routes, and significantly reduce the packet delivery time in general network topologies compared with its counterparts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Networking and Internet Architecture},
  file = {/Users/nobr/Zotero/storage/VIL9V3KM/You et al. - 2019 - Toward Packet Routing with Fully-distributed Multi-agent Deep Reinforcement Learning.pdf;/Users/nobr/Zotero/storage/KEZJW7HR/1905.html}
}

@inproceedings{yu2021,
  title = {The {{Surprising Effectiveness}} of {{PPO}} in {{Cooperative Multi-Agent Games}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Wang, Yu and Bayen, A. and Wu, Yi},
  year = {2021},
  month = mar,
  urldate = {2024-12-10},
  abstract = {Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at {\textbackslash}url\{https://github.com/marlbenchmark/on-policy\}.},
  file = {/Users/nobr/Zotero/storage/CTLEI6BW/Yu et al. - 2021 - The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games.pdf}
}

@inproceedings{yu2021a,
  title = {Information-{{Theoretic Methods}} in {{Deep Neural Networks}}: {{Recent Advances}} and {{Emerging Opportunities}}},
  shorttitle = {Information-{{Theoretic Methods}} in {{Deep Neural Networks}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Yu, Shujian and Sanchez Giraldo, Luis and Principe, Jose},
  year = {2021},
  month = aug,
  pages = {4669--4678},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Montreal, Canada},
  doi = {10.24963/ijcai.2021/633},
  urldate = {2024-11-21},
  abstract = {We present a review on the recent advances and emerging opportunities around the theme of analyzing deep neural networks (DNNs) with information-theoretic methods. We first discuss popular information-theoretic quantities and their estimators. We then introduce recent developments on information-theoretic learning principles (e.g., loss functions, regularizers and objectives) and their parameterization with DNNs. We finally briefly review current usages of informationtheoretic concepts in a few modern machine learning problems and list a few emerging opportunities.},
  isbn = {978-0-9992411-9-6},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/FQP7GGJ2/Yu et al. - 2021 - Information-Theoretic Methods in Deep Neural Networks Recent Advances and Emerging Opportunities.pdf}
}

@article{yu2022,
  title = {Urban Neighbourhood Classification and Multi-Scale Heterogeneity Analysis of {{Greater London}}},
  author = {Yu, Tengfei and S{\"u}tzl, Birgit S and Van Reeuwijk, Maarten},
  year = {2022},
  month = nov,
  journal = {Environment and Planning B: Urban Analytics and City Science},
  pages = {239980832211408},
  issn = {2399-8083, 2399-8091},
  doi = {10.1177/23998083221140890},
  urldate = {2023-07-04},
  abstract = {We study the compositional and configurational heterogeneity of Greater London at the city- and neighbourhood-scale using Geographic Information System (GIS) data. Urban morphometric indicators are calculated including plan-area indices and fractal dimensions of land cover, frontal area index of buildings, evenness, and contagion. To distinguish between city-scale heterogeneity and neighbourhood-scale heterogeneity, the study area of 720 km2 is divided into 1 {\texttimes} 1 km2 neighbourhoods. City-scale heterogeneity is represented by categorisation of the neighbourhoods using a k-means clustering algorithm based on the morphometric indicators. This results in six neighbourhood types ranging from ``greenspace'' to ``central business district''. Neighbourhood-scale heterogeneity is quantified using a hierarchical multi-scale analysis for each neighbourhood type. The analysis reveals the dominant length scales for land-cover and neighbourhood types and the resolutions with the most information gain. We analyse multi-scale anisotropy and show that smallscale features are homogeneous, and that anisotropy is present at larger length scales.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/XLLLUQF8/Yu et al. - 2022 - Urban neighbourhood classification and multi-scale.pdf}
}

@misc{yu2022a,
  title = {The {{Surprising Effectiveness}} of {{PPO}} in {{Cooperative}}, {{Multi-Agent Games}}},
  author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  year = {2022},
  month = nov,
  number = {arXiv:2103.01955},
  eprint = {2103.01955},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.01955},
  urldate = {2024-05-30},
  abstract = {Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at {\textbackslash}url\{https://github.com/marlbenchmark/on-policy\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/nobr/Zotero/storage/ATRYYEKU/Yu et al. - 2022 - The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games.pdf;/Users/nobr/Zotero/storage/6P9A4ZNG/2103.html}
}

@article{yu2023a,
  title = {The {{Analysis}} of {{Deep Neural Networks}} by {{Information Theory}}: {{From Explainability}} to {{Generalization}}},
  shorttitle = {The {{Analysis}} of {{Deep Neural Networks}} by {{Information Theory}}},
  author = {Yu, Shujian},
  year = {2023},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {13},
  pages = {15462--15462},
  issn = {2374-3468},
  doi = {10.1609/aaai.v37i13.26829},
  urldate = {2024-11-21},
  abstract = {Despite their great success in many artificial intelligence tasks, deep neural networks (DNNs) still suffer from a few limitations, such as poor generalization behavior for out-of-distribution (OOD) data and the "black-box" nature. Information theory offers fresh insights to solve these challenges. In this short paper, we briefly review the recent developments in this area, and highlight our contributions.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {New Faculty Highlights},
  file = {/Users/nobr/Zotero/storage/P3HDWYFZ/Yu - 2023 - The Analysis of Deep Neural Networks by Information Theory From Explainability to Generalization.pdf}
}

@article{yuan,
  title = {Native {{Sparse Attention}}: {{Hardware-Aligned}} and {{Natively Trainable Sparse Attention}}},
  author = {Yuan, Jingyang and Gao, Huazuo and Dai, Damai and Luo, Junyu and Zhao, Liang and Zhang, Zhengyan and Xie, Zhenda and Wei, Y X and Wang, Lean and Xiao, Zhiping and Wang, Yuqing and Ruan, Chong and Zhang, Ming and Liang, Wenfeng and Zeng, Wangding},
  abstract = {Long-context modeling is crucial for nextgeneration language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with finegrained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/SZT8WZTC/Yuan et al. - Native Sparse Attention Hardware-Aligned and Natively Trainable Sparse Attention.pdf}
}

@article{yuan2023,
  title = {Ensemble Strategy Learning for Imperfect Information Games},
  author = {Yuan, Weilin and Chen, Shaofei and Li, Peng and Chen, Jing},
  year = {2023},
  month = aug,
  journal = {Neurocomputing},
  volume = {546},
  pages = {126241},
  issn = {09252312},
  doi = {10.1016/j.neucom.2023.126241},
  urldate = {2023-11-12},
  abstract = {Algorithms with several paradigms (such as rule-based methods, game theory and reinforcement learning) have achieved great success in solving imperfect information games (IIGs). However, agents based on a single paradigm tend to be brittle in certain aspects due to the paradigm's weaknesses. In this paper, we first present three base-solvers with diversified paradigms for IIGs, and then combine them to design three ensemble-solvers (including an attention ensemble-solver, a gradient ensemble-solver and an evolution ensemble-solver) to learn ensemble strategies given base-solvers' strengths. We evaluate our methods on Leduc poker with nonstationary opponents and limited games. The results show that our ensemble strategy learning method can effectively integrate the advantages of various advanced individual algorithms and significantly outperform them.},
  langid = {english}
}

@misc{yuanSelfRewardingLanguageModels2024,
  title = {Self-{{Rewarding Language Models}}},
  author = {Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  year = {2024},
  month = jan,
  number = {arXiv:2401.10020},
  eprint = {2401.10020},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-21},
  abstract = {We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/W8JF5NJV/Yuan et al. - 2024 - Self-Rewarding Language Models.pdf}
}

@misc{yuMEGABYTEPredictingMillionbyte2023,
  title = {{{MEGABYTE}}: {{Predicting Million-byte Sequences}} with {{Multiscale Transformers}}},
  shorttitle = {{{MEGABYTE}}},
  author = {Yu, Lili and Simig, D{\'a}niel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
  year = {2023},
  month = may,
  number = {arXiv:2305.07185},
  eprint = {2305.07185},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07185},
  urldate = {2024-02-21},
  abstract = {Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/RQF3YAPT/Yu et al. - 2023 - MEGABYTE Predicting Million-byte Sequences with Multiscale Transformers.pdf;/Users/nobr/Zotero/storage/NJPIL5SP/2305.html}
}

@misc{yun2021,
  title = {Multi-{{Agent Deep Reinforcement Learning}} Using {{Attentive Graph Neural Architectures}} for {{Real-Time Strategy Games}}},
  author = {Yun, Won Joon and Yi, Sungwon and Kim, Joongheon},
  year = {2021},
  month = may,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {In real-time strategy (RTS) game artificial intelligence research, various multi-agent deep reinforcement learning (MADRL) algorithms are widely and actively used nowadays. Most of the research is based on StarCraft II environment because it is the most well-known RTS games in worldwide. In our proposed MADRL-based algorithm, distributed MADRL is fundamentally used that is called QMIX. In addition to QMIX-based distributed computation, we consider state categorization which can reduce computational complexity significantly. Furthermore, self-attention mechanisms are used for identifying the relationship among agents in the form of graphs. Based on these approaches, we propose a categorized state graph attention policy (CSGA-policy). As observed in the performance evaluation of our proposed CSGA-policy with the most well-known StarCraft II simulation environment, our proposed algorithm works well in various settings, as expected.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/97WA8PJU/Yun et al. - 2021 - Multi-Agent Deep Reinforcement Learning using Attentive Graph Neural Architectures for Real-Time Str.pdf}
}

@article{zangerle2022,
  title = {Overview of the {{Style Change Detection Task}} at {{PAN}} 2022},
  author = {Zangerle, Eva and Mayerl, Maximilian and Potthast, Martin and Stein, Benno},
  year = {2022},
  urldate = {2023-04-18},
  abstract = {Style change detection means to identify positions at which the authorship in a multi-author document changes. Reliably detecting these positions is key for multi-author document analyses, and it is a preliminary step for authorship identification. This year style change detection task at PAN features three connected subtasks: (1) For a text written by two authors, which contains a single style change only, find the position of this change, i.e., cut the text into the two authors' texts on the paragraph-level. (2) For a text written by two or more authors, find all positions of writing style change, i.e., assign all paragraphs of the text uniquely to some author out of the number of authors assumed for the multi-author document. (3) For a text written by two or more authors, find all positions of writing style change. In particular, style changes may occur both between paragraphs but also at sentence level. The task is evaluated on a dataset compiled from an English Q\&A platform. The paper in hand introduces the style change detection task, the underlying dataset, the approaches employed by the participants, and the achieved results.},
  file = {/Users/nobr/Zotero/storage/DHSVFG5P/full-text.pdf}
}

@article{zaretskaya2018,
  title = {Advantages of Cortical Surface Reconstruction Using Submillimeter 7 {{Tesla MEMPRAGE}}},
  author = {Zaretskaya, Natalia and Fischl, Bruce and Reuter, Martin and Renvall, Ville and Polimeni, Jonathan R.},
  year = {2018},
  month = jan,
  journal = {NeuroImage},
  volume = {165},
  pages = {11--26},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2017.09.060},
  urldate = {2023-05-12},
  abstract = {Recent advances in MR technology have enabled increased spatial resolution for routine functional and anatomical imaging, which has created demand for software tools that are able to process these data. The availability of high-resolution data also raises the question of whether higher resolution leads to substantial gains in accuracy of quantitative morphometric neuroimaging procedures, in particular the cortical surface reconstruction and cortical thickness estimation. In this study we adapted the FreeSurfer cortical surface reconstruction pipeline to process structural data at native submillimeter resolution. We then quantified the differences in surface placement between meshes generated from 0.75 mm isotropic resolution data acquired in 39 volunteers and the same data downsampled to the conventional 1 mm3 voxel size. We find that when processed at native resolution, cortex is estimated to be thinner in most areas, but thicker around the Cingulate and the Calcarine sulci as well as in the posterior bank of the Central sulcus. Thickness differences are driven by two kinds of effects. First, the gray--white surface is found closer to the white matter, especially in cortical areas with high myelin content, and thus low contrast, such as the Calcarine and the Central sulci, causing local increases in thickness estimates. Second, the gray--CSF surface is placed more interiorly, especially in the deep sulci, contributing to local decreases in thickness estimates. We suggest that both effects are due to reduced partial volume effects at higher spatial resolution. Submillimeter voxel sizes can therefore provide improved accuracy for measuring cortical thickness.},
  pmcid = {PMC6383677},
  pmid = {28970143},
  file = {/Users/nobr/Zotero/storage/R6YJ5RNJ/Zaretskaya et al. - 2018 - Advantages of cortical surface reconstruction usin.pdf}
}

@incollection{zeiler2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10590-1_53},
  urldate = {2024-06-16},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/JCAGIG5W/Zeiler and Fergus - 2014 - Visualizing and Understanding Convolutional Networks.pdf}
}

@misc{zelikmanSelfTaughtOptimizerSTOP2023,
  title = {Self-{{Taught Optimizer}} ({{STOP}}): {{Recursively Self-Improving Code Generation}}},
  shorttitle = {Self-{{Taught Optimizer}} ({{STOP}})},
  author = {Zelikman, Eric and Lorch, Eliana and Mackey, Lester and Kalai, Adam Tauman},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02304},
  eprint = {2310.02304},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.02304},
  urldate = {2023-10-15},
  abstract = {Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our proof-of-concept experiments, is capable of writing code that can call itself to improve itself. We critically consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/KASG78AS/Zelikman et al. - 2023 - Self-Taught Optimizer (STOP) Recursively Self-Improving Code Generation.pdf;/Users/nobr/Zotero/storage/J2CVL2TF/2310.html}
}

@article{zellers2019,
  title = {{{HellaSwag}}: {{Can}} a {{Machine Really Finish Your Sentence}}?},
  author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  year = {2019},
  pages = {4791--4800},
  doi = {10.18653/v1/P19-1472},
  file = {/Users/nobr/Zotero/storage/DAI5D84M/document.pdf}
}

@article{zeng2018,
  title = {Mesoscale Connectomics},
  author = {Zeng, Hongkui},
  year = {2018},
  month = jun,
  journal = {Current Opinion in Neurobiology},
  volume = {50},
  pages = {154--162},
  issn = {1873-6882},
  doi = {10.1016/j.conb.2018.03.003},
  abstract = {Brain cells communicate with one another via local and long-range synaptic connections. Structural connectivity is the foundation for neural function. Brain-wide connectivity can be described at macroscopic, mesoscopic and microscopic levels. The mesoscale connectome represents connections between neuronal types across different brain regions. Building a mesoscale connectome requires a detailed understanding of the cell type composition of different brain regions and the patterns of inputs and outputs that each of these cell types receives and forms, respectively. In this review, I discuss historical and contemporary tracing techniques in both anterograde and retrograde directions to map the input and output connections at population and individual cell levels, as well as imaging and network analysis approaches to build mesoscale connectomes for mammalian brains.},
  langid = {english},
  pmcid = {PMC6027632},
  pmid = {29579713},
  keywords = {Animals,Brain,Connectome,Humans,Nerve Net,Neural Pathways,Neurons},
  file = {/Users/nobr/Zotero/storage/KQJVSKRH/Zeng - 2018 - Mesoscale connectomics.pdf}
}

@article{zerilli2022,
  title = {Explaining {{Machine Learning Decisions}}},
  author = {Zerilli, John},
  year = {2022},
  month = jan,
  journal = {Philosophy of Science},
  volume = {89},
  number = {1},
  pages = {1--19},
  issn = {0031-8248, 1539-767X},
  doi = {10.1017/psa.2021.13},
  urldate = {2024-05-27},
  abstract = {The operations of deep networks are widely acknowledged to be inscrutable. The growing field of ``Explainable AI'' (XAI) has emerged in direct response to this problem. However, owing to the nature of the opacity in question, XAI has been forced to prioritise interpretability at the expense of completeness, and even realism, so that its explanations are frequently interpretable without being underpinned by more comprehensive explanations faithful to the way a network computes its predictions. While this has been taken to be a shortcoming of the field of XAI, I argue that it is broadly the right approach to the problem.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/9YNEA9FA/Zerilli - 2022 - Explaining Machine Learning Decisions.pdf}
}

@article{zhang2015,
  title = {Assessing the Role of Network Topology in Transportation Network Resilience},
  author = {Zhang, X. and {Miller-Hooks}, E. and Denny, K.},
  year = {2015},
  month = jun,
  journal = {Journal of Transport Geography},
  volume = {46},
  pages = {35--45},
  issn = {09666923},
  doi = {10.1016/j.jtrangeo.2015.05.006},
  urldate = {2023-07-06},
  abstract = {The abstract representation of a transportation system as a network of nodes and interconnecting links, whether that system involves roadways, railways, sea links, airspace, or intermodal combinations, defines a network topology. Among the most common in the context of transportation systems are the grid, ring, hub-and-spoke, complete, scale-free and small-world networks. This paper investigates the role of network topology, and the topology's characteristics, in a transportation system's ability to cope with disaster. Specifically, the paper hypothesizes that the topological attributes of a transportation system significantly affect its resilience to disaster events. Resilience accounts for not only the innate ability of the system to absorb externally induced changes, but also cost-effective and efficient, adaptive actions that can be taken to preserve or restore performance post-event. Comprehensive and systematically designed numerical experiments were conducted on 17 network structures with some relation to transportation system layout. Resilience of these network structures in terms of throughput, connectivity or compactness was quantified. Resilience is considered with and without the benefits of preparedness and recovery actions. The impact of component-level damage on system resilience is also investigated. A comprehensive, systematic analysis of results from these experiments provides a basis for the characterization of highly resilient network topologies and conversely identification of network attributes that might lead to poorly performing systems.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/BJYZLTEB/Zhang et al. - 2015 - Assessing the role of network topology in transpor.pdf}
}

@article{zhang2019exposure,
  title = {Exposure to Glyphosate-Based Herbicides and Risk for Non-Hodgkin Lymphoma: A Meta-Analysis and Supporting Evidence},
  author = {Zhang, Luoping and Rana, Iemaan and Taioli, Emanuela and Shaffer, Rachel M. and Sheppard, Lianne},
  year = {2019},
  journal = {Mutation Research-Reviews in Mutation Research},
  volume = {781},
  pages = {186--206},
  doi = {10.1016/j.mrrev.2019.02.001}
}

@misc{zhang2021,
  title = {Multi-{{Agent Reinforcement Learning}}: {{A Selective Overview}} of {{Theories}} and {{Algorithms}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  author = {Zhang, Kaiqing and Yang, Zhuoran and Ba{\c s}ar, Tamer},
  year = {2021},
  month = apr,
  number = {arXiv:1911.10635},
  eprint = {1911.10635},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-10},
  abstract = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered tremendous success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/T46ZYABB/Zhang et al. - 2021 - Multi-Agent Reinforcement Learning A Selective Ov.pdf}
}

@article{zhang2023,
  title = {Llama-Adapter: {{Efficient}} Fine-Tuning of Language Models with Zero-Init Attention},
  author = {Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  year = {2023},
  journal = {arXiv preprint arXiv:2303.16199},
  eprint = {2303.16199},
  archiveprefix = {arXiv}
}

@misc{zhang2023a,
  title = {Building {{Cooperative Embodied Agents Modularly}} with {{Large Language Models}}},
  author = {Zhang, Hongxin and Du, Weihua and Shan, Jiaming and Zhou, Qinhong and Du, Yilun and Tenenbaum, Joshua B. and Shu, Tianmin and Gan, Chuang},
  year = {2023},
  month = jul,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/3YRFZTND/Zhang et al. - 2023 - Building Cooperative Embodied Agents Modularly with Large Language Models.pdf}
}

@misc{zhang2024,
  title = {Accessing {{GPT-4}} Level {{Mathematical Olympiad Solutions}} via {{Monte Carlo Tree Self-refine}} with {{LLaMa-3 8B}}},
  author = {Zhang, Di and Li, Jiatong and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli},
  year = {2024},
  month = jun,
  number = {arXiv:2406.07394},
  eprint = {2406.07394},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.07394},
  urldate = {2024-06-13},
  abstract = {This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/nobr/Zotero/storage/7XLXVL4T/Zhang et al. - 2024 - Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B.pdf}
}

@misc{zhang2024a,
  title = {{{OMNI}}: {{Open-endedness}} via {{Models}} of Human {{Notions}} of {{Interestingness}}},
  shorttitle = {{{OMNI}}},
  author = {Zhang, Jenny and Lehman, Joel and Stanley, Kenneth and Clune, Jeff},
  year = {2024},
  month = feb,
  number = {arXiv:2306.01711},
  eprint = {2306.01711},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.01711},
  urldate = {2024-12-17},
  abstract = {Open-ended algorithms aim to learn new, interesting behaviors forever. That requires a vast environment search space, but there are thus infinitely many possible tasks. Even after filtering for tasks the current agent can learn (i.e., learning progress), countless learnable yet uninteresting tasks remain (e.g., minor variations of previously learned tasks). An Achilles Heel of open-endedness research is the inability to quantify (and thus prioritize) tasks that are not just learnable, but also \${\textbackslash}textit\{interesting\}\$ (e.g., worthwhile and novel). We propose solving this problem by \${\textbackslash}textit\{Open-endedness via Models of human Notions of Interestingness\}\$ (OMNI). The insight is that we can utilize foundation models (FMs) as a model of interestingness (MoI), because they \${\textbackslash}textit\{already\}\$ internalize human concepts of interestingness from training on vast amounts of human-generated data, where humans naturally write about what they find interesting or boring. We show that FM-based MoIs improve open-ended learning by focusing on tasks that are both learnable \${\textbackslash}textit\{and interesting\}\$, outperforming baselines based on uniform task sampling or learning progress alone. This approach has the potential to dramatically advance the ability to intelligently select which tasks to focus on next (i.e., auto-curricula), and could be seen as AI selecting its own next task to learn, facilitating self-improving AI and AI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/EM7QFB2G/Zhang et al. - 2024 - OMNI Open-endedness via Models of human Notions of Interestingness.pdf}
}

@article{zhang2024b,
  title = {{{LLM}} as a {{Mastermind}}: {{A Survey}} of {{Strategic Reasoning}} with {{Large Language Models}}},
  shorttitle = {{{LLM}} as a {{Mastermind}}},
  author = {Zhang, Yadong and Mao, Shaoguang and Ge, Tao and Wang, Xun and {de Wynter}, Adrian and Xia, Yan and Wu, Wenshan and Song, Ting and Lan, Man and Wei, Furu},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2404.01230},
  urldate = {2024-12-03},
  abstract = {This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/nobr/Zotero/storage/9BTPXK3T/Zhang et al. - 2024 - LLM as a Mastermind A Survey of Strategic Reasoning with Large Language Models.pdf}
}

@misc{zhang2024c,
  title = {Towards {{Efficient LLM Grounding}} for {{Embodied Multi-Agent Collaboration}}},
  author = {Zhang, Yang and Yang, Shixin and Bai, Chenjia and Wu, Fei and Li, Xiu and Wang, Zhen and Li, Xuelong},
  year = {2024},
  month = may,
  number = {arXiv:2405.14314},
  eprint = {2405.14314},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.14314},
  urldate = {2024-12-03},
  abstract = {Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at https://read-llm.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics},
  file = {/Users/nobr/Zotero/storage/TQSAX2C6/Zhang et al. - 2024 - Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration.pdf;/Users/nobr/Zotero/storage/T7EDITNU/2405.html}
}

@article{zhangAcceleratingProbabilisticComputing2020,
  title = {Accelerating {{Probabilistic Computing}} with a {{Stochastic Processing Unit}}},
  author = {Zhang, Xiangyu},
  year = {2020},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/CA5DIDZB/Zhang - 2020 - Accelerating Probabilistic Computing with a Stochastic Processing Unit.pdf}
}

@misc{zhangCanTransformersLearn2023,
  title = {Can {{Transformers Learn}} to {{Solve Problems Recursively}}?},
  author = {Zhang, Shizhuo Dylan and Tigges, Curt and Biderman, Stella and Raginsky, Maxim and Ringer, Talia},
  year = {2023},
  month = jun,
  number = {arXiv:2305.14699},
  eprint = {2305.14699},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14699},
  urldate = {2024-04-06},
  abstract = {Neural networks have in recent years shown promise for helping software engineers write programs and even formally verify them. While semantic information plays a crucial part in these processes, it remains unclear to what degree popular neural architectures like transformers are capable of modeling that information. This paper examines the behavior of neural networks learning algorithms relevant to programs and formal verification proofs through the lens of mechanistic interpretability, focusing in particular on structural recursion. Structural recursion is at the heart of tasks on which symbolic tools currently outperform neural models, like inferring semantic relations between datatypes and emulating program behavior. We evaluate the ability of transformer models to learn to emulate the behavior of structurally recursive functions from input-output examples. Our evaluation includes empirical and conceptual analyses of the limitations and capabilities of transformer models in approximating these functions, as well as reconstructions of the ``shortcut" algorithms the model learns. By reconstructing these algorithms, we are able to correctly predict 91 percent of failure cases for one of the approximated functions. Our work provides a new foundation for understanding the behavior of neural networks that fail to solve the very tasks they are trained for.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/nobr/Zotero/storage/YIMMZRMS/Zhang et al. - 2023 - Can Transformers Learn to Solve Problems Recursively.pdf;/Users/nobr/Zotero/storage/NYN7GSVM/2305.html}
}

@inproceedings{zhangImageGSContentAdaptiveImage2025,
  title = {Image-{{GS}}: {{Content-Adaptive Image Representation}} via {{2D Gaussians}}},
  shorttitle = {Image-{{GS}}},
  booktitle = {Proceedings of the {{Special Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference Conference Papers}}},
  author = {Zhang, Yunxiang and Li, Bingxuan and Kuznetsov, Alexandr and Jindal, Akshay and Diolatzis, Stavros and Chen, Kenneth and Sochenov, Anton and Kaplanyan, Anton and Sun, Qi},
  year = {2025},
  month = aug,
  eprint = {2407.01866},
  primaryclass = {cs},
  pages = {1--11},
  doi = {10.1145/3721238.3730596},
  urldate = {2025-09-16},
  abstract = {Neural image representations have emerged as a promising approach for encoding and rendering visual data. Combined with learning-based workflows, they demonstrate impressive trade-offs between visual fidelity and memory footprint. Existing methods in this domain, however, often rely on fixed data structures that suboptimally allocate memory or compute-intensive implicit models, hindering their practicality for real-time graphics applications. Inspired by recent advancements in radiance field rendering, we introduce Image-GS, a content-adaptive image representation based on 2D Gaussians. Leveraging a custom differentiable renderer, Image-GS reconstructs images by adaptively allocating and progressively optimizing a group of anisotropic, colored 2D Gaussians. It achieves a favorable balance between visual fidelity and memory efficiency across a variety of stylized images frequently seen in graphics workflows, especially for those showing non-uniformly distributed features and in low-bitrate regimes. Moreover, it supports hardware-friendly rapid random access for real-time usage, requiring only 0.3K MACs to decode a pixel. Through error-guided progressive optimization, Image-GS naturally constructs a smooth level-of-detail hierarchy. We demonstrate its versatility with several applications, including texture compression, semantics-aware compression, and joint image compression and restoration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/nobr/Zotero/storage/BS6JGHF6/Zhang et al. - 2025 - Image-GS Content-Adaptive Image Representation via 2D Gaussians.pdf;/Users/nobr/Zotero/storage/H3FHMWFI/2407.html}
}

@misc{zhangQuaternionKnowledgeGraph2019,
  title = {Quaternion {{Knowledge Graph Embeddings}}},
  author = {Zhang, Shuai and Tay, Yi and Yao, Lina and Liu, Qi},
  year = {2019},
  month = oct,
  number = {arXiv:1904.10281},
  eprint = {1904.10281},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1904.10281},
  urldate = {2024-04-25},
  abstract = {In this work, we move beyond the traditional complex-valued representations, introducing more expressive hypercomplex representations to model entities and relations for knowledge graph embeddings. More specifically, quaternion embeddings, hypercomplex-valued embeddings with three imaginary components, are utilized to represent entities. Relations are modelled as rotations in the quaternion space. The advantages of the proposed approach are: (1) Latent inter-dependencies (between all components) are aptly captured with Hamilton product, encouraging a more compact interaction between entities and relations; (2) Quaternions enable expressive rotation in four-dimensional space and have more degree of freedom than rotation in complex plane; (3) The proposed framework is a generalization of ComplEx on hypercomplex space while offering better geometrical interpretations, concurrently satisfying the key desiderata of relational representation learning (i.e., modeling symmetry, anti-symmetry and inversion). Experimental results demonstrate that our method achieves state-of-the-art performance on four well-established knowledge graph completion benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nobr/Zotero/storage/Z4TRQHPI/Zhang et al. - 2019 - Quaternion Knowledge Graph Embeddings.pdf}
}

@article{zhao2023,
  title = {Global Trends in Incidence, Death, Burden and Risk Factors of Early-Onset Cancer from 1990 to 2019},
  author = {Zhao, Jianhui and Xu, Liying and Sun, Jing and Song, Mingyang and Wang, Lijuan and Yuan, Shuai and Zhu, Yingshuang and Wan, Zhengwei and Larsson, Susanna and Tsilidis, Konstantinos and Dunlop, Malcolm and Campbell, Harry and Rudan, Igor and Song, Peige and Theodoratou, Evropi and Ding, Kefeng and Li, Xue},
  year = {2023},
  month = jul,
  journal = {BMJ Oncology},
  volume = {2},
  number = {1},
  pages = {e000049},
  issn = {2752-7948},
  doi = {10.1136/bmjonc-2023-000049},
  urldate = {2024-03-24},
  abstract = {Objective This study aimed to explore the global burden of early-onset cancer based on the Global Burden of Disease (GBD) 2019 study for 29 cancers worldwid. Methods and analysis Incidence, deaths, disabilityadjusted life years (DALYs) and risk factors for 29 earlyonset cancer groups were obtained from GBD. Results Global incidence of early-onset cancer increased by 79.1\% and the number of early-onset cancer deaths increased by 27.7\% between 1990 and 2019. Earlyonset breast, tracheal, bronchus and lung, stomach and colorectal cancers showed the highest mortality and DALYs in 2019. Globally, the incidence rates of early-onset nasopharyngeal and prostate cancer showed the fastest increasing trend, whereas early-onset liver cancer showed the sharpest decrease. Early-onset colorectal cancers had high DALYs within the top five ranking for both men and women. High-middle and middle Sociodemographic Index (SDI) regions had the highest burden of early-onset cancer. The morbidity of early-onset cancer increased with the SDI, and the mortality rate decreased considerably when SDI increased from 0.7 to 1. The projections indicated that the global number of incidence and deaths of earlyonset cancer would increase by 31\% and 21\% in 2030, respectively. Dietary risk factors (diet high in red meat, low in fruits, high in sodium and low in milk, etc), alcohol consumption and tobacco use are the main risk factors underlying early-onset cancers. Conclusion Early-onset cancer morbidity continues to increase worldwide with notable variances in mortality and DALYs between areas, countries, sex and cancer types. Encouraging a healthy lifestyle could reduce early-onset cancer disease burden.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/ZQAB9F6H/Zhao et al. - 2023 - Global trends in incidence, death, burden and risk factors of early-onset cancer from 1990 to 2019.pdf}
}

@misc{zhao2025,
  title = {Absolute {{Zero}}: {{Reinforced Self-play Reasoning}} with {{Zero Data}}},
  shorttitle = {Absolute {{Zero}}},
  author = {Zhao, Andrew and Wu, Yiran and Yue, Yang and Wu, Tong and Xu, Quentin and Yue, Yang and Lin, Matthieu and Wang, Shenzhi and Wu, Qingyun and Zheng, Zilong and Huang, Gao},
  year = {2025},
  month = may,
  number = {arXiv:2505.03335},
  eprint = {2505.03335},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.03335},
  urldate = {2025-05-08},
  abstract = {Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/4D5PQH4I/Zhao et al. - 2025 - Absolute Zero Reinforced Self-play Reasoning with Zero Data.pdf}
}

@inproceedings{zhen2018,
  title = {Artificial {{Intelligence Techniques}} on {{Real-time Strategy Games}}},
  booktitle = {Proceedings of the 2018 2nd {{International Conference}} on {{Computer Science}} and {{Artificial Intelligence}}},
  author = {Zhen, Yang and Wanpeng, Zhang and Hongfu, Liu},
  year = {2018},
  month = dec,
  pages = {11--21},
  publisher = {ACM},
  address = {Shenzhen China},
  doi = {10.1145/3297156.3297188},
  urldate = {2023-11-12},
  abstract = {Real-time strategy (RTS) games can be seen as simulating real and complex dynamic environments in a limited and small world, posing important challenges for the development of artificial intelligence. Existing applications of artificial intelligence technology in RTS games are not yet able to compete with professional human players. But there are already ways to control the macro of RTS games, and they can compete with amateur human players. RTS games are an excellent platform for testing artificial intelligence technology, and more and more smarter methods are being used in the overall control of it. The purpose of this paper is to systematically review the artificial intelligence technologies used in RTS games in recent years, including the definition of RTS games, the challenges faced, the platform for research problems, and the artificial intelligence methods for problem solving. Finally, we propose the future research direction of real-time strategy games. This article provides a quick start guide for the researchers, the theoretical framework of the system and possible research directions.},
  isbn = {978-1-4503-6606-9},
  langid = {english}
}

@article{zheng2018,
  title = {{{MAgent}}: {{A Many-Agent Reinforcement Learning Platform}} for {{Artificial Collective Intelligence}}},
  shorttitle = {{{MAgent}}},
  author = {Zheng, Lianmin and Yang, Jiacheng and Cai, Han and Zhou, Ming and Zhang, Weinan and Wang, Jun and Yu, Yong},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11371},
  urldate = {2023-12-13},
  abstract = {We introduce MAgent, a platform to support research and development of many-agent reinforcement learning. Unlike previous research platforms on single or multi-agent reinforcement learning, MAgent focuses on supporting the tasks and the applications that require hundreds to millions of agents. Within the interactions among a population of agents, it enables not only the study of learning algorithms for agents' optimal polices, but more importantly, the observation and understanding of individual agent's behaviors and social phenomena emerging from the AI society, including communication languages, leaderships, altruism. MAgent is highly scalable and can host up to one million agents on a single GPU server. MAgent also provides flexible configurations for AI researchers to design their customized environments and agents. In this demo, we present three environments designed on MAgent and show emerged collective intelligence by learning from scratch.},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/VAVFPEUN/Zheng et al. - 2018 - MAgent A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence.pdf}
}

@misc{zheng2023,
  title = {Judging {{LLM-as-a-Judge}} with {{MT-Bench}} and {{Chatbot Arena}}},
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2023},
  month = jun,
  journal = {arXiv.org},
  urldate = {2023-12-30},
  abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
  howpublished = {https://arxiv.org/abs/2306.05685v4},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3BNDYS4L/Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.pdf}
}

@misc{zheng2024,
  title = {How {{Well Do LLMs Generate Code}} for {{Different Application Domains}}? {{Benchmark}} and {{Evaluation}}},
  shorttitle = {How {{Well Do LLMs Generate Code}} for {{Different Application Domains}}?},
  author = {Zheng, Dewu and Wang, Yanlin and Shi, Ensheng and Zhang, Hongyu and Zheng, Zibin},
  year = {2024},
  month = dec,
  number = {arXiv:2412.18573},
  eprint = {2412.18573},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.18573},
  urldate = {2025-01-02},
  abstract = {Recently, an increasing number of AI-driven programming assistants powered by code LLMs have been integrated into various real-world software development environments, significantly boosting developer productivity. However, existing code generation benchmarks primarily focus on general-purpose scenarios, leaving the code generation performance of LLMs for specific application domains largely unknown. In this paper, we introduce a new benchmark, MultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming tasks, covering 12 popular software development domains and 15 programming languages. Specifically, we perform in-depth research to identify these 12 application domains. Given that each domain may involve multiple technical frameworks, and that different frameworks present distinct challenges in the coding process, we categorize the commonly used frameworks and platforms within each domain. We then sample programming problems from GitHub repositories related to these subdomains. To ensure the quality of the tasks and mitigate data leakage issues, we invite annotators to rewrite the docstrings for each task in MultiCodeBench. Additionally, we build a static analysis-based dependency parsing tool to extract the dependencies in the ground truth for each task, enabling deeper performance analysis. Through extensive experiments on MultiCodeBench with eleven representative mainstream LLMs, we reveal the code generation performance of the LLMs across different application domains, providing practical insights for developers in downstream fields when selecting LLMs. Furthermore, we analyze the reasons behind the models' failures in completing software application development tasks, offering guidance for model developers to enhance domain-specific code generation capabilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/Users/nobr/Zotero/storage/JY4M3Q4A/Zheng et al. - 2024 - How Well Do LLMs Generate Code for Different Application Domains Benchmark and Evaluation.pdf}
}

@article{ZhiNengKeXueXueYuan2023,
  title = {{智能推演综述：博弈论视角下的战术战役兵棋与战略博弈}},
  author = {智能科学学院, 国防科技大学 and 长沙, 湖南},
  year = {2023},
  volume = {35},
  number = {9},
  abstract = {Wargaming is a pre-practice activity to study national security and competition, military conflict and war, crisis management, and other major strategic issues. An intelligent wargaming system needs the ability of artificial intelligence technology. This paper briefly summarizes the research progress of intelligent game, the evolution of wargaming, intelligent wargaming, and strategic gaming methods. From the perspective of game theory, it analyzes the game problem model for intelligent wargaming and sorts out the application mode of intelligent wargaming and the organization mode of a strategic game. An intelligent wargaming service-oriented architecture based on cloud native is proposed. Pre-training method for future decision-making cloud is explored for distributed confrontation at cloud edge, and a meta-game analysis framework for strategic wargaming is constructed for cognition of crisis events. It is expected to provide a scientific and effective reference for the research on the theory and tools of intelligent wargaming and offer theoretical and technical support for the new generation of intelligent cloud-edge cooperative command and control.},
  langid = {chinese},
  file = {/Users/nobr/Zotero/storage/7EW74ZEB/智能科学学院 and 长沙 - 2023 - 智能推演综述：博弈论视角下的战术战役兵棋与战略博弈.pdf}
}

@article{zhou2021,
  title = {Hierarchical Control of Multi-Agent Reinforcement Learning Team in Real-Time Strategy ({{RTS}}) Games},
  author = {Zhou, Weigui Jair and Subagdja, Budhitama and Tan, Ah-Hwee and Ong, Darren Wee-Sze},
  year = {2021},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {186},
  pages = {115707},
  issn = {09574174},
  doi = {10.1016/j.eswa.2021.115707},
  urldate = {2023-11-12},
  abstract = {Coordinated control of multi-agent teams is an important task in many real-time strategy (RTS) games. In most prior work, micromanagement is the commonly used strategy whereby individual agents operate independently and make their own combat decisions. On the other extreme, some employ a macromanagement strategy whereby all agents are controlled by a single decision model. In this paper, we propose a hierarchical command and control architecture, consisting of a single high-level and multiple low-level reinforcement learning agents operating in a dynamic environment. This hierarchical model enables the low-level unit agents to make individual decisions while taking commands from the high-level commander agent. Compared with prior approaches, the proposed model provides the benefits of both flexibility and coordinated control. The performance of such hierarchical control model is demonstrated through empirical experiments in a real-time strategy game known as StarCraft: Brood War (SCBW).},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/2V6XIBFX/Zhou et al. - 2021 - Hierarchical control of multi-agent reinforcement learning team in real-time strategy (RTS) games.pdf}
}

@misc{zhou2023,
  title = {Agents: {{An Open-source Framework}} for {{Autonomous Language Agents}}},
  shorttitle = {Agents},
  author = {Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Li, Long and Wu, Jialong and Wang, Tiannan and Qiu, Shi and Zhang, Jintian and Chen, Jing and Wu, Ruipu and Wang, Shuai and Zhu, Shiding and Chen, Jiyu and Zhang, Wentao and Zhang, Ningyu and Chen, Huajun and Cui, Peng and Sachan, Mrinmaya},
  year = {2023},
  month = oct,
  publisher = {arXiv},
  urldate = {2023-11-12},
  abstract = {Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release AGENTS, an open-source library with the goal of opening up these advances to a wider non-specialist audience. AGENTS is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and finegrained symbolic control. AGENTS is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. AGENTS is available at https://github.com/aiwaves-cn/agents.},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nobr/Zotero/storage/HG4P3PS6/Zhou et al. - 2023 - Agents An Open-source Framework for Autonomous Language Agents.pdf}
}

@misc{zhou2024,
  title = {Towards {{Vision-Language Geo-Foundation Model}}: {{A Survey}}},
  shorttitle = {Towards {{Vision-Language Geo-Foundation Model}}},
  author = {Zhou, Yue and Feng, Litong and Ke, Yiping and Jiang, Xue and Yan, Junchi and Yang, Xue and Zhang, Wayne},
  year = {2024},
  month = jun,
  number = {arXiv:2406.09385},
  eprint = {2406.09385},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-04},
  abstract = {Vision-Language Foundation Models (VLFMs) have made remarkable progress on various multimodal tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding. However, most methods rely on training with general image datasets, and the lack of geospatial data leads to poor performance on earth observation. Numerous geospatial image-text pair datasets and VLFMs fine-tuned on them have been proposed recently. These new approaches aim to leverage large-scale, multimodal geospatial data to build versatile intelligent models with diverse geo-perceptive capabilities, which we refer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper thoroughly reviews VLGFMs, summarizing and analyzing recent developments in the field. In particular, we introduce the background and motivation behind the rise of VLGFMs, highlighting their unique research significance. Then, we systematically summarize the core technologies employed in VLGFMs, including data construction, model architectures, and applications of various multimodal geospatial tasks. Finally, we conclude with insights, issues, and discussions regarding future research directions. To the best of our knowledge, this is the first comprehensive literature review of VLGFMs. We keep tracing related works at https://github.com/zytx121/Awesome-VLGFM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/TFJV77Y3/Zhou et al. - 2024 - Towards Vision-Language Geo-Foundation Model A Survey.pdf}
}

@misc{zhou2024a,
  title = {Transfusion: {{Predict}} the {{Next Token}} and {{Diffuse Images}} with {{One Multi-Modal Model}}},
  shorttitle = {Transfusion},
  author = {Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer},
  year = {2024},
  month = aug,
  number = {arXiv:2408.11039},
  eprint = {2408.11039},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-11},
  abstract = {We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixedmodality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nobr/Zotero/storage/LWQGGIGL/Zhou et al. - 2024 - Transfusion Predict the Next Token and Diffuse Images with One Multi-Modal Model.pdf}
}

@incollection{zhu2023,
  title = {Research on {{Multi-aircraft Cooperative Combat Based}} on {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of 2022 {{International Conference}} on {{Autonomous Unmanned Systems}} ({{ICAUS}} 2022)},
  author = {Zhu, Longtao and Wang, Jinlin and Wang, Yi and Ji, Yulong},
  editor = {Fu, Wenxing and Gu, Mancang and Niu, Yifeng},
  year = {2023},
  volume = {1010},
  pages = {1410--1420},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-99-0479-2_129},
  urldate = {2024-01-14},
  abstract = {Multi-aircraft combat is an important part of air combat. Solving the problem of cooperation in multi-aircraft air combat is an important way to realize the intelligent decision-making of multi-aircraft combat. Therefore, this paper presents an intelligent decision-making method for multi-aircraft cooperative operations, which combines the air combat situation assessment method and the deep reinforcement learning method. The situation assessment method uses analytic hierarchy process (AHP). The deep reinforcement learning algorithm uses the MAPPO algorithm. In this paper, a simulation experiment is carried out on the 2v1 multi-aircraft cooperative combat scene using the combined new algorithm, and the visual analysis and victory rate statistics of the training convergence results are carried out. The results show that the algorithm proposed in this paper can control the aircraft to make effective cooperative combat decisions to complete the combat task. This method can also be applied to similar multi aircraft cooperative combat scenarios.},
  isbn = {978-981-99-0478-5 978-981-99-0479-2},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/5N7QPZLJ/Zhu et al. - 2023 - Research on Multi-aircraft Cooperative Combat Based on Deep Reinforcement Learning.pdf}
}

@misc{zhu2025,
  title = {Transformers without {{Normalization}}},
  author = {Zhu, Jiachen and Chen, Xinlei and He, Kaiming and LeCun, Yann and Liu, Zhuang},
  year = {2025},
  month = mar,
  number = {arXiv:2503.10622},
  eprint = {2503.10622},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2503.10622},
  urldate = {2025-03-15},
  abstract = {Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation \$DyT(\$x\$) = {\textbackslash}tanh({\textbackslash}alpha \$x\$)\$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, \$S\$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/nobr/Zotero/storage/NB6AD2A6/Zhu et al. - 2025 - Transformers without Normalization.pdf}
}

@misc{zotero-3954,
  title = {Battle of {{Dien Bien Phu}} {\textbar} {{French Foreign Legion Information}}},
  urldate = {2025-05-12},
  abstract = {Battle of Dien Bien Phu, 1954. History and images of the well-known battle with the Viet Minh in then French Indochina. Well-ordered data, maps, photos, French and Foreign Legion units...},
  langid = {american}
}

@misc{zotero-4006,
  title = {Noah {{Syrkis}}},
  urldate = {2024-12-28},
  howpublished = {http://localhost:4324/},
  file = {/Users/nobr/Zotero/storage/AX2UVVYH/localhost.html}
}

@misc{zotero-4631,
  title = {Understanding {{Transformers Using A Minimal Example}}},
  journal = {rtti.de {\textbar} Fullstack DevOps ML},
  urldate = {2025-09-04},
  abstract = {Visualizing the internal state of a Transformer model},
  howpublished = {https://rti.github.io/gptvis/},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/3QEECGW7/gptvis.html}
}

@article{zotero-4636,
  title = {{{THE HOLY SCRIPTURES ACCORDING TO THE MASORETIC TEXT}}},
  langid = {english},
  file = {/Users/nobr/Zotero/storage/EQ5ED2HJ/THE HOLY SCRIPTURES ACCORDING TO THE MASORETIC TEXT.pdf}
}

@incollection{zou2023,
  title = {Meta-Reinforcement Learning},
  booktitle = {Meta-{{Learning}}},
  author = {Zou, Lan},
  year = {2023},
  pages = {267--297},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-323-89931-4.00011-0},
  urldate = {2023-11-12},
  isbn = {978-0-323-89931-4},
  langid = {english}
}

@article{zuluaga-gomez2023,
  title = {A {{Virtual Simulation-Pilot Agent}} for {{Training}} of {{Air Traffic Controllers}}},
  author = {{Zuluaga-Gomez}, Juan and Prasad, Amrutha and Nigmatulina, Iuliia and Motlicek, Petr and Kleinert, Matthias},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2304.07842},
  urldate = {2024-02-15},
  abstract = {In this paper we propose a novel virtual simulation-pilot engine for speeding up air traffic controller (ATCo) training by integrating different state-of-the-art artificial intelligence (AI) based tools. The virtual simulation-pilot engine receives spoken communications from ATCo trainees, and it performs automatic speech recognition and understanding. Thus, it goes beyond only transcribing the communication and can also understand its meaning. The output is subsequently sent to a response generator system, which resembles the spoken read back that pilots give to the ATCo trainees. The overall pipeline is composed of the following submodules: (i) automatic speech recognition (ASR) system that transforms audio into a sequence of words; (ii) high-level air traffic control (ATC) related entity parser that understands the transcribed voice communication; and (iii) a text-to-speech submodule that generates a spoken utterance that resembles a pilot based on the situation of the dialogue. Our system employs state-of-the-art AI-based tools such as Wav2Vec 2.0, Conformer, BERT and Tacotron models. To the best of our knowledge, this is the first work fully based on open-source ATC resources and AI tools. In addition, we have developed a robust and modular system with optional submodules that can enhance the system's performance by incorporating real-time surveillance data, metadata related to exercises (such as sectors or runways), or even introducing a deliberate read-back error to train ATCo trainees to identify them. Our ASR system can reach as low as 5.5\% and 15.9\% word error rates (WER) on high and low-quality ATC audio. We also demonstrate that adding surveillance data into the ASR can yield callsign detection accuracy of more than 96\%.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Audio and Speech Processing (eess.AS),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Human-Computer Interaction (cs.HC)},
  file = {/Users/nobr/Zotero/storage/CIVBHIXX/Zuluaga-Gomez et al. - 2023 - A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers.pdf}
}

@article{zuo2022,
  title = {Constructing Brain Functional Network by {{Adversarial Temporal-Spatial Aligned Transformer}} for Early {{AD}} Analysis},
  author = {Zuo, Qiankun and Lu, Libin and Wang, Lin and Zuo, Jiahui and Ouyang, Tao},
  year = {2022},
  month = nov,
  journal = {Frontiers in Neuroscience},
  volume = {16},
  pages = {1087176},
  issn = {1662-4548},
  doi = {10.3389/fnins.2022.1087176},
  urldate = {2023-05-24},
  abstract = {Introduction The brain functional network can describe the spontaneous activity of nerve cells and reveal the subtle abnormal changes associated with brain disease. It has been widely used for analyzing early Alzheimer's disease (AD) and exploring pathological mechanisms. However, the current methods of constructing functional connectivity networks from functional magnetic resonance imaging (fMRI) heavily depend on the software toolboxes, which may lead to errors in connection strength estimation and bad performance in disease analysis because of many subjective settings. Methods To solve this problem, in this paper, a novel Adversarial Temporal-Spatial Aligned Transformer (ATAT) model is proposed to automatically map 4D fMRI into functional connectivity network for early AD analysis. By incorporating the volume and location of anatomical brain regions, the region-guided feature learning network can roughly focus on local features for each brain region. Also, the spatial-temporal aligned transformer network is developed to adaptively adjust boundary features of adjacent regions and capture global functional connectivity patterns of distant regions. Furthermore, a multi-channel temporal discriminator is devised to distinguish the joint distributions of the multi-region time series from the generator and the real sample. Results Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) proved the effectiveness and superior performance of the proposed model in early AD prediction and progression analysis. Discussion To verify the reliability of the proposed model, the detected important ROIs are compared with clinical studies and show partial consistency. Furthermore, the most significant altered connectivity reflects the main characteristics associated with AD. Conclusion Generally, the proposed ATAT provides a new perspective in constructing functional connectivity networks and is able to evaluate the disease-related changing characteristics at different stages for neuroscience exploration and clinical disease analysis.},
  pmcid = {PMC9742604},
  pmid = {36518529},
  file = {/Users/nobr/Zotero/storage/6QGVID25/Zuo et al. - 2022 - Constructing brain functional network by Adversari.pdf}
}

@article{zychowski2023,
  title = {Coevolution of Players Strategies in Security Games},
  author = {{\.Z}ychowski, Adam and Ma{\'n}dziuk, Jacek},
  year = {2023},
  month = apr,
  journal = {Journal of Computational Science},
  volume = {68},
  pages = {101980},
  issn = {18777503},
  doi = {10.1016/j.jocs.2023.101980},
  urldate = {2023-11-12},
  abstract = {Stackelberg Security Games (SSGs) gained recently a lot of attention and popularity due to a bunch of successful practical applications in the field of security maintenance. SSGs model real-life security scenarios as noncooperative games between the security forces (e.g. secret service, police) and the attackers (e.g. terrorists, military groups). The paper proposes a novel coevolutionary method (CoEvoSG) for solving SSGs that develops two competing populations of player strategies, in the process inspired by biological evolution, so as to approximate the Stackelberg Equilibrium (game solution). CoEvoSG is experimentally evaluated on over 800 test instances of three types of games with various characteristics. The results and their detailed analysis presented in the paper prove the CoEvoSG ability to repetitively find optimal or close to optimal solutions with time scalability excelling the state-of-the-art methods. Consequently, CoEvoSG is capable of calculating solutions for games bigger and more complex than ever before. This study extends our previously published conference paper {\.Z}ychowski and Ma{\'n}dziuk (2022).},
  langid = {english}
}
